
#+PROPERTY: header-args:jupyter-python :kernel PROMICE_dev :session PROMICE-README :exports both
#+PROPERTY: header-args:bash :exports both

* Table of contents                               :toc_3:noexport:
- [[#introduction][Introduction]]
  - [[#overview][Overview]]
- [[#level-00][Level 00]]
- [[#level-0][Level 0]]
  - [[#l0-files][L0 files]]
  - [[#level-0m][Level 0M]]
    - [[#format][Format]]
    - [[#additional-files][Additional files]]
    - [[#l0m-reader][L0M Reader]]
- [[#l0m---l1][L0M -> L1]]
  - [[#wrapper][Wrapper]]
  - [[#imports][Imports]]
  - [[#read-in-file][Read in file]]
  - [[#eng-to-phys][Eng to phys]]
  - [[#export-file-as-l1][Export file as L1]]
- [[#level-1][Level 1]]
- [[#l1---l1a][L1 -> L1A]]
  - [[#wrapper-1][Wrapper]]
  - [[#imports-1][Imports]]
  - [[#merge-files][Merge files]]
  - [[#flag-data][Flag data]]
  - [[#export-file-as-l1a][Export file as L1A]]
- [[#level-a1][Level A1]]
- [[#l1a---l2][L1A -> L2]]
  - [[#wrapper-2][Wrapper]]
  - [[#imports-2][Imports]]
  - [[#init][Init]]
  - [[#load][Load]]
  - [[#calibrate-using-secondary-sensors][Calibrate using secondary sensors]]
    - [[#correct-relative-humidity][Correct relative humidity]]
    - [[#cloud-cover][Cloud cover]]
    - [[#correct-shortwave-radiation][Correct shortwave radiation]]
    - [[#wind-direction][Wind direction]]
  - [[#export-file-as-l2][Export file as L2]]
- [[#l2---l3][L2 -> L3]]
  - [[#wrapper-3][Wrapper]]
  - [[#imports-3][Imports]]
  - [[#load-1][Load]]
  - [[#derived-properties][Derived properties]]
    - [[#notdone-comment-turbulent-heat-flux][NOTDONE COMMENT Turbulent heat flux]]
  - [[#downsample-to-hourly-and-daily][Downsample to hourly and daily]]
  - [[#export-file-as-l3][Export file as L3]]
- [[#l0m-to-l3][L0M to L3]]
- [[#helper-functions][Helper functions]]
- [[#compare-python--idl][Compare Python & IDL]]

* Introduction

Code used to process the PROMICE AWS data from Level 0 through Level 3 (end-user product).

We use the following processing levels, described textually and graphically.

** Overview
+ L00 : Sensor data in the CR-1000 logger
+ L0: Raw data from L00 in CSV file format with suggested filenames and organized by station folder
  + =raw=, =STM= (Slim Table Memory), and =TX= (transmitted)
+ L0M: (MANUAL)
  + Standardized NEAD headers for =raw=, =STM=, and =TX=
  + Manually split so no file includes changed sensors
  + Precise file names
+ L1:
  + Engineering units (e.g. current or volts) converted to physical units (e.g. temperature or wind speed)
+ L1A:
  + Invalid / bad / suspicious data flagged
  + Files merged to one time series per station
+ L2:
  + Calibration using secondary sources (e.g. radiometric correction requires input of tilt sensor)
+ L3:
  + Derived products (e.g. SHF and LHF)
  + Merge formats to one product here?

#+begin_src ditaa :file ./fig/levels.png :exports results

                    +----------------+
	            |{d}             |                         Legend
                    | Digital counts |                         +---------------+
     Level 00 (L00) |                |                         |input          |
		    | CR-1000 logger |                         +---------------+
	            |                |
	            +-------+--------+                         +---------------+   +=----+
	                    |				       |{io}process    +--=+ Note|
	                    v				       +---------------+   +-----+
                    +----------------+
	            |{io}            |                         +---------------+
                    |  Manual Carry  |      		       |{d}Files       |
                    |      or        |      		       +---------------+
		    |   Satellite    |
	            |                |			
	            +-------+--------+			
	                    |               +=---------------------------------+            
	                    v            +--+Arbitrary file names              |            
                    +----------------+   |  |Repeat data (more than 1 download)|            
	            |{d}             |   |  |More than 1 sensor per file       |
                    |  raw, STM, TX  +=--+  +----------------------------------+
     Level 0 (L0)   |                |      
		    | GEUS text files|	    
	            |                |	    
	            +-------+--------+	    	   						    
	                    |		         	        /----------------------------------\ 		
	                    v		                   +----+ Split files by sensor changes{io}| 		
                    +----------------+                     |    +----------------------------------+ 		
	            |{io}            |	                   |					    
                    |  Copy L0 to    |	                   |    +--------------------+		    
                    |       L0M      |                     +----+ Precise file names | 		    
	            |                |	   +---------------+    +--------------------+		    
	            +-------+--------+     |               |					    
                            |              |               |    +--------------------+		    
                            v              |               +----+ NEAD headers       |		    
		    +-------+---------+    |	    	        +--------------------+		    
		    |{d}              |    |     	          ^      ^     ^			    
                    |     Manual      |    |                      |      |     |			    
     Level 0M (L0M) |                 |<---+                      |      |   +-+----------+	    
		    | Standardization |                           |      |   |Metadata    |	    
		    |                 |                           |      |   +------------+	    
 		    +-------+---------+                           |    +-+----------------+	    
			    |	      	                          |    |Columns, units, ..|	    
                            v               	                  |    +------------------+	    
	            +-----------------+           	        +-+---------------------------------+
	            |{io}             |                         | Instrument calibration parameters |
	            |  Engineering to |   	   	        |      (recorded, not applied)      |
	            |  physical units |                         +-----------------------------------+
	            |                 |   
                    +-------+---------+   
		            |      	  
	                    v             
                    +-----------------+   
		    |{d}              |   
    Level 1 (L1)    |Measured physical|   
		    |    properties   |
		    |                 |
		    +-------+---------+	  
                            |		  
                            v		  
                    +-----------------+
                    |{io}             |
                    |   Flag bad data |
                    |   Merge files   |
                    |                 |
                    +-------+---------+
                            |           
                            v          
                   +-------------------+
                   |{d}                |
    Level 1A (L1A) |Time series per AWS|
                   |  Initial data QC  |
		   |                   |
                   +-------+-----------+
                           |
                           v
                    +-----------------+
                    |{io}             |       +=------------------------------------------+ 
                    | Cross-sensor    |------=+e.g. ice at 1 m depth via interpolation, or| 
                    |  corrections    |       |radiation adjusting for platform rotation  |
                    |                 |       +-------------------------------------------+ 
                    +-------+---------+       
                            |          
                            v          
                   +-------------------+
                   |{d}                |
     Level 2 (L2)  |  Derived internal |
                   |      values       |
	           |                   |
                   +-------+-----------+
                           |
                           v
                    +-----------------+
                    |{io}             |
                    |     Derive      |       +=-----------------------+
                    |    external     |------=+e.g. sensible heat flux,|
                    |   properties    |       |latent heat flux        |
                    |                 |       +------------------------+
                    +-------+---------+
                            |          
                            v          
                   +-------------------+
                   |{d}                |
     Level 3 (L3)  |  Derived external |
                   |      values       |
		   |                   |
                   +-------------------+


#+END_SRC
		    
#+RESULTS:
[[file:./fig/levels.png]]

* Level 00

+ Digital numbers (DN) in the CR1000 logger.

* Level 0

Level 00 is converted to Level 0 in the CR1000 logger during download, or after the DNs are broadcast to the satellite and processed by https://github.com/GEUS-PROMICE/awsrx.

#+begin_src plantuml :file ./fig/L00_to_L0.png :exports results
@startuml

' plantuml activity diagram (beta)

component Sensor_1
component Sensor_n

frame CR1000_Logger {
  database DB_logger [
  <b>Database</b>
  10 minute sampling
  ----
  var0, var1, ..., varn
] 
}

note right
  Level 00 (L00)
end note

Sensor_1 --> CR1000_Logger
Sensor_n --> CR1000_Logger

node GEUS_(Level_0) {
  file Raw [
  <b>raw</b>
  10 min sampling
  ]

  file SlimTableMem [
  <b>SlimTableMem</b>
  Hourly average from
  10 min sampling
  ]

  file TX [
  <b>TX</b>
  V3:
    DOY 100 to 300: hourly average
    DOY 300 to 100: daily average
  V4:
    hourly average all days
  ]
}

' DB -> hand carry -> raw
actor Scientist
DB_logger --> Scientist : Field\ndownload
Scientist --> Raw : Hand\ncarry
Scientist --> SlimTableMem : Hand\ncarry

' DB -> satellite -> Transmitted
cloud Satellite
file Email
queue awsrx
note right
   https://github.com/GEUS-PROMICE/awsrx
end note

DB_logger -[dashed]-> Satellite : Data subsampled and\npossible transmission loss
Satellite -[dashed]-> Email
Email --> awsrx : L00
awsrx --> TX

@enduml
#+end_src

#+RESULTS:
[[file:./fig/L00_to_L0.png]]

** L0 files

+ =raw= : All 10-minute data stored on the CF-card (external module on CR1000)
+ =SlimTableMem= : Hourly averaged 10-min data stored in the internal logger memory
+ =transmitted= : Transmitted via satellite. Only a subset of data is transmitted, and only hourly or daily average depending on station and day of year.

Level 0 files are stored in the =data/L0/<S>/= folder, where =<S>= is the station name. File names should encode the station, end-of-year of download, a version number if there are multiple files for a given year, and the format. However, no code reads thees files, so there is no strict file naming convention. Still, best practices would use the following conventions:  

=data/<L>/<S>/<S>-<Y>[.<n>]-<F>.txt=

Where 

+ =<L>= is the processing level
  + =<L>= must be one of the following: [L0, L0M, L1, L1A, L2, L3]
+ =<S>= is a station ID
  + =<S>= must be one of the following strings: [CEN, EGP, KAN_B, KAN_L, KAN_M, KAN_U, KPC_L, KPC_U, MIT, NUK_K, NUK_L, NUK_N, NUK_U, QAS_A, QAS_L, QAS_M, QAS_U, SCO_L, SCO_U, TAS_A, TAS_L, TAS_U, THU_L, THU_U, UPE_L, UPE_U]
+ =<Y>= is a four-digit year with a value greater than =2008=
  + =<Y>= should represent the year at the last timestamp in the file
  + Optionally, =.<n>= is a version number if multiple files from the same year are present
+ =<F>= is the format, one of =raw=, =TX=, or =STM=


** Level 0M

Level 0M (L0M) is L0 with the following *manual* changes:

+ Files that will continue in the processing pipeline are copied from =L0/<S>/= to =L0M/<S>/= folders.
+ Strict naming format must be followed: =data/L0M/<S>/<S>-<Y>[.<n>]-<F>.txt= (see L0 naming suggestion above, now a requirement)
+ Headers, if present, are standardized to NEAD 1.0 format
  + NEAD header templates are provided, or should be copied from existing files and updated for the new data/sensors/loggers/etc.
  + NEAD headers should include instrument calibration parameters, serial numbers, etc.
  + Existing headers, if they exist, are removed. Any information that should be retained is incorporated into the NEAD header. The L0 data will always retain the original unmodified header if it is needed.
  + NEAD format specification: https://github.com/GEUS-PROMICE/NEAD/
+ Split files if necessary
  + Files should be split so that each file only contains one of each sensor. That is if a station visit swapped sensor X with Y, then two files should exist, one with X (with metadata such as serial number, calibration paramater, etc. for X), and one with Y (with associated metadata).

All changes should be documented in the [[./data/L0M/README.org]] file.
    
*** Format

We use the [[https://github.com/mankoff/NEAD/][NEAD 1.0]] file format with PROMICE-specific headers. A detailed description of the format is at https://github.com/GEUS-PROMICE/NEAD and a Python reader is at https://github.com/GEUS-PROMICE/pyNEAD.

*** Additional files

Any files that do not conform to the above name format requirement will be ignored. However, for cleanliness, additional files should be placed in sub-folders of =L0M/<S>=. If any additional files are created in order to manually adjust problematic data or for any other purpose, an entry should be created in the top level =data/README.org= linking to the original file, the new file, describing what was done and why, and perhaps including a diff.

*** L0M Reader

#+BEGIN_SRC jupyter-python :exports both
import nead
ds = nead.read("./data/L0M/EGP/EGP-2016-raw.txt", index_col=0)
print(ds)
#+END_SRC

#+RESULTS:
#+begin_example
<xarray.Dataset>
Dimensions:      (time: 10847)
Coordinates:
  ,* time         (time) datetime64[ns] 2016-05-01T14:30:00 ... 2016-07-19T17:...
Data variables:
    rec          (time) float64 51.0 52.0 53.0 ... 1.09e+04 1.09e+04 1.09e+04
    min_y        (time) float64 1.765e+05 1.766e+05 ... 2.905e+05 2.905e+05
    p            (time) float64 724.4 724.1 724.4 724.4 ... 730.8 731.2 730.7
    t_1          (time) float64 -20.1 -19.79 -19.31 ... -6.904 -6.904 -6.861
    t_2          (time) float64 -19.56 -19.11 -18.92 ... -6.866 -6.86 -6.799
    rh           (time) float64 54.1 51.7 50.23 49.51 ... 80.28 80.93 81.81
    wspd         (time) float64 1.062 0.918 0.636 0.486 ... 2.793 2.951 3.069
    wdir         (time) float64 265.1 259.2 216.8 208.4 ... 217.7 216.6 225.4
    wd_std       (time) float64 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0
    dsr          (time) float64 681.7 732.4 688.3 689.6 ... 724.7 711.4 698.8
    usr          (time) float64 518.6 559.3 531.8 534.4 ... 559.2 549.6 524.1
    dlr          (time) float64 -81.57 -102.0 -101.3 ... -135.8 -135.6 -132.4
    ulr          (time) float64 -23.97 -28.65 -33.92 ... -32.33 -32.52 -28.84
    t_rad        (time) float64 -12.78 -11.42 -9.929 ... -1.114 -1.03 -1.135
    z_boom       (time) float64 2.685 2.683 2.683 2.68 ... 2.583 2.584 2.58
    z_boom_q     (time) float64 190.0 192.0 189.0 187.0 ... 192.0 182.0 168.0
    z_stake      (time) float64 nan nan nan nan nan nan ... nan nan nan nan nan
    z_stake_q    (time) float64 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0
    z_pt         (time) float64 nan nan nan nan nan nan ... nan nan nan nan nan
    t_i_1        (time) float64 nan -109.0 -109.0 ... -8.478 -8.458 -8.448
    t_i_2        (time) float64 nan nan -109.0 -109.0 ... -9.67 -9.67 -9.67
    t_i_3        (time) float64 nan -109.0 -109.0 ... -8.879 -8.859 -8.849
    t_i_4        (time) float64 nan -109.0 -109.0 ... -10.74 -10.73 -10.74
    t_i_5        (time) float64 nan -109.0 -109.0 ... -12.67 -12.67 -12.67
    t_i_6        (time) float64 nan -109.0 nan -109.0 ... -14.9 -14.9 -14.9
    t_i_7        (time) float64 nan -109.0 -109.0 nan ... -17.16 -17.16 -17.16
    t_i_8        (time) float64 nan nan -109.0 nan ... -20.75 -20.76 -20.76
    tilt_x       (time) float64 3.527 3.492 3.516 3.489 ... 0.109 0.095 0.174
    tilt_y       (time) float64 -0.945 -0.938 -0.924 ... -0.828 -0.849 -0.859
    gps_time     (time) object nan nan nan ... 'GT170007.00' 'GT170007.00'
    gps_lat      (time) object nan nan nan ... 'NH7537.47563' 'NH7537.47563'
    gps_lon      (time) object nan nan nan ... 'WH03558.49655' 'WH03558.49655'
    gps_alt      (time) float64 nan nan nan ... 2.663e+03 2.663e+03 2.663e+03
    gps_geoid    (time) float64 nan nan nan nan nan ... 41.6 41.6 41.6 41.6 41.6
    gps_geounit  (time) object nan nan nan nan nan nan ... 'M' 'M' 'M' 'M' 'M'
    gps_q        (time) float64 nan nan nan nan nan nan ... 1.0 1.0 1.0 1.0 1.0
    gps_numsat   (time) float64 nan nan nan nan nan ... 11.0 12.0 12.0 12.0 12.0
    gps_hdop     (time) float64 nan nan nan nan nan ... 0.71 0.73 0.73 0.73 0.73
    t_log        (time) float64 -12.6 -12.08 -11.65 ... -1.801 -1.735 -1.5
    fan_dc       (time) float64 137.5 141.3 142.3 141.8 ... 123.5 123.9 124.1
    batt_v_ss    (time) float64 15.52 15.81 15.79 15.81 ... 14.47 14.47 14.47
    batt_v       (time) float64 15.23 15.56 15.53 15.63 ... 14.4 14.41 14.41
Attributes:
    station_id:          EGP
    field_delimiter:     ,
    nodata:              -999
    srid:                EPSG:4326
    geometry:            POINT(-35.9748, 75.6247)
    timezone:            0
    PROMICE_format:      raw
    hygroclip_t_offset:  0
    dswr_eng_coef:       12.71
    uswr_eng_coef:       12.71
    dlwr_eng_coef:       12.71
    ulwr_eng_coef:       12.71
    pt_z_coef:           0
    pt_z_p_coef:         0
    pt_z_factor:         0
    pt_antifreeze:       0
    boom_azimuth:        0
#+end_example



* L0M -> L1
:PROPERTIES:
:header-args:jupyter-python+: :session L0_to_L1 :noweb-ref L0_to_L1 :noweb yes
:END:

+ Convert engineering units to physical units

** Wrapper

Run one:
#+BEGIN_SRC jupyter-python :noweb-ref
infile = "./data/L0M/EGP/EGP-2016-raw.txt"
#+END_SRC

#+BEGIN_SRC jupyter-python :noweb-ref
<<L0_to_L1>>
#+END_SRC

Run all:
#+BEGIN_SRC bash
# conda activate PROMICE_dev
for f in $(ls ./data/L0M/EGP/EGP-201*raw*); do
  echo ${f}
  ./L0_to_L1.py ${f}
done

./L0_to_L1.py data/L0M/EGP/EGP-2017-STM.txt 
./L0_to_L1.py data/L0M/EGP/EGP-2019-TX.txt
#+END_SRC


#+BEGIN_SRC jupyter-python :tangle L0_to_L1.py :noweb-ref :tangle-mode (identity #o744)
#!/usr/bin/env python

def L0_to_L1(infile=None):
    <<L0_to_L1>>


if __name__ == "__main__":
    import sys
    L0_to_L1(sys.argv[1])
#+END_SRC


** Imports

#+BEGIN_SRC jupyter-python
import re
import shapely
from shapely import geometry
import nead
import os
import sys
import numpy as np
import pandas as pd
#+END_SRC

#+RESULTS:

** Read in file

+ GitHub link: [[./IDL/AWSdataprocessing_v3.pro#L51]] through [[./IDL/AWSdataprocessing_v3.pro#L123]]
+ Org link: [[./IDL/AWSdataprocessing_v3.pro::51]] through [[./IDL/AWSdataprocessing_v3.pro::123]]
+ [X] Reads in the file
+ [X] Check that required metadata was included in the NEAD header

#+BEGIN_SRC jupyter-python
ds = nead.read(infile, index_col=0)
assert("geometry" in ds.attrs.keys())
assert(ds.attrs['geometry'][0:5] == "POINT")
assert("srid" in ds.attrs.keys())
assert(ds.attrs['srid'] == "EPSG:4326")
assert("timezone" in ds.attrs.keys())
assert("pt_antifreeze" in ds.attrs.keys())
if 't_2' in list(ds.variables): assert("hygroclip_t_offset" in ds.attrs.keys())
#+END_SRC

#+RESULTS:

** Eng to phys

+ GitHub link: [[./IDL/AWSdataprocessing_v3.pro#L116]] through [[./IDL/AWSdataprocessing_v3.pro#L408]] 
+ Org link: [[./IDL/AWSdataprocessing_v3.pro::116]] through [[./IDL/AWSdataprocessing_v3.pro::408]] 
  + [-] Calculates derived date products (day of century, etc.)
  + [ ] Adjusts start times
    + [ ] ~if slimtablemem eq 'yes' then begin ; change time stamp to start of the hour instead of end~
    + [ ] ~if transmitted eq 'yes' then begin ; change transmission time to start of the hour/day instead of end~
      + [ ] ~if line[col_season-1] eq '!W' then begin ; daily transmissions~
      + [ ] ~if line[col_season-1] eq '!S' then begin ; hourly transmissions~
      + [ ] Makes guesses if season identifier not transmitted
  + [X] Adjusts UTC offset
  + [X] Remove HygroClip temperature offset
  + [X] Reads and adjusts SRin ~SRin = [SRin,float(line[col_SRin-1])*10/C_SRin] ; Calculating radiation (10^-5 V -> W/m2)~
  + [X] SRout
  + [X] LRin: ~LRin = [LRin,float(line[col_LRin-1])*10/C_LRin + 5.67e-8*(float(line[col_Trad-1])+T_0)^4]~
  + [X] LRout
  + [X] Haws: ~Haws = [Haws,float(line[col_Haws-1])*((float(line[col_T-1])+T_0)/T_0)^0.5]~
  + [X] Hstk: ~Hstk = [Hstk,float(line[col_Hstk-1])*((float(line[col_T-1])+T_0)/T_0)^0.5]~
  + [X] Hpt: ~Hpt = [Hpt,float(line[col_Hpt-1])*C_Hpt*F_Hpt*998./rho_af]~
  + [X] Derives Hpt_corrected
  + [X] Decodes GPS - some stations only record minutes not degrees


#+BEGIN_SRC jupyter-python

T_0 = 273.15

# Calculate pressure transducer fluid density
if ds.attrs['pt_antifreeze'] == 50:
    rho_af = 1092
elif ds.attrs['pt_antifreeze'] == 100:
    rho_af = 1145
else:
    rho_af = np.nan
    if np.any(~np.isnan(ds['z_pt'].values)):
        print("ERROR: Incorrect NEAD metadata: 'pt_antifreeze =' ", ds.attrs['pt_antifreeze'])
        print("Antifreeze mix only supported at 50 % or 100%")
        # assert(False)
    

for v in ['gps_geounit','min_y']:
    if v in list(ds.variables): ds = ds.drop_vars(v)
        
## adjust times based on file format.
# raw: No adjust (timestamp is at start of period)
# STM: Adjust timestamp from end of period to start of period
# TX: Adjust timestamp start of period (hour/day) also depending on season
# if ds.attrs['PROMICE_format'] == 'STM': ds['time'] = (('time'), ds['time'].to_dataframe().shift(periods=1))
# if ds.attrs['PROMICE_format'] == 'TX': ds['time'] = (('time'), ds['time'].to_dataframe().shift(periods=1))
if ds.attrs['timezone'] != 0:
    a = ds['time'].attrs
    ds['time'] = (('time'), ds['time'].to_dataframe().shift(periods=ds.attrs['timezone'], freq='H').index)
    ds['time'].attrs = a

# Remove HygroClip temperature offset
ds['t_2'] = ds['t_2'] - ds.attrs['hygroclip_t_offset']

# convert radiation from engineering to physical units
ds['dsr'] = (ds['dsr'] * 10) / ds.attrs['dsr_eng_coef']
ds['usr'] = (ds['usr'] * 10) / ds.attrs['usr_eng_coef']
ds['dlr'] = ((ds['dlr'] * 10) / ds.attrs['dlr_eng_coef']) + 5.67E-8*(ds['t_rad'] + T_0)**4
ds['ulr'] = ((ds['ulr'] * 10) / ds.attrs['ulr_eng_coef']) + 5.67E-8*(ds['t_rad'] + T_0)**4

# Adjust sonic ranger readings for sensitivity to air temperature
ds['z_boom'] = ds['z_boom'] * ((ds['t_1'] + T_0)/T_0)**0.5 
ds['z_stake'] = ds['z_stake'] * ((ds['t_1'] + T_0)/T_0)**0.5
# Adjust pressure transducer due to fluid properties
ds['z_pt'] = ds['z_pt'] * ds.attrs['pt_z_coef'] * ds.attrs['pt_z_factor'] * 998.0 / rho_af

# Calculate pressure transducer depth
ds['z_pt_corr'] = ds['z_pt'] * np.nan # new 'z_pt_corr' copied from 'z_pt'
ds['z_pt_corr'].attrs['long_name'] = ds['z_pt'].long_name + " corrected"
ds['z_pt_corr'] = ds['z_pt'] * ds.attrs['pt_z_coef'] * ds.attrs['pt_z_factor'] * 998.0 / rho_af \
    + 100 * (ds.attrs['pt_z_p_coef'] - ds['p']) / (rho_af * 9.81)


# Decode GPS
if ds['gps_lat'].dtype.kind == 'O': # not a float. Probably has "NH"
    assert('NH' in ds['gps_lat'].dropna(dim='time').values[0])
    for v in ['gps_lat','gps_lon','gps_time']:
        a = ds[v].attrs # store
        str2nums = [re.findall(r"[-+]?\d*\.\d+|\d+", _) if isinstance(_, str) else [np.nan] for _ in ds[v].values]
        ds[v][:] = pd.DataFrame(str2nums).astype(np.float).T.values[0]
        ds[v] = ds[v].astype(np.float)
        ds[v].attrs = a # restore

if np.any((ds['gps_lat'] <= 90) & (ds['gps_lat'] > 0)):  # Some stations only recorded minutes, not degrees
    xyz = np.array(re.findall("[-+]?[\d]*[.][\d]+", ds.attrs['geometry'])).astype(np.float)
    x=xyz[0]; y=xyz[1]; z=xyz[2] if len(xyz) == 3 else 0
    p = shapely.geometry.Point(x,y,z)
    ds['gps_lat'] = ds['gps_lat'] + 100*p.y
if np.any((ds['gps_lon'] <= 90) & (ds['gps_lon'] > 0)):
    ds['gps_lon'] = ds['gps_lon'] + 100*p.x

for v in ['gps_lat','gps_lon']:
    a = ds[v].attrs # store
    ds[v] = np.floor(ds[v] / 100) + (ds[v] / 100 - np.floor(ds[v] / 100)) * 100 / 60
    ds[v].attrs = a # restore

# tilt-o-meter voltage to degrees
abst = np.abs(ds['tilt_x'])
ds['tilt_x'] = ds['tilt_x'] / 10
ds['tilt_x'] = ds['tilt_x'] / (abst * (-0.49*abst**4 + 3.6*abst**3 - 10.4*abst**2 + 21.1*abst))
abst = np.abs(ds['tilt_y'])
ds['tilt_y'] = ds['tilt_y'] / 10
ds['tilt_y'] = ds['tilt_y'] / (abst * (-0.49*abst**4 + 3.6*abst**3 - 10.4*abst**2 + 21.1*abst))
#+END_SRC

#+RESULTS:

** Export file as L1

+ Check with ~cfchecks ./data/L1/EGP/EGP-2016-raw.nc~

#+BEGIN_SRC jupyter-python
outpath = os.path.split(infile)[0].split("/")
outpath[-2] = 'L1'
outpath = '/'.join(outpath)
outfile = os.path.splitext(os.path.basename(infile))[0]

outpathfile = outpath + '/' + outfile + ".nc"
if os.path.exists(outpathfile): os.remove(outpathfile)
ds.to_netcdf(outpathfile, mode='w', format='NETCDF4', compute=True)
#+END_SRC

#+RESULTS:



* Level 1
:PROPERTIES:
:header-args:bash+: :exports both
:END:

File list:

#+BEGIN_SRC bash :exports both :results verbatim
find ./data/L1
#+END_SRC

#+RESULTS:
#+begin_example
./data/L1
./data/L1/EGP
./data/L1/EGP/EGP-2017-STM.nc
./data/L1/EGP/EGP-2016-raw.nc
./data/L1/EGP/EGP-2019-TX.nc
./data/L1/EGP/EGP-2017-raw.nc
./data/L1/EGP/EGP-2019.1-raw.nc
./data/L1/EGP/EGP-2018.2-raw.nc
./data/L1/EGP/EGP-2018.1-raw.nc
./data/L1/EGP/EGP-2019.2-raw.nc
#+end_example

NetCDF format

#+BEGIN_SRC bash :results verbatim :exports both
ncdump -ch ./data/L1/EGP/EGP-2016-raw.nc | head -n35
#+END_SRC

#+RESULTS:
#+begin_example
netcdf EGP-2016-raw {
dimensions:
	time = 10847 ;
variables:
	double rec(time) ;
		rec:_FillValue = NaN ;
		rec:standard_name = "record" ;
		rec:long_name = "Record" ;
		rec:units = "" ;
		rec:scale_factor = 1. ;
		rec:add_offset = 0. ;
	double p(time) ;
		p:_FillValue = NaN ;
		p:standard_name = "air_pressure" ;
		p:long_name = "Air pressure" ;
		p:units = "hPa" ;
		p:scale_factor = 0.01 ;
		p:add_offset = 0. ;
	double t_1(time) ;
		t_1:_FillValue = NaN ;
		t_1:standard_name = "air_temperature" ;
		t_1:long_name = "Air temperature 1" ;
		t_1:units = "C" ;
		t_1:scale_factor = 1. ;
		t_1:add_offset = 273.15 ;
	double t_2(time) ;
		t_2:_FillValue = NaN ;
		t_2:standard_name = "air_temperature" ;
		t_2:long_name = "Air temperature 2" ;
		t_2:units = "C" ;
		t_2:scale_factor = 1. ;
		t_2:add_offset = 273.15 ;
	double rh(time) ;
		rh:_FillValue = NaN ;
		rh:standard_name = "relative_humidity" ;
#+end_example


* L1 -> L1A
:PROPERTIES:
:header-args:jupyter-python+: :session L1_to_L1A :noweb-ref L1_to_L1A :noweb yes
:END:

+ Merge all files by type (keep =raw=, =STM=, and =TX=)
+ Flag out-of-limit (OOL) values from [[./flags.csv]]

** Wrapper

Run one:
#+BEGIN_SRC jupyter-python :noweb-ref
infile = "./data/L1/EGP/EGP-2016-raw.nc"
infile = "./data/L1/EGP/EGP-2017-STM.nc"
<<L1_to_L1A>>
#+END_SRC

#+RESULTS:

Run all:

#+BEGIN_SRC bash
# conda activate PROMICE_dev

# ./L1_to_L1A.py ./data/L1/EGP/EGP-2016-raw.nc
./L1_to_L1A.py data/L1/EGP/*raw.nc
./L1_to_L1A.py data/L1/EGP/*STM.nc
./L1_to_L1A.py data/L1/EGP/*TX.nc
#+END_SRC

#+RESULTS:

#+header:  :tangle L1_to_L1A.py :noweb-ref :tangle-mode (identity #o744)
#+BEGIN_SRC jupyter-python
#!/usr/bin/env python

<<L1_to_L1A_imports>>

def L1_to_L1A(infile=None):
    <<L1_to_L1A>>


if __name__ == "__main__":
    import sys
    # print(sys.argv[1:])
    L1_to_L1A(sys.argv[1:])
#+END_SRC

** Imports

#+header: :noweb-ref L1_to_L1A_imports
#+BEGIN_SRC jupyter-python
import pandas as pd
import xarray as xr
import os
#+END_SRC

#+RESULTS:

** Merge files
#+BEGIN_SRC jupyter-python :exports both
ds = xr.open_mfdataset(infile, combine='by_coords', mask_and_scale=False).load()
#+END_SRC

#+RESULTS:

** Flag data

Out of limit (OOL) data comes from the [[./data/variables.csv]] file.

+ Set each variable to NaN where it is OOL
+ Also set paired or associated variables to NaN

#+BEGIN_SRC jupyter-python
df = pd.read_csv("./data/variables.csv", index_col=0, comment="#", usecols=('fields','lo','hi','OOL'))
df = df.dropna(how='all')

for var in df.index:
    if var not in list(ds.variables): continue
    ds[var] = ds[var].where(ds[var] > df.loc[var, 'lo'])
    ds[var] = ds[var].where(ds[var] < df.loc[var, 'hi'])
    other_vars = df.loc[var]['OOL'] # either NaN or "foo" or "foo bar baz ..."
    if isinstance(other_vars, str): 
        for o in other_vars.split():
            ds[o] = ds[var].where(ds[var] > df.loc[var, 'lo'])
            ds[o] = ds[var].where(ds[var] < df.loc[var, 'hi'])
#+END_SRC

#+RESULTS:


** Export file as L1A

+ Check with ~cfchecks ./data/L1A/EGP/EGP-2016-raw.nc~

#+BEGIN_SRC jupyter-python
if isinstance(infile, list): infile = infile[0]
infile_parts = os.path.splitext(os.path.basename(infile))[0].split('-')
outfile = infile_parts[0] + '-' + infile_parts[-1] + '.nc' # drop year

outpath = os.path.split(infile)[0].split("/")
outpath[-2] = 'L1A'
# outfile = os.path.splitext(os.path.basename(infile))[0] + '.nc'
outpath = '/'.join(outpath)
outpathfile = outpath + '/' + outfile
if os.path.exists(outpathfile): os.remove(outpathfile)
ds.to_netcdf(outpathfile, mode='w', format='NETCDF4', compute=True)
#+END_SRC

#+RESULTS:




* Level A1
* L1A -> L2
:PROPERTIES:
:header-args:jupyter-python+: :session L1A_to_L2 :noweb-ref L1A_to_L2 :noweb yes
:END:

+ Calibration using secondary sources

** Wrapper

Run one:
#+BEGIN_SRC jupyter-python :noweb-ref
infile = "./data/L1A/EGP/EGP-raw.nc"
<<L1A_to_L2>>
#+END_SRC

#+RESULTS:

Run all:

#+BEGIN_SRC bash
# conda activate PROMICE_dev

./L1A_to_L2.py data/L1A/EGP/EGP-raw.nc
./L1A_to_L2.py data/L1A/EGP/EGP-STM.nc
./L1A_to_L2.py data/L1A/EGP/EGP-TX.nc
#+END_SRC


#+BEGIN_SRC jupyter-python :tangle L1A_to_L2.py :noweb-ref :tangle-mode (identity #o744)
#!/usr/bin/env python

<<L1A_to_L2_imports>>

def L1A_to_L2(infile=None):
    <<L1A_to_L2>>


if __name__ == "__main__":
    import sys
    L1A_to_L2(sys.argv[1])
#+END_SRC

** Imports

#+header: :noweb-ref L1A_to_L2_imports
#+BEGIN_SRC jupyter-python
import xarray as xr
import pandas as pd
import os

#+END_SRC

#+RESULTS:

** Init

#+BEGIN_SRC jupyter-python
<<constants>>
#+END_SRC


** Load
#+BEGIN_SRC jupyter-python :exports both
# infile = "./data/L1A/EGP/EGP-raw.nc"
ds = xr.open_dataset(infile, mask_and_scale=False).load()
# print(ds)
#+END_SRC

#+RESULTS:

** Calibrate using secondary sensors

*** Correct relative humidity

+ Correct relative humidity readings for T below 0 to give value with respect to ice
  + GitHub: [[./IDL/AWSdataprocessing_v3.pro#L411]]
  + Org Mode: [[./IDL/AWSdataprocessing_v3.pro::411]]

+ This section implements the Goff-Gratch equation
 
#+BEGIN_SRC jupyter-python
T_0 = 273.15

T_100 = T_0+100            # steam point temperature in K
ews = 1013.246             # saturation pressure at steam point temperature, normal atmosphere
ei0 = 6.1071

T = ds['t_1']

# in hPa (Goff & Gratch)
e_s_wtr = 10**(-7.90298 * (T_100 / (T + T_0) - 1)
               + 5.02808 * np.log10(T_100 / (T + T_0)) 
               - 1.3816E-7 * (10**(11.344 * (1 - (T + T_0) / T_100)) - 1)
               + 8.1328E-3 * (10**(-3.49149 * (T_100/(T + T_0) - 1)) -1)
               + np.log10(ews))

# in hPa (Goff & Gratch)
e_s_ice = 10**(-9.09718 * (T_0 / (T + T_0) - 1)
               - 3.56654 * np.log10(T_0 / (T + T_0))
               + 0.876793 * (1 - (T + T_0) / T_0)
               + np.log10(ei0))

ds['rh_cor'] = (e_s_wtr / e_s_ice) * ds['rh'].where((ds['t_1'] < 0) & (ds['t_1'] > -100))



df = pd.read_csv("./data/variables.csv", index_col=0, comment="#", usecols=('fields','lo','hi','OOL'))
var = 'rh_cor'
if var in list(ds.variables):
    ds[var] = ds[var].where(ds[var] > df.loc[var, 'lo'])
    ds[var] = ds[var].where(ds[var] < df.loc[var, 'hi'])
    other_vars = df.loc[var]['OOL'] # either NaN or "foo" or "foo bar baz ..."
    if isinstance(other_vars, str): 
        for o in other_vars.split():
            ds[o] = ds[var].where(ds[var] > df.loc[var, 'lo'])
            ds[o] = ds[var].where(ds[var] < df.loc[var, 'hi'])
#+END_SRC

#+RESULTS:



*** Cloud cover

+ cloud cover (for iswr correction) and surface temperature
  + GitHub: [[./IDL/AWSdataprocessing_v3.pro#L441]]
  + Org Mode: [[./IDL/AWSdataprocessing_v3.pro::441]]

This is a derived product and belongs is L2->L3 processing appearing in L3, but DifFrac is used in the iswr correction.

#+BEGIN_SRC jupyter-python

eps_overcast = 1.
eps_clear = 9.36508e-6
LR_overcast = eps_overcast * 5.67e-8 *(T + T_0)**4   # assumption
LR_clear = eps_clear * 5.67e-8 * (T + T_0)**6        # Swinbank (1963)

# Special case for selected stations (will need this for all stations eventually)
if ds.attrs['station_id'] == 'KAN_M':
   # print,'KAN_M cloud cover calculations'
   LR_overcast = 315 + 4*T
   LR_clear = 30 + 4.6e-13 * (T + T_0)**6
elif ds.attrs['station_id'] == 'KAN_U':
   # print,'KAN_U cloud cover calculations'
   LR_overcast = 305 + 4*T
   LR_clear = 220 + 3.5*T

cc = (ds['dsr'] - LR_clear) / (LR_overcast - LR_clear)
cc[cc > 1] = 1
cc[cc < 0] = 0
DifFrac = 0.2 + 0.8 * cc

emissivity = 0.97
Tsurf = ((ds['ulr'] - (1 - emissivity) * ds['dsr']) / emissivity / 5.67e-8)**0.25 - T_0
Tsurf[Tsurf > 0] = 0
# too_warm = Tsurf > 0
# if total(too_warm) ne -1 then Tsurf[too_warm] = 0

#+END_SRC

#+RESULTS:



*** Correct shortwave radiation

+ Take into account station tilt, sun angle, etc.
  + GitHub: [[./IDL/AWSdataprocessing_v3.pro#L475]]
  + Org Mode: [[./IDL/AWSdataprocessing_v3.pro::475]]

Calculate tilt angle and direction of sensor and rotating to a north-south aligned coordinate system
#+BEGIN_SRC jupyter-python
tx = ds['tilt_x'] * deg2rad
ty = ds['tilt_y'] * deg2rad

## cartesian coords
X = np.sin(tx) * np.cos(tx) * np.sin(ty)**2 + np.sin(ty) * np.cos(ty)**2
Y = np.sin(ty) * np.cos(ty) * np.sin(tx)**2 + np.sin(ty) * np.cos(tx)**2
Z = np.cos(tx) * np.cos(ty) + np.sin(tx)**2 * np.sin(ty)**2

# spherical coords
phi_sensor_rad = -np.pi /2 - np.arctan(Y/X)
phi_sensor_rad[X > 0] += np.pi
phi_sensor_rad[(X == 0) & (Y < 0)] = np.pi
phi_sensor_rad[(X == 0) & (Y == 0)] = 0
phi_sensor_rad[phi_sensor_rad < 0] += 2*np.pi

deg2rad = 1 / rad2deg 
phi_sensor_deg = phi_sensor_rad * deg2rad

# spherical coordinate (or actually total tilt of the sensor, i.e. 0 when horizontal)
theta_sensor_rad = np.arccos(Z / (X**2 + Y**2 + Z**2)**0.5) 
theta_sensor_deg = theta_sensor_rad * rad2deg

## Offset correction (determine offset yourself using data for solar
## zenith angles larger than 110 deg) I actually don't do this as it
## shouldn't improve accuracy for well calibrated instruments
# ;ds['dsr'] = ds['dsr'] - ds['dwr_offset']
# ;SRout = SRout - SRout_offset

# Calculating zenith and hour angle of the sun
doy = ds['time'].to_dataframe().index.dayofyear.values
hour = ds['time'].to_dataframe().index.hour.values
minute = ds['time'].to_dataframe().index.minute.values
lat = ds['gps_lat']
lon = ds['gps_lon']

d0_rad = 2 * np.pi * (doy + (hour + minute / 60) / 24 -1) / 365

Declination_rad = np.arcsin(0.006918 - 0.399912 * np.cos(d0_rad) + 0.070257 * np.sin(d0_rad) - 0.006758 * np.cos(2 * d0_rad) + 0.000907 * np.sin(2 * d0_rad) - 0.002697 * np.cos(3 * d0_rad) + 0.00148 * np.sin(3 * d0_rad))

HourAngle_rad = 2 * np.pi * (((hour + minute / 60) / 24 - 0.5) - lon/360)
# ; - 15.*timezone/360.) ; NB: Make sure time is in UTC and longitude is positive when west! Hour angle should be 0 at noon.

# This is 180 deg at noon (NH), as opposed to HourAngle.
DirectionSun_deg = HourAngle_rad * 180/np.pi - 180

DirectionSun_deg[DirectionSun_deg < 0] += 360
DirectionSun_deg[DirectionSun_deg < 0] += 360

ZenithAngle_rad = np.arccos(np.cos(lat * np.pi/180) * np.cos(Declination_rad) * np.cos(HourAngle_rad) + np.sin(lat * np.pi/180) * np.sin(Declination_rad))

ZenithAngle_deg = ZenithAngle_rad * rad2deg

sundown = ZenithAngle_deg >= 90
dwr_toa = 1372 * np.cos(ZenithAngle_rad) # DWR at the top of the atmosphere
dwr_toa[sundown] = 0

# Calculating the correction factor for direct beam radiation
# http://solardat.uoregon.edu/SolarRadiationBasics.html
CorFac = np.sin(Declination_rad) * np.sin(lat * deg2rad) * np.cos(theta_sensor_rad) - np.sin(Declination_rad) * np.cos(lat * deg2rad) * np.sin(theta_sensor_rad) * np.cos(phi_sensor_rad + np.pi) + np.cos(Declination_rad) * np.cos(lat * deg2rad) * np.cos(theta_sensor_rad) * np.cos(HourAngle_rad) + np.cos(Declination_rad) * np.sin(lat * deg2rad) * np.sin(theta_sensor_rad) * np.cos(phi_sensor_rad + np.pi) * np.cos(HourAngle_rad) + np.cos(Declination_rad) * np.sin(theta_sensor_rad) * np.sin(phi_sensor_rad + np.pi) * np.sin(HourAngle_rad)

CorFac = np.cos(ZenithAngle_rad) / CorFac
# sun out of field of view upper sensor
CorFac[(CorFac < 0) | (ZenithAngle_deg > 90)] = 1

# Calculating ds['dsr'] over a horizontal surface corrected for station/sensor tilt
CorFac_all = CorFac / (1 - DifFrac + CorFac * DifFrac)
ds['dwr_cor'] = ds['dsr'] * CorFac_all


# Calculating albedo based on albedo values when sun is in sight of the upper sensor
AngleDif_deg = 180 / np.pi * np.arccos(np.sin(ZenithAngle_rad) * np.cos(HourAngle_rad + np.pi) * np.sin(theta_sensor_rad) * np.cos(phi_sensor_rad) + np.sin(ZenithAngle_rad) * np.sin(HourAngle_rad + np.pi) * np.sin(theta_sensor_rad) * np.sin(phi_sensor_rad) + np.cos(ZenithAngle_rad) * np.cos(theta_sensor_rad)) # angle between sun and sensor

# ;AngleDif_deg = 180./!pi*acos(cos(!pi/2.-ZenithAngle_rad)*cos(!pi/2.-theta_sensor_rad)*cos(HourAngle_rad-phi_sensor_rad)+sin(!pi/2.-ZenithAngle_rad)*sin(!pi/2.-theta_sensor_rad)) ; angle between sun and sensor

albedo = ds['usr'] / ds['dwr_cor']
OKalbedos = (AngleDif_deg < 70) & (ZenithAngle_deg < 70) & (albedo < 1) & (albedo > 0)
albedo[~OKalbedos] = np.nan

# ;OKalbedos = where(angleDif_deg lt 82.5 and ZenithAngle_deg lt 70 and albedo lt 1 and albedo gt 0, complement=notOKalbedos)
# ;The running mean calculation doesn't work for non-continuous data sets or variable temporal resolution (e.g. with multiple files)
# ;albedo_rm = 0*albedo
# ;albedo_rm[OKalbedos] = smooth(albedo[OKalbedos],obsrate+1,/edge_truncate) ; boxcar average of reliable albedo values
# ;albedo[notOKalbedos] = interpol(albedo_rm[OKalbedos],OKalbedos,notOKalbedos) ; interpolate over gaps
# ;albedo_rm[notOKalbedos] = albedo[notOKalbedos]
# ;So instead:

# albedo[notOKalbedos] = interpol(albedo[OKalbedos],OKalbedos,notOKalbedos) ; interpolate over gaps - gives problems for discontinuous data sets, but is not the end of the world

# Correcting SR using DWR when sun is in field of view of lower sensor assuming sensor measures only diffuse radiation
sunonlowerdome =(AngleDif_deg >= 90) & (ZenithAngle_deg <= 90)
ds['dwr_cor'][sunonlowerdome] = ds['dsr'][sunonlowerdome] / DifFrac[sunonlowerdome]
ds['uswr_cor'] = ds['usr']
ds['uswr_cor'][sunonlowerdome] = albedo * ds['dsr'][sunonlowerdome] / DifFrac[sunonlowerdome]

# Setting DWR and USWR to zero for solar zenith angles larger than 95 deg or either DWR or USWR are (less than) zero
bad = (ZenithAngle_deg > 95) | (ds['dwr_cor'] <= 0) | (ds['uswr_cor'] <= 0)
ds['dwr_cor'][bad] = 0
ds['uswr_cor'][bad] = 0

# Correcting DWR using more reliable USWR when sun not in sight of upper sensor
ds['dwr_cor'] = ds['uswr_cor'] / albedo
# albedo[~OKalbedos] = np.nan
# albedo[OKalbedos[n_elements(OKalbedos)-1]:*] = -999 ; Removing albedos that were extrapolated (as opposed to interpolated) at the end of the time series - see above
# ds['dsr']_cor[OKalbedos[n_elements(OKalbedos)-1]:*] = -999 ; Removing the corresponding ds['dsr']_cor as well
# ds['uswr_cor'][OKalbedos[n_elements(OKalbedos)-1]:*] = -999 ; Removing the corresponding ds['uswr_cor'] as well

# ; Removing spikes by interpolation based on a simple top-of-the-atmosphere limitation
#      TOA_crit_nopass = where(ds['dsr']_cor gt 0.9*dwr_toa+10)
#      TOA_crit_pass = where(ds['dsr']_cor le 0.9*dwr_toa+10)
#      if total(TOA_crit_nopass) ne -1 then begin
#         ds['dsr']_cor[TOA_crit_nopass] = interpol(ds['dsr']_cor[TOA_crit_pass],TOA_crit_pass,TOA_crit_nopass)
#         ds['uswr_cor'][TOA_crit_nopass] = interpol(ds['uswr_cor'][TOA_crit_pass],TOA_crit_pass,TOA_crit_nopass)
#      endif
TOA_crit_nopass = (ds['dwr_cor'] > (0.9 * dwr_toa + 10))
ds['dwr_cor'][TOA_crit_nopass] = np.nan
ds['uswr_cor'][TOA_crit_nopass] = np.nan

# print,'- Sun in view of upper sensor / workable albedos:',n_elements(OKalbedos),100*n_elements(OKalbedos)/n_elements(ds['dsr']),'%'
# print,'- Sun below horizon:',n_elements(sundown),100*n_elements(sundown)/n_elements(ds['dsr']),'%'
# print,'- Sun in view of lower sensor:',n_elements(sunonlowerdome),100*n_elements(sunonlowerdome)/n_elements(ds['dsr']),'%'
# print,'- Spikes removed using TOA criteria:',n_elements(TOA_crit_nopass),100*n_elements(TOA_crit_nopass)/n_elements(ds['dsr']),'%'
# print,'- Mean net SR change by corrections:',total(ds['dsr']_cor-ds['uswr_cor']-ds['dsr']+SRout)/n_elements(ds['dsr']),' W/m2'
     
#+END_SRC

#+RESULTS:


*** Wind direction

+ GitHub: [[./IDL/AWSdataprocessing_v3.pro#L423]]
+ Org Mode: [[./IDL/AWSdataprocessing_v3.pro::423]]
    
#+BEGIN_SRC jupyter-python

ds['wspd_x'] = ds['wspd'] * np.sin(ds['wdir'] * deg2rad)
ds['wspd_y'] = ds['wspd'] * np.cos(ds['wdir'] * deg2rad)

# adjust properties
#+END_SRC

#+RESULTS:


** Export file as L2

+ Check with ~cfchecks ./data/L2/EGP/EGP-raw.nc~

#+BEGIN_SRC jupyter-python
outpath = os.path.split(infile)[0].split("/")
outpath[-2] = 'L2'
outpath = '/'.join(outpath)
outfile = os.path.basename(infile)
outpathfile = outpath + '/' + outfile
if os.path.exists(outpathfile): os.remove(outpathfile)
ds.to_netcdf(outpathfile, mode='w', format='NETCDF4', compute=True)
#+END_SRC

#+RESULTS:





* L2 -> L3
:PROPERTIES:
:header-args:jupyter-python+: :session L2_to_L3 :noweb-ref L2_to_L3 :noweb yes
:END:

+ Derived values
  + [ ] Cloud cover
  + [ ] Wind direction components
  + [ ] Turbulent heat flux

** Wrapper

Run one:
#+BEGIN_SRC jupyter-python :noweb-ref
infile = "./data/L2/EGP/EGP-raw.nc"
<<L2_to_L3>>
#+END_SRC

#+RESULTS:
:RESULTS:
# [goto error]
:   File "<tokenize>", line 78
:     endfor
:     ^
: IndentationError: unindent does not match any outer indentation level
:END:

Run all:

#+BEGIN_SRC bash
# conda activate PROMICE_dev

./L2_to_L3.py data/L2/EGP/*raw.nc
./L2_to_L3.py data/L2/EGP/*STM.nc
./L2_to_L3.py data/L2/EGP/*TX.nc
#+END_SRC


#+BEGIN_SRC jupyter-python :tangle L2_to_L3.py :noweb-ref :tangle-mode (identity #o744)
#!/usr/bin/env python

def L2_to_L3(infile=None):
    <<L2_to_L3>>


if __name__ == "__main__":
    import sys
    L2_to_L3(sys.argv[1])
#+END_SRC

** Imports

#+BEGIN_SRC jupyter-python
import xarray as xr
import os

<<constants>>
#+END_SRC

#+RESULTS:

** Load
#+BEGIN_SRC jupyter-python :exports both
ds = xr.open_dataset(infile, mask_and_scale=False).load()
# print(ds)
#+END_SRC

#+RESULTS:

** Derived properties

*** NOTDONE COMMENT Turbulent heat flux

+ GitHub: [[./IDL/AWSdataprocessing_v3.pro#L866]]
+ Org Mode: [[./IDL/AWSdataprocessing_v3.pro::866]]


+ Requires hourly averages

Constants

#+BEGIN_SRC jupyter-python
z_0    =    0.001    # aerodynamic surface roughness length for momention (assumed constant for all ice/snow surfaces)
eps    =    0.622
es_0   =    6.1071   # saturation vapour pressure at the melting point (hPa)
es_100 = 1013.246    # saturation vapour pressure at steam point temperature (hPa)
g      =    9.82     # gravitational acceleration (m/s2)
gamma  =   16.       # flux profile correction (Paulson & Dyer)
kappa  =    0.4      # Von Karman constant (0.35-0.42)
L_sub  =    2.83e6   # latent heat of sublimation (J/kg)
R_d    =  287.05     # gas constant of dry air
aa     =    0.7      # flux profile correction constants (Holtslag & De Bruin '88)
bb     =    0.75
cc     =    5.
dd     =    0.35
c_pd   = 1005.       # specific heat of dry air (J/kg/K)
WS_lim =    1.
L_dif_max = 0.01


T_0 = 273.15
#+END_SRC

#+RESULTS:

#+BEGIN_SRC jupyter-python
# ds_h = ds.resample({'time':"1H"}).mean() # this takes ~2-3 minuteso

T_h = ds_h['t_1']

z_WS = ds_h['z_boom'] + 0.4  # height of W
z_T = ds_h['z_boom'] - 0.1   # height of thermometer
rho_atm = 100 * ds_h['p'] / R_d / (T_h + T_0)   # atmospheric density

# dynamic viscosity of air (Pa s) (Sutherlands' equation using C = 120 K)
mu = 18.27e-6 * (291.15 + 120) / ((T_h + T_0) + 120) * ((T_h + T_0) / 291.15)**1.5

nu = mu / rho_atm                                                   # kinematic viscosity of air (m^2/s)
u_star = kappa * WS_h / np.log(z_WS / z_0)
     Re = u_star*z_0/nu
     z_0h = z_0*exp(1.5-0.2*alog(Re)-0.11*(alog(Re))^2) ; rough surfaces: Smeets & Van den Broeke 2008
     z_0h[where(WS_h le 0)] = 1e-10
     es_ice_surf = 10.^(-9.09718*(T_0/(Tsurf_h+T_0)-1.) - 3.56654*ALOG10(T_0/(Tsurf_h+T_0))+0.876793*(1.-(Tsurf_h+T_0)/T_0) + ALOG10(es_0))
     q_surf = eps*es_ice_surf/(p_h-(1-eps)*es_ice_surf)
     es_wtr = 10.^(-7.90298*(T_100/(T_h+T_0)-1.) + 5.02808 * ALOG10(T_100/(T_h+T_0)) $ ; saturation vapour pressure above 0 C (hPa)
                   - 1.3816E-7 * (10.^(11.344*(1.-(T_h+T_0)/T_100))-1.) $
                   + 8.1328E-3*(10.^(-3.49149*(T_100/(T_h+T_0)-1)) -1.) + ALOG10(es_100))
     es_ice = 10.^(-9.09718 * (T_0 / (T_h+T_0) - 1.) - 3.56654 * ALOG10(T_0 / (T_h+T_0)) + 0.876793 * (1. - (T_h+T_0) / T_0) + ALOG10(es_0)) ; saturation vapour pressure below 0 C (hPa)
     q_sat = eps * es_wtr/(p_h-(1-eps)*es_wtr) ; specific humidity at saturation (incorrect below melting point)
     freezing = where(T_h lt 0)                ; replacing saturation specific humidity values below melting point
     q_sat[freezing] = eps * es_ice[freezing]/(p_h[freezing]-(1-eps)*es_ice[freezing])
     q_h = RH_cor_h*q_sat/100   ; specific humidity in kg/kg
     theta = T_h + z_T*g/c_pd
     SHF_h = T_h & SHF_h[*] = 0 & LHF_h = SHF_h & L = SHF_h+1e5

     stable   = where(theta ge Tsurf_h and WS_h gt WS_lim and T_h ne -999 and Tsurf_h ne -999 and RH_cor_h ne -999 and p_h ne -999 and Haws_h ne -999)
     unstable = where(theta lt Tsurf_h and WS_h gt WS_lim and T_h ne -999 and Tsurf_h ne -999 and RH_cor_h ne -999 and p_h ne -999 and Haws_h ne -999)
;no_wind  = where( WS_h ne -999    and WS_h le WS_lim and T_h ne -999 and Tsurf_h ne -999 and RH_cor_h ne -999 and p_h ne -999 and Haws_h ne -999)

     for i=0,30 do begin        ; stable stratification
        psi_m1 = -(aa*         z_0/L[stable] + bb*(         z_0/L[stable]-cc/dd)*exp(-dd*         z_0/L[stable]) + bb*cc/dd)
        psi_m2 = -(aa*z_WS[stable]/L[stable] + bb*(z_WS[stable]/L[stable]-cc/dd)*exp(-dd*z_WS[stable]/L[stable]) + bb*cc/dd)
        psi_h1 = -(aa*z_0h[stable]/L[stable] + bb*(z_0h[stable]/L[stable]-cc/dd)*exp(-dd*z_0h[stable]/L[stable]) + bb*cc/dd)
        psi_h2 = -(aa* z_T[stable]/L[stable] + bb*( z_T[stable]/L[stable]-cc/dd)*exp(-dd* z_T[stable]/L[stable]) + bb*cc/dd)
        u_star[stable] = kappa*WS_h[stable]/(alog(z_WS[stable]/z_0)-psi_m2+psi_m1)
        Re[stable] = u_star[stable]*z_0/nu[stable]
        z_0h[stable] = z_0*exp(1.5-0.2*alog(Re[stable])-0.11*(alog(Re[stable]))^2)
        if n_elements(where(z_0h[stable] lt 1e-6)) gt 1 then z_0h[stable[where(z_0h[stable] lt 1e-6)]] = 1e-6
        th_star = kappa*(theta[stable]-Tsurf_h[stable])/(alog(z_T[stable]/z_0h[stable])-psi_h2+psi_h1)
        q_star  = kappa*(  q_h[stable]- q_surf[stable])/(alog(z_T[stable]/z_0h[stable])-psi_h2+psi_h1)
        SHF_h[stable] = rho_atm[stable]*c_pd *u_star[stable]*th_star
        LHF_h[stable] = rho_atm[stable]*L_sub*u_star[stable]* q_star
        L_prev = L[stable]
        L[stable] = u_star[stable]^2*(theta[stable]+T_0)*(1+((1-eps)/eps)*q_h[stable])/(g*kappa*th_star*(1+((1-eps)/eps)*q_star))
        L_dif = abs((L_prev-L[stable])/L_prev)
;  print,"HF iterations stable stratification: ",i+1,n_elements(where(L_dif gt L_dif_max)),100.*n_elements(where(L_dif gt L_dif_max))/n_elements(where(L_dif))
        if n_elements(where(L_dif gt L_dif_max)) eq 1 then break
     endfor

     if n_elements(unstable) gt 1 then begin
        for i=0,20 do begin     ; unstable stratification
           x1  = (1-gamma*z_0           /L[unstable])^0.25
           x2  = (1-gamma*z_WS[unstable]/L[unstable])^0.25
           y1  = (1-gamma*z_0h[unstable]/L[unstable])^0.5
           y2  = (1-gamma*z_T[unstable] /L[unstable])^0.5
           psi_m1 = alog(((1+x1)/2)^2*(1+x1^2)/2)-2*atan(x1)+!pi/2
           psi_m2 = alog(((1+x2)/2)^2*(1+x2^2)/2)-2*atan(x2)+!pi/2
           psi_h1 = alog(((1+y1)/2)^2)
           psi_h2 = alog(((1+y2)/2)^2)
           u_star[unstable] = kappa*WS_h[unstable]/(alog(z_WS[unstable]/z_0)-psi_m2+psi_m1)
           Re[unstable] = u_star[unstable]*z_0/nu[unstable]
           z_0h[unstable] = z_0*exp(1.5-0.2*alog(Re[unstable])-0.11*(alog(Re[unstable]))^2)
           if n_elements(where(z_0h[unstable] lt 1e-6)) gt 1 then z_0h[unstable[where(z_0h[unstable] lt 1e-6)]] = 1e-6
           th_star = kappa*(theta[unstable]-Tsurf_h[unstable])/(alog(z_T[unstable]/z_0h[unstable])-psi_h2+psi_h1)
           q_star  = kappa*(  q_h[unstable]- q_surf[unstable])/(alog(z_T[unstable]/z_0h[unstable])-psi_h2+psi_h1)
           SHF_h[unstable] = rho_atm[unstable]*c_pd *u_star[unstable]*th_star
           LHF_h[unstable] = rho_atm[unstable]*L_sub*u_star[unstable]* q_star
           L_prev = L[unstable]
           L[unstable] = u_star[unstable]^2*(theta[unstable]+T_0)*(1+((1-eps)/eps)*q_h[unstable])/(g*kappa*th_star*(1+((1-eps)/eps)*q_star))
           L_dif = abs((L_prev-L[unstable])/L_prev)
;    print,"HF iterations unstable stratification: ",i+1,n_elements(where(L_dif gt L_dif_max)),100.*n_elements(where(L_dif gt L_dif_max))/n_elements(where(L_dif))
           if n_elements(where(L_dif gt L_dif_max)) eq 1 then break
        endfor
     endif

     q_h = 1000.*q_h            ; from kg/kg to g/kg
;no_q = where(p_h eq -999 or T_h eq -999 or RH_cor_h eq -999)
;if total(no_q) ne -1 then q_h[no_q] = -999
;no_HF  = where(p_h eq -999 or T_h eq -999 or Tsurf_h eq -999 or WS_h eq -999 or Haws_h eq -999)
     no_HF = where(p_h eq -999 or T_h eq -999 or Tsurf_h eq -999 or RH_cor_h eq -999 or WS_h eq -999 or Haws_h eq -999)
     no_qh = where(T_h eq -999 or RH_cor_h eq -999 or p_h eq -999 or Tsurf_h eq -999)
;no_SHF = where(p_h eq -999 or T_h eq -999 or Tsurf_h eq -999 or WS_h eq -999 or Haws_h eq -999)
;no_LHF = where(p_h eq -999 or T_h eq -999 or Tsurf_h eq -999 or RH_cor_h eq -999 or WS_h eq -999 or Haws_h eq -999)
;if total(no_LHF) ne -1 then LHF_h[no_LHF] = -999
;if total(no_SHF) ne -1 then SHF_h[no_SHF] = -999
     if total(no_HF) ne -1 then begin
        SHF_h[no_HF] = -999
        LHF_h[no_HF] = -999
     endif
     if total(no_qh) ne -1 then begin
        q_h[no_qh] = -999
     endif
;print,'size q_h: ',size(q_h)

#+END_SRC


** Downsample to hourly and daily

Downsampling should be 1 line
#+BEGIN_SRC jupyter-python :noweb-ref nil
ds_h = ds.resample({'time':"1H"}).mean() # this takes ~2-3 minutes
ds_d = ds.resample({'time':"1D"}).mean()
#+END_SRC
But due to xarray implementation, this takes several minutes, while it takes << 1 second in Pandas.
See https://github.com/pydata/xarray/issues/4498

Therefore, we do downsampling in Pandas (for now) even though the code is more complex.

#+BEGIN_SRC jupyter-python
df_h = ds.to_dataframe().resample("1H").mean()  # what we want (quickly), but in Pandas form
# now, rebuild xarray dataset (https://www.theurbanist.com.au/2020/03/how-to-create-an-xarray-dataset-from-scratch/)
vals = [xr.DataArray(data=df_h[c], dims=['time'], coords={'time':df_h.index}, attrs=ds[c].attrs) for c in df_h.columns]
ds_h = xr.Dataset(dict(zip(df_h.columns,vals)), attrs=ds.attrs)


df_d = ds.to_dataframe().resample("1D").mean()  # what we want (quickly), but in Pandas form
# now, rebuild xarray dataset (https://www.theurbanist.com.au/2020/03/how-to-create-an-xarray-dataset-from-scratch/)
vals = [xr.DataArray(data=df_d[c], dims=['time'], coords={'time':df_d.index}, attrs=ds[c].attrs) for c in df_d.columns]
ds_d = xr.Dataset(dict(zip(df_d.columns,vals)), attrs=ds.attrs)
#+END_SRC

#+RESULTS:


** Export file as L3

+ Check with ~cfchecks ./data/L2/EGP/EGP-raw.nc~

#+BEGIN_SRC jupyter-python
outpath = os.path.split(infile)[0].split("/")
outpath[-2] = 'L3'
outpath = '/'.join(outpath)
outfile_base = os.path.splitext(os.path.basename(infile))[0]
outpathfile = outpath + '/' + outfile_base

if os.path.exists(outpathfile+"_hour.nc"): os.remove(outpathfile+"_hour.nc")
ds_h.to_netcdf(outpathfile+"_hour.nc", mode='w', format='NETCDF4', compute=True)
ds_h.to_dataframe().dropna(how='all').to_csv(outpathfile+"_hour.csv")

if os.path.exists(outpathfile+"_day.nc"): os.remove(outpathfile+"_day.nc")
ds_d.to_netcdf(outpathfile+"_day.nc", mode='w', format='NETCDF4', compute=True)
ds_d.to_dataframe().dropna(how='all').to_csv(outpathfile+"_day.csv")
#+END_SRC

#+RESULTS:






* L0M to L3

#+BEGIN_SRC bash :tangle ppp.sh :tangle-mode (identity #o744) :var s="CEN"

# conda activate PROMICE_dev

# L0M -> L1
# for f in $(ls ./data/L0M/${s}/*); do ./L0_to_L1.py ${f}; done
parallel "./L0_to_L1.py {}" ::: $(ls ./data/L0M/${s}/*)

# L1 -> L1A
# ./L1_to_L1A.py data/L1/${s}/*raw.nc
# ./L1_to_L1A.py data/L1/${s}/*STM.nc
# ./L1_to_L1A.py data/L1/${s}/*TX.nc
parallel "./L1_to_L1A.py {}" ::: $(ls data/L1/${s}/*)

# L1A to L2
# ./L1A_to_L2.py data/L1A/${s}/${s}-raw.nc
# ./L1A_to_L2.py data/L1A/${s}/${s}-STM.nc
# ./L1A_to_L2.py data/L1A/${s}/${s}-TX.nc
parallel "./L1A_to_L2.py {}" ::: $(ls data/L1A/${s}/*)

# L2 to L3
# ./L2_to_L3.py data/L2/${s}/*raw.nc
# ./L2_to_L3.py data/L2/${s}/*STM.nc
# ./L2_to_L3.py data/L2/${s}/*TX.nc
parallel "./L2_to_L3.py {}" ::: $(ls data/L2/${s}/*)

#+END_SRC


* Helper functions

#+NAME: constants
#+BEGIN_SRC jupyter-python
import numpy as np

deg2rad = np.pi / 180
rad2deg = 1 / deg2rad
#+END_SRC

* Compare Python & IDL
:PROPERTIES:
:header-args:jupyter-python+: :session compare
:END:

#+BEGIN_SRC jupyter-python
import pandas as pd

py = pd.read_csv("./data/L3/EGP/EGP-raw_hour.csv", index_col=0, parse_dates=True)


def mydf(y,m,d,h):
    return pd.datetime(int(y),int(m),int(d),int(h))

idl = pd.read_csv("./IDL/data/out/EGP_hour_v03.txt",
                  delimiter="\s+",
                  parse_dates={'time':[0,1,2,3]},
                  infer_datetime_format=True,
                  date_parser=mydf,
                  index_col=0)


# t
clf()
ax = py['t_1'].plot(linewidth=3, label='v4')
idl['AirTemperature(C)'].plot(ax=ax, label='v3')
legend()

# dwr
clf()
ax = py['dsr'].plot(linewidth=3, label='v4')
idl['ShortwaveRadiationDown(W/m2)'].plot(ax=ax, label='v3')
legend()
#+END_SRC

#+RESULTS:
:RESULTS:
: <ipython-input-37-4f0e9ef3a40c>:7: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.
:   return pd.datetime(int(y),int(m),int(d),int(h))
: <matplotlib.legend.Legend at 0x7f9512cac040>
:END:

