
#+PROPERTY: header-args:jupyter-python :kernel PROMICE_dev :session PROMICE-README :exports both
#+PROPERTY: header-args:bash :exports both

* Table of contents                               :toc_3:noexport:
- [[#introduction][Introduction]]
  - [[#overview][Overview]]
- [[#level-0][Level 0]]
  - [[#l0-files][L0 files]]
    - [[#additional-files][Additional files]]
    - [[#l0-reader][L0 Reader]]
- [[#l0---l1][L0 -> L1]]
  - [[#wrapper][Wrapper]]
  - [[#imports][Imports]]
  - [[#read-in-file][Read in file]]
  - [[#eng-to-phys][Eng to phys]]
  - [[#export-file-as-l1][Export file as L1]]
- [[#level-1][Level 1]]
- [[#l1---l1a][L1 -> L1A]]
  - [[#wrapper-1][Wrapper]]
  - [[#imports-1][Imports]]
  - [[#merge-files][Merge files]]
  - [[#flag-data][Flag data]]
  - [[#export-file-as-l1a][Export file as L1A]]
- [[#level-1a][Level 1A]]
- [[#l1a---l2][L1A -> L2]]
  - [[#wrapper-2][Wrapper]]
  - [[#imports-2][Imports]]
  - [[#init][Init]]
  - [[#load][Load]]
  - [[#calibrate-using-secondary-sensors][Calibrate using secondary sensors]]
    - [[#correct-relative-humidity][Correct relative humidity]]
    - [[#cloud-cover][Cloud cover]]
    - [[#correct-shortwave-radiation][Correct shortwave radiation]]
    - [[#wind-direction][Wind direction]]
    - [[#cleaning][Cleaning]]
  - [[#export-file-as-l2][Export file as L2]]
- [[#l2---l3][L2 -> L3]]
  - [[#wrapper-3][Wrapper]]
  - [[#imports-3][Imports]]
  - [[#load-1][Load]]
  - [[#downsample-to-hourly-and-daily][Downsample to hourly and daily]]
    - [[#circular-averaging][Circular averaging]]
  - [[#derived-properties][Derived properties]]
    - [[#turbulent-heat-flux][Turbulent heat flux]]
  - [[#export-file-as-l3][Export file as L3]]
- [[#l0-to-l3][L0 to L3]]
- [[#helper-functions][Helper functions]]
  - [[#load-l0-hdr--data][Load L0 hdr + data]]
  - [[#flag-invalid-data][Flag invalid data]]
  - [[#add-variable-metadata][Add variable metadata]]
  - [[#metadatacsv-to-toml][metadata.csv to TOML]]
- [[#compare-python--idl][Compare Python & IDL]]
  - [[#load-both-to-dfs-10-min][Load both to dfs (10 min)]]
  - [[#compare-2][Compare 2]]

* Introduction

Code used to process the PROMICE AWS data from Level 0 through Level 3 (end-user product).

We use the following processing levels, described textually and graphically.

** Overview
+ L0: Raw data in CSV file format in one of three formats:
  + =raw=, =STM= (Slim Table Memory), and =TX= (transmitted)
  + Manually split so no file includes changed sensors
  + Manually created paired header files based on [[./hdr.template]]
+ L1:
  + Engineering units (e.g. current or volts) converted to physical units (e.g. temperature or wind speed)
+ L1A:
  + Invalid / bad / suspicious data flagged
  + Files merged to one time series per station
+ L2:
  + Calibration using secondary sources (e.g. radiometric correction requires input of tilt sensor)
+ L3:
  + Derived products (e.g. SHF and LHF)
  + Merge formats to one product here?

#+begin_src ditaa :file ./fig/levels.png :exports results

                    +----------------+
	            |{d}             |                         Legend
                    | Digital counts |                         +---------------+
                    |                |                         |input          |
		    | CR-1000 logger |                         +---------------+
	            |                |
	            +-------+--------+                         +---------------+   +=----+
	                    |				       |{io}process    +--=+ Note|
	                    v				       +---------------+   +-----+
                    +----------------+
	            |{io}            |                         +---------------+
                    |  Manual Carry  |      		       |{d}Files       |
                    |      or        |      		       +---------------+
		    |   Satellite    |
	            |                |			
	            +-------+--------+
	                    |                               +------------------+
	                    v         			  +-+Column names      |
                    +----------------+   +------------+   | +------------------+
	            |{d}             |   |{d}         |<--+
                    |  raw, STM, TX  |   |            |	    +------------------+
     Level 0 (L0)   |                |   |  L0 header |<----+Metadata          |
		    | GEUS text files|	 |            |	    +------------------+
	            |                |	 |            |<--+
	            +-------+--------+   +--+---------+   | +-----------------------------------+
	                    |               |	          +-+ Instrument calibration parameters |
                            |               |		    |      (recorded, not applied)      |
			    |  	+-----------+               +-----------------------------------+
	                    |	|			    
	                    v   v			    
	            +-----------------+           	            
	            |{io}             |                         
	            |  Engineering to |   	   	        
	            |  physical units |                         
	            |                 |   
                    +-------+---------+   
		            |      	  
	                    v             
                    +-----------------+   
		    |{d}              |   
    Level 1 (L1)    |Measured physical|   
		    |    properties   |
		    |                 |
		    +-------+---------+	  
                            |		  
                            v		  
                    +-----------------+
                    |{io}             |
                    |   Flag bad data |
                    |   Merge files   |
                    |                 |
                    +-------+---------+
                            |           
                            v          
                   +-------------------+
                   |{d}                |
    Level 1A (L1A) |Time series per AWS|
                   |  Initial data QC  |
		   |                   |
                   +-------+-----------+
                           |
                           v
                    +-----------------+
                    |{io}             |       +=------------------------------------------+ 
                    | Cross-sensor    |------=+e.g. ice at 1 m depth via interpolation, or| 
                    |  corrections    |       |radiation adjusting for platform rotation  |
                    |                 |       +-------------------------------------------+ 
                    +-------+---------+       
                            |          
                            v          
                   +-------------------+
                   |{d}                |
     Level 2 (L2)  |  Derived internal |
                   |      values       |
	           |                   |
                   +-------+-----------+
                           |
                           v
                    +-----------------+
                    |{io}             |
                    |     Derive      |       +=-----------------------+
                    |    external     |------=+e.g. sensible heat flux,|
                    |   properties    |       |latent heat flux        |
                    |                 |       +------------------------+
                    +-------+---------+
                            |          
                            v          
                   +-------------------+
                   |{d}                |
     Level 3 (L3)  |  Derived external |
                   |      values       |
		   |                   |
                   +-------------------+


#+END_SRC
		    
#+RESULTS:
[[file:./fig/levels.png]]

* Level 0

Level 0 is generated from one of three methods:
+ Copied from CF card in the field
+ Downloaded from logger box in the field
+ Transmitted via satellite and processed by https://github.com/GEUS-PROMICE/awsrx.

#+begin_src plantuml :file ./fig/L00_to_L0.png :exports results
@startuml

' plantuml activity diagram (beta)

component Sensor_1
component Sensor_n

frame CR_Logger {
  database DB_logger [
  <b>Database</b>
  10 minute sampling
  ----
  var0, var1, ..., varn
] 
}

Sensor_1 --> CR_Logger
Sensor_n --> CR_Logger

node GEUS_(Level_0) {
  file Raw [
  <b>raw</b>
  10 min sampling
  ]

  file SlimTableMem [
  <b>SlimTableMem</b>
  Hourly average from
  10 min sampling
  ]

  file TX [
  <b>TX</b>
  V3:
    DOY 100 to 300: hourly average
    DOY 300 to 100: daily average
  V4:
    hourly average all days
  ]
}

' DB -> hand carry -> raw
actor Scientist
DB_logger --> Scientist : Field\ndownload
Scientist --> Raw : Hand\ncarry
Scientist --> SlimTableMem : Hand\ncarry

' DB -> satellite -> Transmitted
cloud Satellite
file Email
queue awsrx
note right
   https://github.com/GEUS-PROMICE/awsrx
end note

DB_logger -[dashed]-> Satellite : Data subsampled and\npossible transmission loss
Satellite -[dashed]-> Email
Email --> awsrx
awsrx --> TX

@enduml
#+end_src

#+RESULTS:
[[file:./fig/L00_to_L0.png]]

** L0 files

+ =raw= : All 10-minute data stored on the CF-card (external module on CR logger)
+ =SlimTableMem= : Hourly averaged 10-min data stored in the internal logger memory
+ =transmitted= : Transmitted via satellite. Only a subset of data is transmitted, and only hourly or daily average depending on station and day of year.

Level 0 files are stored in the =data/L0/<S>/= folder, where =<S>= is the station name. File names should encode the station, end-of-year of download, a version number if there are multiple files for a given year, and the format. Best practices would use the following conventions:  

=data/<L>/<S>/<S>_<Y>[.<n>]_<F>.txt=

Where 

+ =<L>= is the processing level
  + =<L>= must be one of the following: [L0, L1, L1A, L2, L3]
+ =<S>= is a station ID
  + =<S>= must be one of the following strings: [CEN, EGP, KAN_B, KAN_L, KAN_M, KAN_U, KPC_L, KPC_U, MIT, NUK_K, NUK_L, NUK_N, NUK_U, QAS_A, QAS_L, QAS_M, QAS_U, SCO_L, SCO_U, TAS_A, TAS_L, TAS_U, THU_L, THU_U, UPE_L, UPE_U]
+ =<Y>= is a four-digit year with a value greater than =2008=
  + =<Y>= should represent the year at the last timestamp in the file
  + Optionally, =.<n>= is a version number if multiple files from the same year are present
+ =<F>= is the format, one of =raw=, =TX=, or =STM=

Each L0 file that will be processed must have an entry in the TOML-formatted configuration file. The config file can be located anywhere, and the processing script receives the config file and the location of the L0 data. An [[./example.toml][example (template) L0 config file]] is:

#+BEGIN_SRC bash :results verbatim :exports results
cat example.toml
#+END_SRC

#+RESULTS:
#+begin_example
# EGP L0 config

station_id         = "EGP"
latitude           = 75.62
longitude          = -35.98
nodata             = ['-999', 'NAN'] # if one is a string, all must be strings
dsr_eng_coef       = 12.71  # from manufacturer to convert from eng units (1E-5 V) to  physical units (W m-2)
usr_eng_coef       = 12.71
dlr_eng_coef       = 12.71
ulr_eng_coef       = 12.71
hygroclip_t_offset = 0      # degrees C

columns = ["time", "rec", "min_y",
	"p", "t_1", "t_2", "rh", "wspd", "wdir", "wd_std",
	"dsr", "usr", "dlr", "ulr", "t_rad",
	"z_boom", "z_boom_q", "z_stake", "z_stake_q", "z_pt",
	"t_i_1", "t_i_2", "t_i_3", "t_i_4", "t_i_5", "t_i_6", "t_i_7", "t_i_8",
	"tilt_x", "tilt_y",
	"gps_time", "gps_lat", "gps_lon", "gps_alt", "gps_geoid", "gps_geounit", "gps_q", "gps_numsat", "gps_hdop",
	"t_log", "fan_dc", "batt_v_ss", "batt_v"]

["EGP_2016_raw.txt"]
format    = "raw"
skiprows  = 3

["EGP_2019_raw_transmitted.txt"]
skiprows = 0
format   = "TX"
columns = ["time", "rec",
	"p", "t_1", "t_2", "rh", "wspd", "wdir",
	"dsr", "usr", "dlr", "ulr", "t_rad",
	"z_boom", "z_stake", "z_pt",
	"t_i_1", "t_i_2", "t_i_3", "t_i_4", "t_i_5", "t_i_6", "t_i_7", "t_i_8",
	"tilt_x", "tilt_y",
	"gps_time", "gps_lat", "gps_lon", "gps_alt", "gps_hdop",
	"fan_dc", "batt_v"]
#+end_example

The TOML config file has the following expectations and behaviors:
+ Properties can be defined at the top level or under a section
+ Each file that will be processed gets its own section
+ Properties at the top level are copied to each section (assumed to apply to all files), but do not overwrite properties if they are already defined for a section.

In the example above,
+ The =station_id=, =latitude=, etc. properties are the same in both files (=EGP_2016_raw.txt= and =EGP_2019_raw_transmitted.txt=) and so they are defined once at the top of the file. They could have been defined in each of the sections similar to =hygroclip_t_offset=.
+ The =format= and =skiprows= properties are different in each section and defined in each section
+ The top-level defined =columns= is applied only to =EGP_2016_raw.txt= because it is defined differently in the =EGP_2019_raw_transmitted.txt= section.

*** Additional files

Any files that do not have an associated section in the config file will be ignored. However, for cleanliness, L0 files that will not be processed should be placed in an =archive= subfolder.

Any changes made to L0 files should be documented in the [[./L0/README.org]]. *Manual changes to these files should only be done when necessary*. An example of a manual change might be:

+ Raw file contains multiple years of data, including replacing sensors that have different calibration units. The file should be split so that each file only contains one version of each sensor (assuming different versions need different metadata).

*** L0 Reader

#+BEGIN_SRC jupyter-python :exports both
<<load_conf>>
<<read_L0>>

conf = load_conf("./data/L0/config/EGP_2016_raw.toml", "./data/L0")
ds = read_L0(conf[list(conf.keys())[0]])
print(ds)
#+END_SRC

#+RESULTS:
#+begin_example
<xarray.Dataset>
Dimensions:      (time: 10847)
Coordinates:
  ,* time         (time) datetime64[ns] 2016-05-01T14:30:00 ... 2016-07-19T17:...
Data variables:
    rec          (time) int64 51 52 53 54 55 ... 10893 10894 10895 10896 10897
    min_y        (time) int64 176540 176550 176560 ... 290460 290470 290480
    p            (time) float64 724.4 724.1 724.4 724.4 ... 730.8 731.2 730.7
    t_1          (time) float64 -20.1 -19.79 -19.31 ... -6.904 -6.904 -6.861
    t_2          (time) float64 -19.56 -19.11 -18.92 ... -6.866 -6.86 -6.799
    rh           (time) float64 54.1 51.7 50.23 49.51 ... 80.28 80.93 81.81
    wspd         (time) float64 1.062 0.918 0.636 0.486 ... 2.793 2.951 3.069
    wdir         (time) float64 265.1 259.2 216.8 208.4 ... 217.7 216.6 225.4
    wd_std       (time) float64 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0
    dsr          (time) float64 681.7 732.4 688.3 689.6 ... 724.7 711.4 698.8
    usr          (time) float64 518.6 559.3 531.8 534.4 ... 559.2 549.6 524.1
    dlr          (time) float64 -81.57 -102.0 -101.3 ... -135.8 -135.6 -132.4
    ulr          (time) float64 -23.97 -28.65 -33.92 ... -32.33 -32.52 -28.84
    t_rad        (time) float64 -12.78 -11.42 -9.929 ... -1.114 -1.03 -1.135
    z_boom       (time) float64 2.685 2.683 2.683 2.68 ... 2.583 2.584 2.58
    z_boom_q     (time) int64 190 192 189 187 191 191 ... 180 177 192 182 168
    z_stake      (time) float64 nan nan nan nan nan nan ... nan nan nan nan nan
    z_stake_q    (time) int64 0 0 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 0
    z_pt         (time) float64 nan nan nan nan nan nan ... nan nan nan nan nan
    t_i_1        (time) float64 nan -109.0 -109.0 ... -8.478 -8.458 -8.448
    t_i_2        (time) float64 nan nan -109.0 -109.0 ... -9.67 -9.67 -9.67
    t_i_3        (time) float64 nan -109.0 -109.0 ... -8.879 -8.859 -8.849
    t_i_4        (time) float64 nan -109.0 -109.0 ... -10.74 -10.73 -10.74
    t_i_5        (time) float64 nan -109.0 -109.0 ... -12.67 -12.67 -12.67
    t_i_6        (time) float64 nan -109.0 nan -109.0 ... -14.9 -14.9 -14.9
    t_i_7        (time) float64 nan -109.0 -109.0 nan ... -17.16 -17.16 -17.16
    t_i_8        (time) float64 nan nan -109.0 nan ... -20.75 -20.76 -20.76
    tilt_x       (time) float64 3.527 3.492 3.516 3.489 ... 0.109 0.095 0.174
    tilt_y       (time) float64 -0.945 -0.938 -0.924 ... -0.828 -0.849 -0.859
    gps_time     (time) object nan nan nan ... 'GT170007.00' 'GT170007.00'
    gps_lat      (time) object nan nan nan ... 'NH7537.47563' 'NH7537.47563'
    gps_lon      (time) object nan nan nan ... 'WH03558.49655' 'WH03558.49655'
    gps_alt      (time) float64 nan nan nan ... 2.663e+03 2.663e+03 2.663e+03
    gps_geoid    (time) float64 nan nan nan nan nan ... 41.6 41.6 41.6 41.6 41.6
    gps_geounit  (time) object nan nan nan nan nan nan ... 'M' 'M' 'M' 'M' 'M'
    gps_q        (time) float64 nan nan nan nan nan nan ... 1.0 1.0 1.0 1.0 1.0
    gps_numsat   (time) float64 nan nan nan nan nan ... 11.0 12.0 12.0 12.0 12.0
    gps_hdop     (time) float64 nan nan nan nan nan ... 0.71 0.73 0.73 0.73 0.73
    t_log        (time) float64 -12.6 -12.08 -11.65 ... -1.801 -1.735 -1.5
    fan_dc       (time) float64 137.5 141.3 142.3 141.8 ... 123.5 123.9 124.1
    batt_v_ss    (time) float64 15.52 15.81 15.79 15.81 ... 14.47 14.47 14.47
    batt_v       (time) float64 15.23 15.56 15.53 15.63 ... 14.4 14.41 14.41
Attributes:
    format:              raw
    station_id:          EGP
    latitude:            75.62
    longitude:           -35.98
    nodata:              -999
    dsr_eng_coef:        12.71
    usr_eng_coef:        12.71
    dlr_eng_coef:        12.71
    ulr_eng_coef:        12.71
    hygroclip_t_offset:  0
    conf:                ./data/L0/config/EGP_2016_raw.toml
    file:                ./data/L0/EGP/EGP_2016_raw.txt
#+end_example


* L0 -> L1
:PROPERTIES:
:header-args:jupyter-python+: :session L0_to_L1 :noweb-ref L0_to_L1 :noweb yes
:END:

+ Convert engineering units to physical units

** Wrapper

#+BEGIN_SRC jupyter-python :noweb-ref
<<load_conf>>
<<read_L0>>
#+END_SRC

#+RESULTS:


#+BEGIN_SRC jupyter-python :tangle L0_to_L1.py :noweb-ref :tangle-mode (identity #o544)
#!/usr/bin/env python

def L0_to_L1(conf=None):
    <<L0_to_L1>>

<<load_conf>>

if __name__ == "__main__":
    import sys
    assert(len(sys.argv) == 3)
    L0_path = sys.argv[1]
    conf = load_conf(sys.argv[2], L0_path)
    for k in conf.keys():
        if 'Slim' in k: continue
        # if 'transmitted' in k: continue
        L0_to_L1(conf[k])
    
#+END_SRC

** Imports

#+BEGIN_SRC jupyter-python
import re
import shapely
from shapely import geometry
import os
import sys
import numpy as np
import pandas as pd
#+END_SRC

#+RESULTS:

** Read in file

+ GitHub link: [[./IDL/AWSdataprocessing_v3.pro#L51]] through [[./IDL/AWSdataprocessing_v3.pro#L123]]
+ Org link: [[./IDL/AWSdataprocessing_v3.pro::51]] through [[./IDL/AWSdataprocessing_v3.pro::123]]
+ [X] Reads in the file

#+BEGIN_SRC jupyter-python
<<read_L0>>
ds = read_L0(conf)

ds['n'] = (('time'), np.arange(ds.time.size)+1)

<<flag_NAN>>
ds = flag_NAN(ds)

<<add_variable_metadata>>
ds = add_variable_metadata(ds)

# create variables that are missing
df = pd.read_csv("./variables.csv", index_col=0, comment="#", usecols=('field','lo','hi','OOL'))
for v in df.index:
    if v not in list(ds.variables):
        ds[v] = (('time'), np.arange(ds['time'].size)*np.nan)

if ~ds['z_pt'].isnull().all(): assert("pt_antifreeze" in ds.attrs.keys())
if 't_2' in list(ds.variables): assert("hygroclip_t_offset" in ds.attrs.keys())
#+END_SRC


** Eng to phys

+ GitHub link: [[./IDL/AWSdataprocessing_v3.pro#L116]] through [[./IDL/AWSdataprocessing_v3.pro#L408]] 
+ Org link: [[./IDL/AWSdataprocessing_v3.pro::116]] through [[./IDL/AWSdataprocessing_v3.pro::408]] 
  + [-] Calculates derived date products (day of century, etc.)
  + [ ] Adjusts start times
    + [ ] ~if slimtablemem eq 'yes' then begin ; change time stamp to start of the hour instead of end~
    + [ ] ~if transmitted eq 'yes' then begin ; change transmission time to start of the hour/day instead of end~
      + [ ] ~if line[col_season-1] eq '!W' then begin ; daily transmissions~
      + [ ] ~if line[col_season-1] eq '!S' then begin ; hourly transmissions~
      + [ ] Makes guesses if season identifier not transmitted
  + [X] Adjusts UTC offset
  + [X] Remove HygroClip temperature offset
  + [X] Reads and adjusts SRin ~SRin = [SRin,float(line[col_SRin-1])*10/C_SRin] ; Calculating radiation (10^-5 V -> W/m2)~
  + [X] SRout
  + [X] LRin: ~LRin = [LRin,float(line[col_LRin-1])*10/C_LRin + 5.67e-8*(float(line[col_Trad-1])+T_0)^4]~
  + [X] LRout
  + [X] Haws: ~Haws = [Haws,float(line[col_Haws-1])*((float(line[col_T-1])+T_0)/T_0)^0.5]~
  + [X] Hstk: ~Hstk = [Hstk,float(line[col_Hstk-1])*((float(line[col_T-1])+T_0)/T_0)^0.5]~
  + [X] Hpt: ~Hpt = [Hpt,float(line[col_Hpt-1])*C_Hpt*F_Hpt*998./rho_af]~
  + [X] Derives Hpt_corrected
  + [X] Decodes GPS - some stations only record minutes not degrees


#+BEGIN_SRC jupyter-python

T_0 = 273.15

# Calculate pressure transducer fluid density
if ~ds['z_pt'].isnull().all():
    if ds.attrs['pt_antifreeze'] == 50:
        rho_af = 1092
    elif ds.attrs['pt_antifreeze'] == 100:
        rho_af = 1145
    else:
        rho_af = np.nan
        print("ERROR: Incorrect metadata: 'pt_antifreeze =' ", ds.attrs['pt_antifreeze'])
        print("Antifreeze mix only supported at 50 % or 100%")
        # assert(False)
    

for v in ['gps_geounit','min_y']:
    if v in list(ds.variables): ds = ds.drop_vars(v)
        
## adjust times based on file format.
# raw: No adjust (timestamp is at start of period)
# STM: Adjust timestamp from end of period to start of period
# TX: Adjust timestamp start of period (hour/day) also depending on season

def time_shift(da):
    """ Adjust times
    raw: (10 min) values are sampled instantaneously. Don't call this function
    STM: (1 hour) values are averaged and timestamp is end. Shift 1 h earlier to beginning
    TX: Some 10 min, some 1 hour, some 1 day? Shift appropriately.
    """
    # assert(ds.attrs['format'] != 'raw')
    if ds.attrs['format'] == 'raw':
        # diff = da['time'].diff(dim='time')
        # diffarr = diff.values.astype('timedelta64[h]').astype(int)
        # # assume the 1st time step (dropped via diff) is equal to the 2nd timestep
        # diffarr = np.append(diffarr[0], diffarr)
        # da['time'] = (da['time'] + pd.to_timedelta("-1 hour"))\
        #     .where((diffarr == 1) & (da['time'].dt.dayofyear <= 300) & (da['time'].dt.dayofyear >= 100), other=da['time'])

        ### NOTE: The following line re-implements bug: https://github.com/GEUS-PROMICE/AWS_v3/issues/2
        ### See also https://github.com/GEUS-PROMICE/PROMICE-AWS-processing/issues/20
        t = (da['time'] + pd.to_timedelta("-24 hours"))\
            .where((da['time'].dt.hour == 23) & ((da['time'].dt.dayofyear <= 300) & (da['time'].dt.dayofyear >= 100)), other=da['time'])
        for t in zip(t,da['time'].values):
            print(t)
    if ds.attrs['format'] == 'STM':
        t = da['time'] + pd.to_timedelta("-1 hour")
    if ds.attrs['format'] == 'TX':
        diff = da['time'].diff(dim='time')
        diffarr = diff.values.astype('timedelta64[h]').astype(int)
        # assume the 1st time step (dropped via diff) is equal to the 2nd timestep
        # diffarr = np.append(diffarr[0], diffarr)
        diffarr = np.append(0, diffarr) # no, don't.
        t = (da['time'] + pd.to_timedelta("-1 hour"))\
            .where(# (diffarr == 1) &
                   (da['time'].dt.dayofyear <= 300) &
                   (da['time'].dt.dayofyear >= 100),
                   other=da['time'])

        ### NOTE: The following line re-implements bug: https://github.com/GEUS-PROMICE/AWS_v3/issues/2
        ### See also https://github.com/GEUS-PROMICE/PROMICE-AWS-processing/issues/20
        # print(da['time'])
        # t = (da['time'] + pd.to_timedelta("+24 hours"))\
        #     .where((da['time'].dt.hour == 23) & ((da['time'].dt.dayofyear <= 300) & (da['time'].dt.dayofyear >= 100)), other=da['time'])
        # print(da['time'])
    return t


# import pdb; pdb.set_trace()


# print(ds.attrs['format'])
# if ds.attrs['format'] != 'raw':
ds['time_orig'] = ds['time']
ds['time'] = time_shift(ds['time'].copy(deep=True))
_, index = np.unique(ds['time'], return_index=True)
ds = ds.isel(time=index)


###
### DEBUGGING
### 
import matplotlib.pyplot as plt
ds['n'] = (('time'), np.arange(ds['time'].size)+1)

# Remove HygroClip temperature offset
ds['t_2'] = ds['t_2'] - ds.attrs['hygroclip_t_offset']

# convert radiation from engineering to physical units
ds['dsr'] = (ds['dsr'] * 10) / ds.attrs['dsr_eng_coef']
ds['usr'] = (ds['usr'] * 10) / ds.attrs['usr_eng_coef']
ds['dlr'] = ((ds['dlr'] * 10) / ds.attrs['dlr_eng_coef']) + 5.67E-8*(ds['t_rad'] + T_0)**4
ds['ulr'] = ((ds['ulr'] * 10) / ds.attrs['ulr_eng_coef']) + 5.67E-8*(ds['t_rad'] + T_0)**4

# Adjust sonic ranger readings for sensitivity to air temperature
ds['z_boom'] = ds['z_boom'] * ((ds['t_1'] + T_0)/T_0)**0.5
ds['z_stake'] = ds['z_stake'] * ((ds['t_1'] + T_0)/T_0)**0.5

# Adjust pressure transducer due to fluid properties
if ~ds['z_pt'].isnull().all():
    ds['z_pt'] = ds['z_pt'] * ds.attrs['pt_z_coef'] * ds.attrs['pt_z_factor'] * 998.0 / rho_af

    # Calculate pressure transducer depth
    ds['z_pt_corr'] = ds['z_pt'] * np.nan # new 'z_pt_corr' copied from 'z_pt'
    ds['z_pt_corr'].attrs['long_name'] = ds['z_pt'].long_name + " corrected"
    ds['z_pt_corr'] = ds['z_pt'] * ds.attrs['pt_z_coef'] * ds.attrs['pt_z_factor'] * 998.0 / rho_af \
        + 100 * (ds.attrs['pt_z_p_coef'] - ds['p']) / (rho_af * 9.81)


# Decode GPS
if ds['gps_lat'].dtype.kind == 'O': # not a float. Probably has "NH"
    assert('NH' in ds['gps_lat'].dropna(dim='time').values[0])
    for v in ['gps_lat','gps_lon','gps_time']:
        a = ds[v].attrs # store
        str2nums = [re.findall(r"[-+]?\d*\.\d+|\d+", _) if isinstance(_, str) else [np.nan] for _ in ds[v].values]
        ds[v][:] = pd.DataFrame(str2nums).astype(float).T.values[0]
        ds[v] = ds[v].astype(float)
        ds[v].attrs = a # restore
        
if np.any((ds['gps_lat'] <= 90) & (ds['gps_lat'] > 0)):  # Some stations only recorded minutes, not degrees
    xyz = np.array(re.findall("[-+]?[\d]*[.][\d]+", ds.attrs['geometry'])).astype(float)
    x=xyz[0]; y=xyz[1]; z=xyz[2] if len(xyz) == 3 else 0
    p = shapely.geometry.Point(x,y,z)
    ds['gps_lat'] = ds['gps_lat'] + 100*p.y
if np.any((ds['gps_lon'] <= 90) & (ds['gps_lon'] > 0)):
    ds['gps_lon'] = ds['gps_lon'] + 100*p.x
        
for v in ['gps_lat','gps_lon']:
    a = ds[v].attrs # store
    ds[v] = np.floor(ds[v] / 100) + (ds[v] / 100 - np.floor(ds[v] / 100)) * 100 / 60
    ds[v].attrs = a # restore

# tilt-o-meter voltage to degrees
# if transmitted ne 'yes' then begin
#    tiltX = smooth(tiltX,7,/EDGE_MIRROR,MISSING=-999) & tiltY = smooth(tiltY,7,/EDGE_MIRROR, MISSING=-999)
# endif

# Should just be
# if ds.attrs['format'] != 'TX': dstxy = dstxy.rolling(time=7, win_type='boxcar', center=True).mean()
# but the /EDGE_MIRROR makes it a bit more complicated...
if ds.attrs['format'] != 'TX':
    win_size=7
    s = int(win_size/2)
    tdf = ds['tilt_x'].to_dataframe()
    ds['tilt_x'] = (('time'), tdf.iloc[:s][::-1].append(tdf).append(tdf.iloc[-s:][::-1]).rolling(win_size, win_type='boxcar', center=True).mean()[s:-s].values.flatten())
    tdf = ds['tilt_y'].to_dataframe()
    ds['tilt_y'] = (('time'), tdf.iloc[:s][::-1].append(tdf).append(tdf.iloc[-s:][::-1]).rolling(win_size, win_type='boxcar', center=True).mean()[s:-s].values.flatten())

# # notOKtiltX = where(tiltX lt -100, complement=OKtiltX) & notOKtiltY = where(tiltY lt -100, complement=OKtiltY)
notOKtiltX = (ds['tilt_x'] < -100)
OKtiltX = (ds['tilt_x'] >= -100)
notOKtiltY = (ds['tilt_y'] < -100)
OKtiltY = (ds['tilt_y'] >= -100)

# tiltX = tiltX/10.
ds['tilt_x'] = ds['tilt_x'] / 10
ds['tilt_y'] = ds['tilt_y'] / 10

# tiltnonzero = where(tiltX ne 0 and tiltX gt -40 and tiltX lt 40)
# if n_elements(tiltnonzero) ne 1 then tiltX[tiltnonzero] = tiltX[tiltnonzero]/abs(tiltX[tiltnonzero])*(-0.49*(abs(tiltX[tiltnonzero]))^4 + 3.6*(abs(tiltX[tiltnonzero]))^3 - 10.4*(abs(tiltX[tiltnonzero]))^2 +21.1*(abs(tiltX[tiltnonzero])))

# tiltY = tiltY/10.
# tiltnonzero = where(tiltY ne 0 and tiltY gt -40 and tiltY lt 40)
# if n_elements(tiltnonzero) ne 1 then tiltY[tiltnonzero] = tiltY[tiltnonzero]/abs(tiltY[tiltnonzero])*(-0.49*(abs(tiltY[tiltnonzero]))^4 + 3.6*(abs(tiltY[tiltnonzero]))^3 - 10.4*(abs(tiltY[tiltnonzero]))^2 +21.1*(abs(tiltY[tiltnonzero])))

dstx = ds['tilt_x']
nz = (dstx != 0) & (np.abs(dstx) < 40)
dstx = dstx.where(~nz, other = dstx / np.abs(dstx) * (-0.49 * (np.abs(dstx))**4 + 3.6 * (np.abs(dstx))**3 - 10.4 * (np.abs(dstx))**2 + 21.1 * (np.abs(dstx))))
ds['tilt_x'] = dstx

dsty = ds['tilt_y']
nz = (dsty != 0) & (np.abs(dsty) < 40)
dsty = dsty.where(~nz, other = dsty / np.abs(dsty) * (-0.49 * (np.abs(dsty))**4 + 3.6 * (np.abs(dsty))**3 - 10.4 * (np.abs(dsty))**2 + 21.1 * (np.abs(dsty))))
ds['tilt_y'] = dsty

# if n_elements(OKtiltX) gt 1 then tiltX[notOKtiltX] = interpol(tiltX[OKtiltX],OKtiltX,notOKtiltX) ; Interpolate over gaps for radiation correction; set to -999 again below.
# if n_elements(OKtiltY) gt 1 then tiltY[notOKtiltY] = interpol(tiltY[OKtiltY],OKtiltY,notOKtiltY) ; Interpolate over gaps for radiation correction; set to -999 again below.

ds['tilt_x'] = ds['tilt_x'].where(~notOKtiltX)
ds['tilt_y'] = ds['tilt_y'].where(~notOKtiltY)
ds['tilt_x'] = ds['tilt_x'].interpolate_na(dim='time')
ds['tilt_y'] = ds['tilt_y'].interpolate_na(dim='time')
# ds['tilt_x'] = ds['tilt_x'].ffill(dim='time')
# ds['tilt_y'] = ds['tilt_y'].ffill(dim='time')


deg2rad = np.pi / 180
ds['wdir'] = ds['wdir'].where(ds['wspd'] != 0)
ds['wspd_x'] = ds['wspd'] * np.sin(ds['wdir'] * deg2rad)
ds['wspd_y'] = ds['wspd'] * np.cos(ds['wdir'] * deg2rad)
#+END_SRC


** Export file as L1

+ Check with ~cfchecks ./data/L1/EGP/EGP-2016-raw.nc~

#+BEGIN_SRC jupyter-python
infile = ds.attrs['file']
outpath = os.path.split(infile)[0].split("/")
outpath[-2] = 'L1'
outpath = '/'.join(outpath)
outfile = os.path.splitext(os.path.splitext(os.path.basename(infile))[0])[0]

outpathfile = outpath + '/' + outfile + ".nc"
if os.path.exists(outpathfile): os.remove(outpathfile)
ds.to_netcdf(outpathfile, mode='w', format='NETCDF4', compute=True)
#+END_SRC

#+RESULTS:



* Level 1
:PROPERTIES:
:header-args:bash+: :exports both
:END:

File list:

#+BEGIN_SRC bash :exports both :results verbatim
find ./data/L1
#+END_SRC

#+RESULTS:
#+begin_example
./data/L1
./data/L1/EGP
./data/L1/EGP/EGP-2017-STM.nc
./data/L1/EGP/EGP-2016-raw.nc
./data/L1/EGP/EGP-2019-TX.nc
./data/L1/EGP/EGP-2017-raw.nc
./data/L1/EGP/EGP-2019.1-raw.nc
./data/L1/EGP/EGP-2018.2-raw.nc
./data/L1/EGP/EGP-2018.1-raw.nc
./data/L1/EGP/EGP-2019.2-raw.nc
#+end_example

NetCDF format

#+BEGIN_SRC bash :results verbatim :exports both
ncdump -ch ./data/L1/EGP/EGP-2016-raw.nc | head -n35
#+END_SRC

#+RESULTS:
#+begin_example
netcdf EGP-2016-raw {
dimensions:
	time = 10847 ;
variables:
	double rec(time) ;
		rec:_FillValue = NaN ;
		rec:standard_name = "record" ;
		rec:long_name = "Record" ;
		rec:units = "" ;
		rec:scale_factor = 1. ;
		rec:add_offset = 0. ;
	double p(time) ;
		p:_FillValue = NaN ;
		p:standard_name = "air_pressure" ;
		p:long_name = "Air pressure" ;
		p:units = "hPa" ;
		p:scale_factor = 0.01 ;
		p:add_offset = 0. ;
	double t_1(time) ;
		t_1:_FillValue = NaN ;
		t_1:standard_name = "air_temperature" ;
		t_1:long_name = "Air temperature 1" ;
		t_1:units = "C" ;
		t_1:scale_factor = 1. ;
		t_1:add_offset = 273.15 ;
	double t_2(time) ;
		t_2:_FillValue = NaN ;
		t_2:standard_name = "air_temperature" ;
		t_2:long_name = "Air temperature 2" ;
		t_2:units = "C" ;
		t_2:scale_factor = 1. ;
		t_2:add_offset = 273.15 ;
	double rh(time) ;
		rh:_FillValue = NaN ;
		rh:standard_name = "relative_humidity" ;
#+end_example


* L1 -> L1A
:PROPERTIES:
:header-args:jupyter-python+: :session L1_to_L1A :noweb-ref L1_to_L1A :noweb yes
:END:

+ Merge all files by type (keep =raw=, =STM=, and =TX=)
+ Flag out-of-limit (OOL) values from [[./flags.csv]]

** Wrapper

Run one:
#+BEGIN_SRC jupyter-python :noweb-ref
infile = "./data/L1/EGP/EGP_2016_raw.nc"
<<L1_to_L1A>>
#+END_SRC

#+RESULTS:

Run all:

#+BEGIN_SRC bash
# conda activate PROMICE_dev

# ./L1_to_L1A.py ./data/L1/EGP/EGP-2016-raw.nc
./L1_to_L1A.py data/L1/EGP/*raw.nc
./L1_to_L1A.py data/L1/EGP/*STM.nc
./L1_to_L1A.py data/L1/EGP/*TX.nc
#+END_SRC

#+RESULTS:

#+header:  :tangle L1_to_L1A.py :noweb-ref :tangle-mode (identity #o544)
#+BEGIN_SRC jupyter-python
#!/usr/bin/env python

<<L1_to_L1A_imports>>

def L1_to_L1A(infile=None):
    <<L1_to_L1A>>

if __name__ == "__main__":
    import sys
    # for arg in sys.argv[1:]: L1_to_L1A(arg)
    L1_to_L1A(sys.argv[1:])
#+END_SRC

** Imports

#+header: :noweb-ref L1_to_L1A_imports
#+BEGIN_SRC jupyter-python
import pandas as pd
import xarray as xr
import os
#+END_SRC

#+RESULTS:

** Merge files
#+BEGIN_SRC jupyter-python :exports both

# This could be as simple as:
# ds = xr.open_mfdataset(infile, combine='by_coords', mask_and_scale=False).load()
# Except that some files have overlapping times.

# try:
#     ds = xr.open_mfdataset(infile, combine='by_coords', mask_and_scale=False).load()
# except ValueError:
#     print("Error: files with overlapping times")
#     print("Flag out times using flagging feature")
#     for f in infile:
#         print(f, xr.open_dataset(f)['time'].isel({'time':[0,-1]}).values)
#     assert(False)
    
if not isinstance(infile, list):
    ds = xr.open_mfdataset(infile)
else:
    ds = xr.open_mfdataset(infile[0]).load().dropna(dim='time', how='all')
    for f in infile[1:]:
        tmp = xr.open_mfdataset(f).load().dropna(dim='time', how='all')
        ds = ds.combine_first(tmp)
        
#+END_SRC

#+RESULTS:

** Flag data

Out of limit (OOL) data comes from the [[./variables.csv]] file.

+ Set each variable to NaN where it is OOL
+ Also set paired or associated variables to NaN

#+NAME: flag_data
#+BEGIN_SRC jupyter-python
df = pd.read_csv("./variables.csv", index_col=0, comment="#", usecols=('field','lo','hi','OOL'))
df = df.dropna(how='all')
for var in df.index:
    if var not in list(ds.variables): continue
    if var == 'rh_cor':
         ds[var] = ds[var].where(ds[var] >= df.loc[var, 'lo'], other = 0)
         ds[var] = ds[var].where(ds[var] <= df.loc[var, 'hi'], other = 100)
    else:
        ds[var] = ds[var].where(ds[var] >= df.loc[var, 'lo'])
        ds[var] = ds[var].where(ds[var] <= df.loc[var, 'hi'])
    other_vars = df.loc[var]['OOL'] # either NaN or "foo" or "foo bar baz ..."
    if isinstance(other_vars, str): 
        for o in other_vars.split():
            if o not in list(ds.variables): continue
            ds[o] = ds[o].where(ds[var] >= df.loc[var, 'lo'])
            ds[o] = ds[o].where(ds[var] <= df.loc[var, 'hi'])
#+END_SRC

#+RESULTS:


** Export file as L1A

+ Check with ~cfchecks ./data/L1A/EGP/EGP-2016-raw.nc~

#+BEGIN_SRC jupyter-python
if isinstance(infile, list): infile = infile[0]
# infile_parts = os.path.splitext(os.path.basename(infile))[0].split('_')
# outfile = infile_parts[0] + '-' + infile_parts[-1] + '.nc' # drop year
outfile = ds.attrs['station_id'] + '-' + ds.attrs['format'] + '.nc'

outpath = os.path.split(infile)[0].split("/")
outpath[-2] = 'L1A'
# outfile = os.path.splitext(os.path.basename(infile))[0] + '.nc'
outpath = '/'.join(outpath)
outpathfile = outpath + '/' + outfile
if os.path.exists(outpathfile): os.remove(outpathfile)
ds.to_netcdf(outpathfile, mode='w', format='NETCDF4', compute=True)
#+END_SRC

#+RESULTS:




* Level 1A
* L1A -> L2
:PROPERTIES:
:header-args:jupyter-python+: :session L1A_to_L2 :noweb-ref L1A_to_L2 :noweb yes
:END:

+ Calibration using secondary sources

** Wrapper

Run one:
#+BEGIN_SRC jupyter-python :noweb-ref
infile = "./data/L1A/EGP/EGP-raw.nc"
#+END_SRC

#+RESULTS:

#+BEGIN_SRC jupyter-python :noweb-ref
<<L1A_to_L2>>
#+END_SRC

#+RESULTS:

Run all:

#+BEGIN_SRC bash
# conda activate PROMICE_dev

./L1A_to_L2.py data/L1A/EGP/EGP-raw.nc
./L1A_to_L2.py data/L1A/EGP/EGP-STM.nc
./L1A_to_L2.py data/L1A/EGP/EGP-TX.nc
#+END_SRC


#+BEGIN_SRC jupyter-python :tangle L1A_to_L2.py :noweb-ref :tangle-mode (identity #o544)
#!/usr/bin/env python

<<L1A_to_L2_imports>>

def L1A_to_L2(infile=None):
    <<L1A_to_L2>>


if __name__ == "__main__":
    import sys
    for arg in sys.argv[1:]: L1A_to_L2(arg)
#+END_SRC

** Imports

#+header: :noweb-ref L1A_to_L2_imports
#+BEGIN_SRC jupyter-python
import xarray as xr
import pandas as pd
import os

#+END_SRC

#+RESULTS:

** Init

#+BEGIN_SRC jupyter-python
<<constants>>
#+END_SRC

#+RESULTS:


** Load
#+BEGIN_SRC jupyter-python :exports both
# infile = "./data/L1A/EGP/EGP-raw.nc"
ds = xr.open_dataset(infile, mask_and_scale=False).load()
# print(ds)
#+END_SRC

#+RESULTS:

** Calibrate using secondary sensors

*** Correct relative humidity

+ Correct relative humidity readings for T below 0 to give value with respect to ice
  + GitHub: [[./IDL/AWSdataprocessing_v3.pro#L411]]
  + Org Mode: [[./IDL/AWSdataprocessing_v3.pro::411]]

+ This section implements the Goff-Gratch equation
 
#+BEGIN_SRC jupyter-python
T_0 = 273.15

T_100 = T_0+100            # steam point temperature in K
ews = 1013.246             # saturation pressure at steam point temperature, normal atmosphere
ei0 = 6.1071

T = ds['t_1'].copy(deep=True)

# in hPa (Goff & Gratch)
e_s_wtr = 10**(-7.90298 * (T_100 / (T + T_0) - 1)
               + 5.02808 * np.log10(T_100 / (T + T_0)) 
               - 1.3816E-7 * (10**(11.344 * (1 - (T + T_0) / T_100)) - 1)
               + 8.1328E-3 * (10**(-3.49149 * (T_100/(T + T_0) - 1)) -1)
               + np.log10(ews))

# in hPa (Goff & Gratch)
e_s_ice = 10**(-9.09718 * (T_0 / (T + T_0) - 1)
               - 3.56654 * np.log10(T_0 / (T + T_0))
               + 0.876793 * (1 - (T + T_0) / T_0)
               + np.log10(ei0))

# ds['rh_cor'] = (e_s_wtr / e_s_ice) * ds['rh'].where((ds['t_1'] < 0) & (ds['t_1'] > -100))
freezing = (ds['t_1'] < 0) & (ds['t_1'] > -100).values # why > -100?
# set to Geoff & Gratch values when freezing, otherwise just rh.
ds['rh_cor'] = ds['rh'].where(~freezing, other = ds['rh']*(e_s_wtr / e_s_ice))


# https://github.com/GEUS-PROMICE/PROMICE-AWS-processing/issues/23
# Just adding special treatment here in service of replication. rh_cor is clipped not NaN'd
# https://github.com/GEUS-PROMICE/PROMICE-AWS-processing/issues/20
<<flag_data>>
#+END_SRC

#+RESULTS:



*** Cloud cover

+ cloud cover (for iswr correction) and surface temperature
  + GitHub: [[./IDL/AWSdataprocessing_v3.pro#L441]]
  + Org Mode: [[./IDL/AWSdataprocessing_v3.pro::441]]

This is a derived product and belongs is L2->L3 processing appearing in L3, but DifFrac is used in the iswr correction.

#+BEGIN_SRC jupyter-python

eps_overcast = 1.
eps_clear = 9.36508e-6
LR_overcast = eps_overcast * 5.67e-8 *(T + T_0)**4   # assumption
LR_clear = eps_clear * 5.67e-8 * (T + T_0)**6        # Swinbank (1963)

# Special case for selected stations (will need this for all stations eventually)
if ds.attrs['station_id'] == 'KAN_M':
   # print,'KAN_M cloud cover calculations'
   LR_overcast = 315 + 4*T
   LR_clear = 30 + 4.6e-13 * (T + T_0)**6
elif ds.attrs['station_id'] == 'KAN_U':
   # print,'KAN_U cloud cover calculations'
   LR_overcast = 305 + 4*T
   LR_clear = 220 + 3.5*T

cc = (ds['dlr'] - LR_clear) / (LR_overcast - LR_clear)
cc[cc > 1] = 1
cc[cc < 0] = 0
DifFrac = 0.2 + 0.8 * cc

ds['cc'] = (('time'), cc.data)

emissivity = 0.97
ds['t_surf'] = ((ds['ulr'] - (1 - emissivity) * ds['dlr']) / emissivity / 5.67e-8)**0.25 - T_0
ds['t_surf'] = ds['t_surf'].where(ds['t_surf'] <= 0, other = 0) # if > 0, set to 0
#+END_SRC

#+RESULTS:



*** Correct shortwave radiation

+ Take into account station tilt, sun angle, etc.
  + GitHub: [[./IDL/AWSdataprocessing_v3.pro#L475]]
  + Org Mode: [[./IDL/AWSdataprocessing_v3.pro::475]]

Calculate tilt angle and direction of sensor and rotating to a north-south aligned coordinate system
#+BEGIN_SRC jupyter-python
tx = ds['tilt_x'] * deg2rad
ty = ds['tilt_y'] * deg2rad

## cartesian coords
X = np.sin(tx) * np.cos(tx) * np.sin(ty)**2 + np.sin(tx) * np.cos(ty)**2
Y = np.sin(ty) * np.cos(ty) * np.sin(tx)**2 + np.sin(ty) * np.cos(tx)**2
Z = np.cos(tx) * np.cos(ty) + np.sin(tx)**2 * np.sin(ty)**2

# spherical coords
phi_sensor_rad = -np.pi /2 - np.arctan(Y/X)
phi_sensor_rad[X > 0] += np.pi
phi_sensor_rad[(X == 0) & (Y < 0)] = np.pi
phi_sensor_rad[(X == 0) & (Y == 0)] = 0
phi_sensor_rad[phi_sensor_rad < 0] += 2*np.pi

phi_sensor_deg = phi_sensor_rad * rad2deg

# spherical coordinate (or actually total tilt of the sensor, i.e. 0 when horizontal)
theta_sensor_rad = np.arccos(Z / (X**2 + Y**2 + Z**2)**0.5) 
theta_sensor_deg = theta_sensor_rad * rad2deg

## Offset correction (determine offset yourself using data for solar
## zenith angles larger than 110 deg) I actually don't do this as it
## shouldn't improve accuracy for well calibrated instruments
# ;ds['dsr'] = ds['dsr'] - ds['dwr_offset']
# ;SRout = SRout - SRout_offset

# Calculating zenith and hour angle of the sun
doy = ds['time'].to_dataframe().index.dayofyear.values
hour = ds['time'].to_dataframe().index.hour.values
minute = ds['time'].to_dataframe().index.minute.values
# lat = ds['gps_lat']
# lon = ds['gps_lon']
lat = ds.attrs['latitude']
lon = ds.attrs['longitude']

d0_rad = 2 * np.pi * (doy + (hour + minute / 60) / 24 -1) / 365

Declination_rad = np.arcsin(0.006918 - 0.399912 * np.cos(d0_rad) + 0.070257 * np.sin(d0_rad) - 0.006758 * np.cos(2 * d0_rad) + 0.000907 * np.sin(2 * d0_rad) - 0.002697 * np.cos(3 * d0_rad) + 0.00148 * np.sin(3 * d0_rad))

HourAngle_rad = 2 * np.pi * (((hour + minute / 60) / 24 - 0.5) - lon/360)
# ; - 15.*timezone/360.) ; NB: Make sure time is in UTC and longitude is positive when west! Hour angle should be 0 at noon.

# This is 180 deg at noon (NH), as opposed to HourAngle.
DirectionSun_deg = HourAngle_rad * 180/np.pi - 180

DirectionSun_deg[DirectionSun_deg < 0] += 360
DirectionSun_deg[DirectionSun_deg < 0] += 360

ZenithAngle_rad = np.arccos(np.cos(lat * deg2rad) * np.cos(Declination_rad) * np.cos(HourAngle_rad) + np.sin(lat * deg2rad) * np.sin(Declination_rad))

ZenithAngle_deg = ZenithAngle_rad * rad2deg

sundown = ZenithAngle_deg >= 90
isr_toa = 1372 * np.cos(ZenithAngle_rad) # Incoming shortware radiation at the top of the atmosphere
isr_toa[sundown] = 0

# Calculating the correction factor for direct beam radiation
# http://solardat.uoregon.edu/SolarRadiationBasics.html
CorFac = np.sin(Declination_rad) * np.sin(lat * deg2rad) * np.cos(theta_sensor_rad) - np.sin(Declination_rad) * np.cos(lat * deg2rad) * np.sin(theta_sensor_rad) * np.cos(phi_sensor_rad + np.pi) + np.cos(Declination_rad) * np.cos(lat * deg2rad) * np.cos(theta_sensor_rad) * np.cos(HourAngle_rad) + np.cos(Declination_rad) * np.sin(lat * deg2rad) * np.sin(theta_sensor_rad) * np.cos(phi_sensor_rad + np.pi) * np.cos(HourAngle_rad) + np.cos(Declination_rad) * np.sin(theta_sensor_rad) * np.sin(phi_sensor_rad + np.pi) * np.sin(HourAngle_rad)

CorFac = np.cos(ZenithAngle_rad) / CorFac
# sun out of field of view upper sensor
CorFac[(CorFac < 0) | (ZenithAngle_deg > 90)] = 1

# Calculating ds['dsr'] over a horizontal surface corrected for station/sensor tilt
CorFac_all = CorFac / (1 - DifFrac + CorFac * DifFrac)
ds['dsr_cor'] = ds['dsr'].copy(deep=True) * CorFac_all

# Calculating albedo based on albedo values when sun is in sight of the upper sensor
AngleDif_deg = 180 / np.pi * np.arccos(np.sin(ZenithAngle_rad) * np.cos(HourAngle_rad + np.pi) * np.sin(theta_sensor_rad) * np.cos(phi_sensor_rad) + np.sin(ZenithAngle_rad) * np.sin(HourAngle_rad + np.pi) * np.sin(theta_sensor_rad) * np.sin(phi_sensor_rad) + np.cos(ZenithAngle_rad) * np.cos(theta_sensor_rad)) # angle between sun and sensor

# ds['add'] = (('time'),AngleDif_deg)
# ds['zar'] = (('time'),ZenithAngle_rad)
# ds['har'] = (('time'),HourAngle_rad)
# ds['tsr'] = (('time'),theta_sensor_rad)
# ds['X'] = (('time'),X)
# ds['Y'] = (('time'),Y)
# ds['Z'] = (('time'),Z)
# from IPython import embed; embed()
# ds[['dsr','dsr_cor','usr','add','X','Y','Z','tilt_x','tilt_y']].to_dataframe().head(40)


# ;AngleDif_deg = 180./!pi*acos(cos(!pi/2.-ZenithAngle_rad)*cos(!pi/2.-theta_sensor_rad)*cos(HourAngle_rad-phi_sensor_rad)+sin(!pi/2.-ZenithAngle_rad)*sin(!pi/2.-theta_sensor_rad)) ; angle between sun and sensor

# from IPython import embed; embed()

ds['albedo'] = ds['usr'] / ds['dsr_cor']
albedo_nan = np.isnan(ds['albedo']) # store existing NaN
OKalbedos = (AngleDif_deg < 70) & (ZenithAngle_deg < 70) & (ds['albedo'] < 1) & (ds['albedo'] > 0)
ds['albedo'][~OKalbedos] = np.nan

# NOTE: "use_coordinate=False" is used here to force comparison against the GDL code when that is run with *only* a TX file.
# Should eventually set to default (True) and interpolate based on time, not index.
ds['albedo'] = ds['albedo'].interpolate_na(dim='time', use_coordinate=False) # Interpolate all NaN (old and new NotOK)
ds['albedo'] = ds['albedo'].ffill(dim='time').bfill(dim='time')
# TODO: Remove above?

# ds['albedo'] = ds['albedo'].ffill(dim='time') # Interpolate all NaN (old and new NotOK)
# ds['albedo'][albedo_nan] = np.nan # restore old NaN

# ;OKalbedos = where(angleDif_deg lt 82.5 and ZenithAngle_deg lt 70 and albedo lt 1 and albedo gt 0, complement=notOKalbedos)
# ;The running mean calculation doesn't work for non-continuous data sets or variable temporal resolution (e.g. with multiple files)
# ;albedo_rm = 0*albedo
# ;albedo_rm[OKalbedos] = smooth(albedo[OKalbedos],obsrate+1,/edge_truncate) ; boxcar average of reliable albedo values
# ;albedo[notOKalbedos] = interpol(albedo_rm[OKalbedos],OKalbedos,notOKalbedos) ; interpolate over gaps
# ;albedo_rm[notOKalbedos] = albedo[notOKalbedos]
# ;So instead:

# albedo[notOKalbedos] = interpol(albedo[OKalbedos],OKalbedos,notOKalbedos) ; interpolate over gaps - gives problems for discontinuous data sets, but is not the end of the world

# Correcting SR using DWR when sun is in field of view of lower sensor assuming sensor measures only diffuse radiation
sunonlowerdome =(AngleDif_deg >= 90) & (ZenithAngle_deg <= 90)
# ds['dsr_cor'][sunonlowerdome] = ds['dsr'][sunonlowerdome] / DifFrac[sunonlowerdome]
ds['dsr_cor'] = ds['dsr_cor'].where(~sunonlowerdome, other=ds['dsr'] / DifFrac)
ds['usr_cor'] = ds['usr'].copy(deep=True)
# ds['usr_cor'][sunonlowerdome] = albedo * ds['dsr'][sunonlowerdome] / DifFrac[sunonlowerdome]
ds['usr_cor'] = ds['usr_cor'].where(~sunonlowerdome, other=ds['albedo'] * ds['dsr'] / DifFrac)


# Setting DWR and USWR to zero for solar zenith angles larger than 95 deg or either DWR or USWR are (less than) zero
bad = (ZenithAngle_deg > 95) | (ds['dsr_cor'] <= 0) | (ds['usr_cor'] <= 0)
ds['dsr_cor'][bad] = 0
ds['usr_cor'][bad] = 0

# Correcting DWR using more reliable USWR when sun not in sight of upper sensor
ds['dsr_cor'] = ds['usr_cor'].copy(deep=True) / ds['albedo']
# albedo[~OKalbedos] = np.nan
ds['albedo'] = ds['albedo'].where(OKalbedos)
# albedo[OKalbedos[n_elements(OKalbedos)-1]:*] = -999 ; Removing albedos that were extrapolated (as opposed to interpolated) at the end of the time series - see above
# ds['dsr']_cor[OKalbedos[n_elements(OKalbedos)-1]:*] = -999 ; Removing the corresponding ds['dsr']_cor as well
# ds['uswr_cor'][OKalbedos[n_elements(OKalbedos)-1]:*] = -999 ; Removing the corresponding ds['uswr_cor'] as well

# ; Removing spikes by interpolation based on a simple top-of-the-atmosphere limitation
#      TOA_crit_nopass = where(ds['dsr']_cor gt 0.9*dwr_toa+10)
#      TOA_crit_pass = where(ds['dsr']_cor le 0.9*dwr_toa+10)
#      if total(TOA_crit_nopass) ne -1 then begin
#         ds['dsr']_cor[TOA_crit_nopass] = interpol(ds['dsr']_cor[TOA_crit_pass],TOA_crit_pass,TOA_crit_nopass)
#         ds['uswr_cor'][TOA_crit_nopass] = interpol(ds['uswr_cor'][TOA_crit_pass],TOA_crit_pass,TOA_crit_nopass)
#      endif
TOA_crit_nopass = (ds['dsr_cor'] > (0.9 * isr_toa + 10))

ds['dsr_cor'][TOA_crit_nopass] = np.nan
ds['usr_cor'][TOA_crit_nopass] = np.nan
ds['dsr_cor'] = ds['dsr_cor'].interpolate_na(dim='time')
ds['usr_cor'] = ds['usr_cor'].interpolate_na(dim='time')
#ds['dsr_cor'] = ds['dsr_cor'].ffill(dim='time')
#ds['usr_cor'] = ds['usr_cor'].ffill(dim='time')
# ds['dsr_cor'] = ds['dsr_cor'].interpolate_na(dim='time', method='linear', limit=12, max_gap='2H')
# ds['usr_cor'] = ds['usr_cor'].interpolate_na(dim='time', method='linear', limit=12, max_gap='2H')

# from IPython import embed; embed()
# print,'- Sun in view of upper sensor / workable albedos:',n_elements(OKalbedos),100*n_elements(OKalbedos)/n_elements(ds['dsr']),'%'
valid = (~(ds['dsr_cor'].isnull())).sum()
print('- Sun in view of upper sensor / workable albedos:',
      OKalbedos.sum().values,
      (100*OKalbedos.sum()/valid).round().values,
      "%")

# print,'- Sun below horizon:',n_elements(sundown),100*n_elements(sundown)/n_elements(ds['dsr']),'%'
print('- Sun below horizon:',
      sundown.sum(),
      (100*sundown.sum()/valid).round().values,
      "%")

# print,'- Sun in view of lower sensor:',n_elements(sunonlowerdome),100*n_elements(sunonlowerdome)/n_elements(ds['dsr']),'%'
print('- Sun in view of lower sensor:',
      sunonlowerdome.sum().values,
      (100*sunonlowerdome.sum()/valid).round().values,
      "%")

# print,'- Spikes removed using TOA criteria:',n_elements(TOA_crit_nopass),100*n_elements(TOA_crit_nopass)/n_elements(ds['dsr']),'%'
print('- Spikes removed using TOA criteria:',
      TOA_crit_nopass.sum().values,
      (100*TOA_crit_nopass.sum()/valid).round().values,
      "%")

# print,'- Mean net SR change by corrections:',total(ds['dsr']_cor-ds['uswr_cor']-ds['dsr']+SRout)/n_elements(ds['dsr']),' W/m2'
print('- Mean net SR change by corrections:',
      (ds['dsr_cor']-ds['usr_cor']-ds['dsr']+ds['usr']).sum().values/valid.values,
      "W/m2")

#+END_SRC

*** Wind direction

+ GitHub: [[./IDL/AWSdataprocessing_v3.pro#L423]]
+ Org Mode: [[./IDL/AWSdataprocessing_v3.pro::423]]
    
#+BEGIN_SRC jupyter-python

# ds['wspd_x'] = ds['wspd'] * np.sin(ds['wdir'] * deg2rad)
# ds['wspd_y'] = ds['wspd'] * np.cos(ds['wdir'] * deg2rad)

# adjust properties
#+END_SRC

#+RESULTS:

*** Cleaning

#+BEGIN_SRC jupyter-python
# https://github.com/GEUS-PROMICE/PROMICE-AWS-processing/issues/23
# Just adding special treatment here in service of replication. rh_cor is clipped not NaN'd
# https://github.com/GEUS-PROMICE/PROMICE-AWS-processing/issues/20
<<flag_data>>
#+END_SRC

** Export file as L2

+ Check with ~cfchecks ./data/L2/EGP/EGP-raw.nc~

#+BEGIN_SRC jupyter-python
outpath = os.path.split(infile)[0].split("/")
outpath[-2] = 'L2'
outpath = '/'.join(outpath)
outfile = os.path.basename(infile)
outpathfile = outpath + '/' + outfile
if os.path.exists(outpathfile): os.remove(outpathfile)
ds.to_netcdf(outpathfile, mode='w', format='NETCDF4', compute=True)
#+END_SRC

#+RESULTS:





* L2 -> L3
:PROPERTIES:
:header-args:jupyter-python+: :session L2_to_L3 :noweb-ref L2_to_L3 :noweb yes
:END:

+ Derived values
  + [ ] Cloud cover
  + [ ] Wind direction components
  + [ ] Turbulent heat flux

** Wrapper

Run one:
#+BEGIN_SRC jupyter-python :noweb-ref
infile = "./data/L2/EGP/EGP-raw.nc"
<<L2_to_L3>>
#+END_SRC

Run all:

#+BEGIN_SRC bash
# conda activate PROMICE_dev

./L2_to_L3.py data/L2/EGP/*raw.nc
./L2_to_L3.py data/L2/EGP/*STM.nc
./L2_to_L3.py data/L2/EGP/*TX.nc
#+END_SRC


#+BEGIN_SRC jupyter-python :tangle L2_to_L3.py :noweb-ref :tangle-mode (identity #o544)
#!/usr/bin/env python

def L2_to_L3(infile=None):
    <<L2_to_L3>>


if __name__ == "__main__":
    import sys
    for arg in sys.argv[1:]: L2_to_L3(arg)
#+END_SRC

** Imports

#+BEGIN_SRC jupyter-python
import xarray as xr
import os

<<constants>>
#+END_SRC

#+RESULTS:

** Load
#+BEGIN_SRC jupyter-python :exports both
ds = xr.open_dataset(infile, mask_and_scale=False).load()
# print(ds)
#+END_SRC

#+RESULTS:

** Downsample to hourly and daily

Downsampling should be 1 line
#+BEGIN_SRC jupyter-python :noweb-ref nil
ds_h = ds.resample({'time':"1H"}).mean() # this takes ~2-3 minutes
ds_d = ds.resample({'time':"1D"}).mean()
#+END_SRC

But due to xarray implementation, this takes several minutes, while it takes << 1 second in Pandas.
See https://github.com/pydata/xarray/issues/4498

Therefore, we do downsampling in Pandas (for now) even though the code is more complex.

#+BEGIN_SRC jupyter-python
df_h = ds.to_dataframe().resample("1H").mean()  # what we want (quickly), but in Pandas form
# now, rebuild xarray dataset (https://www.theurbanist.com.au/2020/03/how-to-create-an-xarray-dataset-from-scratch/)
vals = [xr.DataArray(data=df_h[c], dims=['time'], coords={'time':df_h.index}, attrs=ds[c].attrs) for c in df_h.columns]
ds_h = xr.Dataset(dict(zip(df_h.columns,vals)), attrs=ds.attrs)


df_d = ds.to_dataframe().resample("1D").mean()  # what we want (quickly), but in Pandas form
# now, rebuild xarray dataset (https://www.theurbanist.com.au/2020/03/how-to-create-an-xarray-dataset-from-scratch/)
vals = [xr.DataArray(data=df_d[c], dims=['time'], coords={'time':df_d.index}, attrs=ds[c].attrs) for c in df_d.columns]
ds_d = xr.Dataset(dict(zip(df_d.columns,vals)), attrs=ds.attrs)
#+END_SRC

*** Circular averaging

Calculating average wind direction takes a bit more work...

#+BEGIN_SRC jupyter-python
ds_h['wdir'] = np.arctan2(ds_h['wspd_x'], ds_h['wspd_y']) * rad2deg
ds_d['wdir'] = np.arctan2(ds_d['wspd_x'], ds_d['wspd_y']) * rad2deg
ds_h['wdir'] = (ds_h['wdir'] + 360) % 360
ds_d['wdir'] = (ds_d['wdir'] + 360) % 360
#+END_SRC

** Derived properties

*** Turbulent heat flux

+ GitHub: [[./IDL/AWSdataprocessing_v3.pro#L866]]
+ Org Mode: [[./IDL/AWSdataprocessing_v3.pro::866]]


+ Requires hourly averages

Constants

#+BEGIN_SRC jupyter-python
z_0    =    0.001    # aerodynamic surface roughness length for momention (assumed constant for all ice/snow surfaces)
eps    =    0.622
es_0   =    6.1071   # saturation vapour pressure at the melting point (hPa)
es_100 = 1013.246    # saturation vapour pressure at steam point temperature (hPa)
g      =    9.82     # gravitational acceleration (m/s2)
gamma  =   16.       # flux profile correction (Paulson & Dyer)
kappa  =    0.4      # Von Karman constant (0.35-0.42)
L_sub  =    2.83e6   # latent heat of sublimation (J/kg)
R_d    =  287.05     # gas constant of dry air
aa     =    0.7      # flux profile correction constants (Holtslag & De Bruin '88)
bb     =    0.75
cc     =    5.
dd     =    0.35
c_pd   = 1005.       # specific heat of dry air (J/kg/K)
WS_lim =    1.
L_dif_max = 0.01


T_0 = 273.15
T_100 = T_0+100            # steam point temperature in K

#+END_SRC

#+RESULTS:

#+BEGIN_SRC jupyter-python
# ds_h = ds.resample({'time':"1H"}).mean() # this takes ~2-3 minuteso

T_h = ds_h['t_1'].copy()
p_h = ds_h['p'].copy()
WS_h = ds_h['wspd'].copy()
Tsurf_h = ds_h['t_surf'].copy()
RH_cor_h = ds_h['rh_cor'].copy()

z_WS = ds_h['z_boom'].copy() + 0.4  # height of W
z_T = ds_h['z_boom'].copy() - 0.1   # height of thermometer

rho_atm = 100 * p_h / R_d / (T_h + T_0)   # atmospheric density

# dynamic viscosity of air (Pa s) (Sutherlands' equation using C = 120 K)
mu = 18.27e-6 * (291.15 + 120) / ((T_h + T_0) + 120) * ((T_h + T_0) / 291.15)**1.5

nu = mu / rho_atm                                                   # kinematic viscosity of air (m^2/s)
u_star = kappa * WS_h / np.log(z_WS / z_0)
Re = u_star * z_0 / nu
z_0h = z_0 * np.exp(1.5 - 0.2 * np.log(Re) - 0.11 * np.log(Re)**2) # rough surfaces: Smeets & Van den Broeke 2008
z_0h[WS_h <= 0] = 1e-10
es_ice_surf = 10**(-9.09718 * (T_0 / (Tsurf_h + T_0) -1) - 3.56654 * np.log10(T_0 / (Tsurf_h + T_0)) + 0.876793 * (1 - (Tsurf_h + T_0) / T_0) + np.log10(es_0))
q_surf = eps * es_ice_surf / (p_h - (1 - eps) * es_ice_surf)
# saturation vapour pressure above 0 C (hPa)
es_wtr = 10**(-7.90298 * (T_100 / (T_h + T_0) - 1) + 5.02808 * np.log10(T_100 / (T_h + T_0))
              - 1.3816E-7 * (10**(11.344 * (1 - (T_h + T_0) / T_100)) - 1)
              + 8.1328E-3 * (10**(-3.49149 * (T_100 / (T_h + T_0) -1)) - 1) + np.log10(es_100))
es_ice = 10**(-9.09718 * (T_0 / (T_h + T_0) - 1) - 3.56654 * np.log10(T_0 / (T_h + T_0)) + 0.876793 * (1 - (T_h + T_0) / T_0) + np.log10(es_0)) # saturation vapour pressure below 0 C (hPa)
q_sat = eps * es_wtr / (p_h - (1 - eps) * es_wtr) # specific humidity at saturation (incorrect below melting point)
freezing = T_h < 0  # replacing saturation specific humidity values below melting point
q_sat[freezing] = eps * es_ice[freezing] / (p_h[freezing] - (1 - eps) * es_ice[freezing])
q_h = RH_cor_h * q_sat / 100   # specific humidity in kg/kg
theta = T_h + z_T *g / c_pd
SHF_h = T_h
SHF_h[:] = 0
LHF_h = SHF_h
L = SHF_h + 1E5

stable = (theta > Tsurf_h) & (WS_h > WS_lim)
unstable = (theta < Tsurf_h) & (WS_h > WS_lim)
# no_wind  = (WS_h <= WS_lim)

for i in np.arange(0,31): # stable stratification
    psi_m1 = -(aa*         z_0/L[stable] + bb*(         z_0/L[stable]-cc/dd)*np.exp(-dd*         z_0/L[stable]) + bb*cc/dd)
    psi_m2 = -(aa*z_WS[stable]/L[stable] + bb*(z_WS[stable]/L[stable]-cc/dd)*np.exp(-dd*z_WS[stable]/L[stable]) + bb*cc/dd)
    psi_h1 = -(aa*z_0h[stable]/L[stable] + bb*(z_0h[stable]/L[stable]-cc/dd)*np.exp(-dd*z_0h[stable]/L[stable]) + bb*cc/dd)
    psi_h2 = -(aa* z_T[stable]/L[stable] + bb*( z_T[stable]/L[stable]-cc/dd)*np.exp(-dd* z_T[stable]/L[stable]) + bb*cc/dd)
    u_star[stable] = kappa*WS_h[stable]/(np.log(z_WS[stable]/z_0)-psi_m2+psi_m1)
    Re[stable] = u_star[stable]*z_0/nu[stable]
    z_0h[stable] = z_0*np.exp(1.5-0.2*np.log(Re[stable])-0.11*(np.log(Re[stable]))**2)
    # if n_elements(where(z_0h[stable] lt 1e-6)) gt 1 then z_0h[stable[where(z_0h[stable] lt 1e-6)]] = 1e-6
    z_0h[stable][z_0h[stable] < 1E-6] == 1E-6
    th_star = kappa*(theta[stable]-Tsurf_h[stable])/(np.log(z_T[stable]/z_0h[stable])-psi_h2+psi_h1)
    q_star  = kappa*(  q_h[stable]- q_surf[stable])/(np.log(z_T[stable]/z_0h[stable])-psi_h2+psi_h1)
    SHF_h[stable] = rho_atm[stable]*c_pd *u_star[stable]*th_star
    LHF_h[stable] = rho_atm[stable]*L_sub*u_star[stable]* q_star
    L_prev = L[stable]
    L[stable] = u_star[stable]**2*(theta[stable]+T_0)*(1+((1-eps)/eps)*q_h[stable])/(g*kappa*th_star*(1+((1-eps)/eps)*q_star))
    L_dif = np.abs((L_prev-L[stable])/L_prev)
    # print,"HF iterations stable stratification: ",i+1,n_elements(where(L_dif gt L_dif_max)),100.*n_elements(where(L_dif gt L_dif_max))/n_elements(where(L_dif))
    # if n_elements(where(L_dif gt L_dif_max)) eq 1 then break
    if np.all(L_dif <= L_dif_max):
        print("LDIF BREAK: ", i)
        break

if len(unstable) > 0:
    for i in np.arange(0,21):
        x1  = (1-gamma*z_0           /L[unstable])**0.25
        x2  = (1-gamma*z_WS[unstable]/L[unstable])**0.25
        y1  = (1-gamma*z_0h[unstable]/L[unstable])**0.5
        y2  = (1-gamma*z_T[unstable] /L[unstable])**0.5
        psi_m1 = np.log(((1+x1)/2)**2*(1+x1**2)/2)-2*np.arctan(x1)+np.pi/2
        psi_m2 = np.log(((1+x2)/2)**2*(1+x2**2)/2)-2*np.arctan(x2)+np.pi/2
        psi_h1 = np.log(((1+y1)/2)**2)
        psi_h2 = np.log(((1+y2)/2)**2)
        u_star[unstable] = kappa*WS_h[unstable]/(np.log(z_WS[unstable]/z_0)-psi_m2+psi_m1)
        Re[unstable] = u_star[unstable]*z_0/nu[unstable]
        z_0h[unstable] = z_0*np.exp(1.5-0.2*np.log(Re[unstable])-0.11*(np.log(Re[unstable]))**2)
        # if n_elements(where(z_0h[unstable] lt 1e-6)) gt 1 then z_0h[unstable[where(z_0h[unstable] lt 1e-6)]] = 1e-6
        z_0h[stable][z_0h[stable] < 1E-6] == 1E-6
        th_star = kappa*(theta[unstable]-Tsurf_h[unstable])/(np.log(z_T[unstable]/z_0h[unstable])-psi_h2+psi_h1)
        q_star  = kappa*(  q_h[unstable]- q_surf[unstable])/(np.log(z_T[unstable]/z_0h[unstable])-psi_h2+psi_h1)
        SHF_h[unstable] = rho_atm[unstable]*c_pd *u_star[unstable]*th_star
        LHF_h[unstable] = rho_atm[unstable]*L_sub*u_star[unstable]* q_star
        L_prev = L[unstable]
        L[unstable] = u_star[unstable]**2*(theta[unstable]+T_0)*(1+((1-eps)/eps)*q_h[unstable])/(g*kappa*th_star*(1+((1-eps)/eps)*q_star))
        L_dif = abs((L_prev-L[unstable])/L_prev)
        # print,"HF iterations unstable stratification: ",i+1,n_elements(where(L_dif gt L_dif_max)),100.*n_elements(where(L_dif gt L_dif_max))/n_elements(where(L_dif))
        # if n_elements(where(L_dif gt L_dif_max)) eq 1 then break
        if np.all(L_dif <= L_dif_max):
            print("LDIF BREAK: ", i)
            break

           
q_h = 1000 * q_h            # from kg/kg to g/kg
HF_nan = np.isnan(p_h) | np.isnan(T_h) | np.isnan(Tsurf_h) | np.isnan(RH_cor_h) | np.isnan(WS_h) | np.isnan(ds_h['z_boom'])
qh_nan = np.isnan(T_h) | np.isnan(RH_cor_h) | np.isnan(p_h) | np.isnan(Tsurf_h)
SHF_h[HF_nan] = np.nan
LHF_h[HF_nan] = np.nan
q_h[qh_nan] = np.nan

#+END_SRC

#+RESULTS:
: LDIF BREAK:  0
: LDIF BREAK:  0


** Export file as L3

+ Check with ~cfchecks ./data/L2/EGP/EGP-raw.nc~

#+BEGIN_SRC jupyter-python
outpath = os.path.split(infile)[0].split("/")
outpath[-2] = 'L3'
outpath = '/'.join(outpath)
outfile_base = os.path.splitext(os.path.basename(infile))[0]
outpathfile = outpath + '/' + outfile_base

ds.to_dataframe().dropna(how='all').to_csv(outpathfile+".csv", float_format="%.7f")

if os.path.exists(outpathfile+"_hour.nc"): os.remove(outpathfile+"_hour.nc")
ds_h.to_netcdf(outpathfile+"_hour.nc", mode='w', format='NETCDF4', compute=True)
ds_h.to_dataframe().dropna(how='all').to_csv(outpathfile+"_hour.csv", float_format="%.2f")

if os.path.exists(outpathfile+"_day.nc"): os.remove(outpathfile+"_day.nc")
ds_d.to_netcdf(outpathfile+"_day.nc", mode='w', format='NETCDF4', compute=True)
ds_d.to_dataframe().dropna(how='all').to_csv(outpathfile+"_day.csv")
#+END_SRC

#+RESULTS:





* L0 to L3

for s in CEN; do ./ppp.sh $s; done
for s in $(ls ./data/L0); do ./ppp.sh $s; done

for s in $(ls ./test_QAS/L0 | grep QAS); do echo $s; done
for s in $(ls ./test_QAS/L0 | grep QAS); do ./ppp.sh $s; done

#+BEGIN_SRC bash :tangle ppp.sh :tangle-mode (identity #o544) :var s="QAS_L"
s=$1
# conda activate PROMICE_dev

rm ./test_QAS/L{1,1A,2,3}/${s}/*

## L0 -> L1
# ./L0_to_L1.py ./data/L0/config/EGP_2016_raw.toml # 1 raw
# ./L0_to_L1.py ./data/L0/config/STM.toml # 1 STM
# ./L0_to_L1.py ./data/L0/config/TX.toml  # 1 TX
./L0_to_L1.py ./test_QAS/L0 ./test_QAS/L0/config/${s}.toml          # all EGP stations

# L1 -> L1A
# for f in $(ls ./test_QAS/L1/${s}/); do
#   echo $f
#   ./L1_to_L1A.py ./test_QAS/L1/${s}/${f}
# done
# # ./L1_to_L1A.py test_QAS/L1/${s}/*raw.nc
# # ./L1_to_L1A.py test_QAS/L1/${s}/*STM.nc
# # ./L1_to_L1A.py test_QAS/L1/${s}/*TX.nc
# parallel --bar "./L1_to_L1A.py {}" ::: $(ls test_QAS/L1/${s}/*)
./L1_to_L1A.py ./test_QAS/L1/${s}/*

# L1A to L2
# ./L1A_to_L2.py test_QAS/L1A/${s}/${s}-raw.nc
# ./L1A_to_L2.py test_QAS/L1A/${s}/${s}-STM.nc
# ./L1A_to_L2.py test_QAS/L1A/${s}/${s}-TX.nc
# parallel --bar "./L1A_to_L2.py {}" ::: $(ls test_QAS/L1A/${s}/*)
./L1A_to_L2.py ./test_QAS/L1A/${s}/*

# L2 to L3
# ./L2_to_L3.py test_QAS/L2/${s}/*raw.nc
# ./L2_to_L3.py test_QAS/L2/${s}/*STM.nc
# ./L2_to_L3.py test_QAS/L2/${s}/*TX.nc
# parallel --bar "./L2_to_L3.py {}" ::: $(ls test_QAS/L2/${s}/*)
./L2_to_L3.py ./test_QAS/L2/${s}/*

#+END_SRC


* Helper functions

#+NAME: constants
#+BEGIN_SRC jupyter-python
import numpy as np

deg2rad = np.pi / 180
rad2deg = 1 / deg2rad
#+END_SRC


** Load L0 hdr + data

#+NAME: load_conf
#+BEGIN_SRC jupyter-python
import toml
import os

def load_conf(filename, L0_path):
    """Load a TOML file
    PROMICE TOML supports defining features at the top level which apply to all nested properties,
    but do not overwrite nested properties if they are defined.
    """
    conf = toml.load(filename)
    # Move all top level keys to nested properties,
    # if they are not already defined in the nested properties
    # Also, insert the section name (filename) as a file property, and configuration file.
    top = [_ for _ in conf.keys() if not type(conf[_]) is dict]
    subs = [_ for _ in conf.keys() if type(conf[_]) is dict]
    for s in subs:
        for t in top:
            if t not in conf[s].keys():
                conf[s][t] = conf[t]

        conf[s]['conf'] = filename
        conf[s]['file'] = os.path.join(L0_path, conf[s]['station_id'], s)

    # Delete all top level keys, because each file (sub-level) should carry all
    # properties with it.
    for t in top: conf.pop(t)

    # check required fields are present
    for k in conf.keys():
        for field in ["columns", "station_id", "format", "latitude", "longitude", "skiprows"]:
            assert(field in conf[k].keys())

    return conf
#+END_SRC

#+RESULTS: load_conf


#+NAME: read_L0
#+BEGIN_SRC jupyter-python
import os
import numpy as np
import pandas as pd
import pathlib
pd.set_option('display.precision', 2)
import xarray as xr
xr.set_options(keep_attrs=True)

def read_L0(conf):

    df = pd.read_csv(conf['file'],
                     comment = "#",
                     index_col = 0,
                     na_values = conf['nodata'],
                     names = conf['columns'],
                     parse_dates = True,
                     sep = ",",
                     skiprows = conf["skiprows"],
                     skip_blank_lines = True,
                     usecols=np.arange(len(conf['columns'])))

    ds = xr.Dataset.from_dataframe(df)

    # carry relevant metadata with ds
    meta = {}
    skip = ["columns", "skiprows"]
    for k in conf.keys():
        if k not in skip: meta[k] = conf[k]
    ds.attrs = meta

    return ds
#+END_SRC

#+RESULTS: read_L0

** Flag invalid data

#+NAME: flag_NAN
#+BEGIN_SRC jupyter-python
def flag_NAN(ds):
    flag_file = "./data/flags/" + ds.attrs["station_id"] + ".csv"

    if not pathlib.Path(flag_file).is_file(): return ds # no flag file
    
    df = pd.read_csv(flag_file, parse_dates=[0,1], comment="#")\
           .dropna(how='all', axis='rows')

    # check format of flags.csv. Either both or neither of t0 and t1 must be defined.
    assert(((np.isnan(df['t0'].values).astype(int) + np.isnan(df['t1'].values).astype(int)) % 2).sum() == 0)
    # for now we only process the NAN flag
    df = df[df['flag'] == "NAN"]
    if df.shape[0] == 0: return ds

    for i in df.index:
        t0, t1, avar = df.loc[i,['t0','t1','variable']]
        # set to all vars if var is "*"
        varlist = avar.split() if avar != '*' else list(ds.variables)
        if 'time' in varlist: varlist.remove("time")
        # set to all times if times are "n/a"
        if pd.isnull(t0): t0, t1 = ds['time'].values[[0,-1]]
        for v in varlist:
            ds[v] = ds[v].where((ds['time'] < t0) | (ds['time'] > t1))

        # TODO: Mark these values in the ds_flags dataset using perhaps flag_LUT.loc["NAN"]['value']

    return ds
#+END_SRC

#+RESULTS: flag_NAN

** Add variable metadata

This function reads in the variables db ([[./variables.csv]]) and adds the metadata contained therein to the xarray variable (and therefore, eventually, the NetCDF file). See [[./variables.org]] for documentation on the variable DB.

#+NAME: add_variable_metadata
#+BEGIN_SRC jupyter-python
def add_variable_metadata(ds):
    """Uses the variable DB (variables.csv) to add metadata to the xarray dataset."""
    df = pd.read_csv("./variables.csv", index_col=0, comment="#")

    for v in df.index:
        if v == 'time': continue # coordinate variable, not normal var
        if v not in list(ds.variables): continue
        for c in ['standard_name', 'long_name', 'units']:
            if isinstance(df[c][v], float) and np.isnan(df[c][v]): continue
            ds[v].attrs[c] = df[c][v]
            
    return ds
#+END_SRC

** metadata.csv to TOML

Automatically generate v4 TOML files from the v3 metadata.csv files

#+BEGIN_SRC jupyter-python
import glob
import pandas as pd

mlist = glob.glob('./data/L0/metadata/*_metadata.csv')

rename = {'latitude_N(dd.ddddd)' : 'latitude',
          'longitude_W(dd.ddddd)' : 'longitude',
          'T_hygroclip_offset(C)' : 'hygroclip_t_offset',
          'SRin_calcoef' : 'dsr_eng_coef',
          'SRout_calcoef' : 'usr_eng_coef',
          'LRin_calcoef' : 'dlr_eng_coef',
          'LRout_calcoef' : 'ulr_eng_coef',
          'H_PT_calcoef' : 'pt_z_coef',
          'H_PT_p_cal' : 'pt_z_p_coef',
          'H_PT_factor' : 'pt_z_factor',
          'header_lines' : 'skiprows',
          'antifreeze_PT_(%)' : 'pt_antifreeze',
          'rotation_ini(deg)' : 'boom_azimuth',
          'col_minute_of_year' : 'min_y',
          'col_p' : 'p',
          'col_T' : 't_1',
          'col_T_hygroclip' : 't_2',
          'col_RH' : 'rh',
          'col_WS' : 'wspd',
          'col_WD' : 'wdir',
          'col_WD_sd' : 'wd_std',
          'col_SRin' : 'dsr',
          'col_SRout' : 'usr',
          'col_LRin' : 'dlr',
          'col_LRout' : 'ulr',
          'col_T_rad' : 't_rad',
          'col_H_aws' : 'z_boom',
          'col_qual_H_aws' : 'z_boom_q',
          'col_H_stk' : 'z_stake',
          'col_qual_H_stk' : 'z_stake_q',
          'col_H_pt' : 'z_pt',
          'col_Tice1' : 't_i_1',
          'col_Tice2' : 't_i_2',
          'col_Tice3' : 't_i_3',
          'col_Tice4' : 't_i_4',
          'col_Tice5' : 't_i_5',
          'col_Tice6' : 't_i_6',
          'col_Tice7' : 't_i_7',
          'col_Tice8' : 't_i_8',
          'col_ornt' : 'ORIENTATION',
          'col_tiltX' : 'tilt_x',
          'col_tiltY' : 'tilt_y',
          'col_GPStime' : 'gps_time',
          'col_GPSlat' : 'gps_lat',
          'col_GPSlon' : 'gps_lon',
          'col_GPSelev' : 'gps_alt',
          'col_GPSgeoid' : 'gps_geoid',
          'col_GPSqual' : 'gps_q',
          'col_GPSnumsats' : 'gps_numsats',
          'col_GPShdop' : 'gps_hdop',
          'col_Tlog' : 't_log',
          'col_Ifan' : 'fan_dc',
          'col_Vbat_ini' : 'batt_v_ss',
          'col_Vbat' : 'batt_v',
          'col_season' : 'SEASON'}


for md in mlist:
    df = pd.read_csv(md, index_col=0).rename(columns=rename)

    station = '_'.join(md.split("/")[-1].split("_")[:-1])
    out_file = open('./data/L0/config/' + station + '.toml', 'w')
    print(f"station_id = '{station}'", file=out_file)
    print("nodata     = ['-999', 'NAN'] # if one is a string, all must be strings\n", file=out_file)
          
    for f in df['filename']:
        ddf = df[df['filename'] == f].T
        if len(ddf.loc['filename']) == 0: continue
        if ddf.loc['filename'].values[0][-4:] != ".txt": continue

        print("['{}']".format(ddf.loc['filename'].values[0]), file=out_file)
        if 'transmitted' in str(ddf.loc['filename']):
            print("format     = 'TX'", file=out_file)
        elif 'SlimTable' in str(ddf.loc['filename']):
            print("format     = 'STM'", file=out_file)
        else:
            print("format     = 'raw'", file=out_file)

        for n in ['skiprows', 'latitude', 'longitude', 'hygroclip_t_offset', 'dsr_eng_coef', 'usr_eng_coef', 'dlr_eng_coef', 'ulr_eng_coef', 'pt_z_coef', 'pt_z_p_coef', 'pt_z_factor', 'pt_antifreeze', 'boom_azimuth']:
            print(n +  ' = {}'.format(np.round(ddf.loc[n].values[0],4)), file=out_file)

        cols = ddf.loc[['min_y','p', 't_1', 't_2', 'rh', 'wspd', 'wdir', 'wd_std', 'dsr', 'usr', 'dlr', 'ulr', 't_rad',
                        'z_boom', 'z_boom_q', 'z_stake', 'z_stake_q', 'z_pt', 't_i_1', 't_i_2', 't_i_3', 't_i_4',
                        't_i_5', 't_i_6', 't_i_7', 't_i_8', 'ORIENTATION', 'tilt_x', 'tilt_y', 'gps_time',
                        'gps_lat', 'gps_lon', 'gps_alt', 'gps_geoid', 'gps_q', 'gps_numsats', 'gps_hdop', 't_log',
                        'fan_dc', 'batt_v_ss', 'batt_v', 'SEASON']]
        cols.loc['rec'] = 2
        cols.loc['time'] = 1
        cols.columns = ['col']
        for i in np.arange(int(cols.max())):
            if i not in cols['col'].values:
                cols.loc['SKIP_' + str(i)] = i
        cols = cols.sort_values(by='col').replace(0,np.nan).dropna()
        print('columns = {}'.format(np.array2string(cols.index.values, separator=',', max_line_width=60)), file=out_file)
            
        # print('latitude   = {}'.format(str(ddf.loc['latitude'].values[0])), file=out_file)
        # print('longitude  = {}'.format(-ddf.loc['longitude'].values[0]), file=out_file)
        print('\n', file=out_file)
    out_file.close()

ddf    
#+END_SRC

#+RESULTS:
|                    |                              9 |
|--------------------+--------------------------------|
| filename           | TAS_A_2020_raw_transmitted.txt |
| slimtablemem?      |                             no |
| transmitted?       |                            yes |
| skiprows           |                              0 |
| data_lines         |                            884 |
| columns            |                             38 |
| year start         |                           2019 |
| latitude           |                          65.78 |
| longitude          |                           38.9 |
| UTC_offset(h)      |                              0 |
| hygroclip_t_offset |                              0 |
| dsr_eng_coef       |                          14.15 |
| usr_eng_coef       |                          12.97 |
| dlr_eng_coef       |                          10.23 |
| ulr_eng_coef       |                          13.47 |
| pt_z_coef          |                          0.407 |
| pt_z_p_coef        |                          982.1 |
| pt_z_factor        |                            2.5 |
| pt_antifreeze      |                             50 |
| boom_azimuth       |                              0 |
| col_datetime       |                              1 |
| col_date           |                              0 |
| col_year           |                              0 |
| col_month          |                              0 |
| col_day            |                              0 |
| col_day_of_year    |                              0 |
| col_time           |                              0 |
| col_hour           |                              0 |
| col_minute         |                              0 |
| min_y              |                              0 |
| p                  |                              3 |
| t_1                |                              4 |
| t_2                |                              5 |
| rh                 |                              6 |
| wspd               |                              7 |
| wdir               |                              8 |
| wd_std             |                              0 |
| dsr                |                              9 |
| usr                |                             10 |
| dlr                |                             11 |
| ulr                |                             12 |
| t_rad              |                             13 |
| z_boom             |                             14 |
| z_boom_q           |                              0 |
| z_stake            |                             15 |
| z_stake_q          |                              0 |
| z_pt               |                             16 |
| t_i_1              |                             17 |
| t_i_2              |                             18 |
| t_i_3              |                             19 |
| t_i_4              |                             20 |
| t_i_5              |                             21 |
| t_i_6              |                             22 |
| t_i_7              |                             23 |
| t_i_8              |                             24 |
| ORIENTATION        |                              0 |
| tilt_x             |                             25 |
| tilt_y             |                             26 |
| gps_time           |                             27 |
| gps_lat            |                             28 |
| gps_lon            |                             29 |
| gps_alt            |                             30 |
| gps_geoid          |                              0 |
| gps_q              |                              0 |
| gps_numsats        |                              0 |
| gps_hdop           |                             31 |
| t_log              |                              0 |
| fan_dc             |                             32 |
| batt_v_ss          |                              0 |
| batt_v             |                             33 |
| SEASON             |                              0 |

* Compare Python & IDL
:PROPERTIES:
:header-args:jupyter-python+: :session compare
:END:
** Load both to dfs (10 min)

#+BEGIN_SRC jupyter-python :tangle compare.py
import numpy as np
import pandas as pd
import xarray as xr

station='QAS_L'
# if 'idl' in locals(): del(idl)
if 'df' in locals(): del(df)

def mydf(y,m,d,h):
    return pd.to_datetime(f'{int(y)}-{int(m)}-{int(d)}:{int(h)}', format='%Y-%m-%d:%H')

def mydf2(y,mo,d,h,mi):
    return pd.to_datetime(f'{int(y)}-{int(mo)}-{int(d)}:{int(h)}:{int(mi)}', format='%Y-%m-%d:%H:%M')

if 'idl' not in locals():
    ## INST
    #idl = pd.read_csv("./IDL/out/"+station+"_inst_v03.txt",
    idl = pd.read_csv("/home/kdm/data.me/PROMICE/inst/"+station+"_inst_v03.txt",
                      delimiter="\s+",
                      parse_dates={'time':[0,1,2,3,4]},
                      infer_datetime_format=True,
                      date_parser=mydf2,
                      index_col=0)
    
    ## HOUR
    # idl = pd.read_csv("./IDL/out/EGP_hour_v03.txt",
    #                   delimiter="\s+",
    #                   parse_dates={'time':[0,1,2,3]},
    #                   infer_datetime_format=True,
    #                   date_parser=mydf,
    #                   index_col=0)

    idl = idl.drop(columns=['DayOfYear'])\
             .replace(-999, np.nan)\
             .apply(pd.to_numeric, errors='coerce')\
             .dropna(how='all')\
             .rename(columns={'AirPressure(hPa)' : 'p',
                          'AirTemperature(C)' : 't_1',
                          'AirTemperatureHygroClip(C)' : 't_2',
                          'RelativeHumidity(%)' : 'rh_cor',
                          'SpecificHumidity(g/kg)' : 'rh_cor2',
                          'WindSpeed(m/s)' : 'wspd',
                          'WindDirection(d)' : 'wdir',
                          'SensibleHeatFlux(W/m2)' : 'shf',
                          'LatentHeatFlux(W/m2)' : 'lhf',
                          'ShortwaveRadiationDown(W/m2)': 'dsr',
                          'ShortwaveRadiationDown_Cor(W/m2)' : 'dsr_cor',
                          'ShortwaveRadiationUp(W/m2)' : 'usr',
                          'ShortwaveRadiationUp_Cor(W/m2)' : 'usr_cor',
                          'Albedo_theta<70d' : 'albedo',
                          'Albedo' : 'albedo',
                          'LongwaveRadiationDown(W/m2)' : 'dlr',
                          'LongwaveRadiationUp(W/m2)' : 'ulr',
                          'CloudCover' : 'cc',
                          'SurfaceTemperature(C)' : 't_surf',
                          'HeightSensorBoom(m)' : 'z_boom',
                          'HeightStakes(m)': 'z_stake',
                          'DepthPressureTransducer(m)' : 'z_pt',
                          'DepthPressureTransducer_Cor(m)' : 'z_pt_cor',
                          'IceTemperature1(C)' : 't_i_1',
                          'IceTemperature2(C)' : 't_i_2',
                          'IceTemperature3(C)' : 't_i_3',
                          'IceTemperature4(C)' : 't_i_4',
                          'IceTemperature5(C)' : 't_i_5',
                          'IceTemperature6(C)' : 't_i_6',
                          'IceTemperature7(C)' : 't_i_7',
                          'IceTemperature8(C)' : 't_i_8',
                          'TiltToEast(d)' : 'tilt_x',
                          'TiltToNorth(d)' : 'tilt_y',
                          'TimeGPS(hhmmssUTC)' : 'gps_t',
                          'LatitudeGPS(degN)' : 'gps_lat',
                          'LongitudeGPS(degW)' : 'gps_lon',
                          'ElevationGPS(m)' : 'gps_alt',
                          'HorDilOfPrecGPS' : 'gps_hdop',
                          'LoggerTemperature(C)' : 't_logger',
                          'FanCurrent(mA)' : 'fan_dc',
                          'BatteryVoltage(V)' : 'batt_v'})

if 'df' not in locals():
    # df = xr.open_mfdataset("./data/L3/EGP/EGP-*_hour.nc", mask_and_scale=False)\
        #        .to_dataframe()

    # df = pd.read_csv("./data/L3/"+station+"/"+station+"-raw.csv", index_col=0, parse_dates=True)
    # df = pd.read_csv("./data/L3/"+station+"/"+station+"-STM.csv", index_col=0, parse_dates=True)
    # df = pd.read_csv("./data/L3/"+station+"/"+station+"-TX.csv", index_col=0, parse_dates=True)


    df = pd.read_csv("./test_QAS/L3/"+station+"/"+station+"-TX_hour.csv",
                      delimiter="\s+",
                      parse_dates={'time':[0,1,2,3,4]},
                      infer_datetime_format=True,
                      date_parser=mydf2,
                      index_col=0)
    
    df = df.drop(columns=['DayOfYear'])\
             .replace(-999, np.nan)\
             .apply(pd.to_numeric, errors='coerce')\
             .dropna(how='all')\
             .rename(columns={'AirPressure(hPa)' : 'p',
                          'AirTemperature(C)' : 't_1',
                          'AirTemperatureHygroClip(C)' : 't_2',
                          'RelativeHumidity(%)' : 'rh_cor',
                          'SpecificHumidity(g/kg)' : 'rh_cor2',
                          'WindSpeed(m/s)' : 'wspd',
                          'WindDirection(d)' : 'wdir',
                          'SensibleHeatFlux(W/m2)' : 'shf',
                          'LatentHeatFlux(W/m2)' : 'lhf',
                          'ShortwaveRadiationDown(W/m2)': 'dsr',
                          'ShortwaveRadiationDown_Cor(W/m2)' : 'dsr_cor',
                          'ShortwaveRadiationUp(W/m2)' : 'usr',
                          'ShortwaveRadiationUp_Cor(W/m2)' : 'usr_cor',
                          'Albedo_theta<70d' : 'albedo',
                          'Albedo' : 'albedo',
                          'LongwaveRadiationDown(W/m2)' : 'dlr',
                          'LongwaveRadiationUp(W/m2)' : 'ulr',
                          'CloudCover' : 'cc',
                          'SurfaceTemperature(C)' : 't_surf',
                          'HeightSensorBoom(m)' : 'z_boom',
                          'HeightStakes(m)': 'z_stake',
                          'DepthPressureTransducer(m)' : 'z_pt',
                          'DepthPressureTransducer_Cor(m)' : 'z_pt_cor',
                          'IceTemperature1(C)' : 't_i_1',
                          'IceTemperature2(C)' : 't_i_2',
                          'IceTemperature3(C)' : 't_i_3',
                          'IceTemperature4(C)' : 't_i_4',
                          'IceTemperature5(C)' : 't_i_5',
                          'IceTemperature6(C)' : 't_i_6',
                          'IceTemperature7(C)' : 't_i_7',
                          'IceTemperature8(C)' : 't_i_8',
                          'TiltToEast(d)' : 'tilt_x',
                          'TiltToNorth(d)' : 'tilt_y',
                          'TimeGPS(hhmmssUTC)' : 'gps_t',
                          'LatitudeGPS(degN)' : 'gps_lat',
                          'LongitudeGPS(degW)' : 'gps_lon',
                          'ElevationGPS(m)' : 'gps_alt',
                          'HorDilOfPrecGPS' : 'gps_hdop',
                          'LoggerTemperature(C)' : 't_logger',
                          'FanCurrent(mA)' : 'fan_dc',
                          'BatteryVoltage(V)' : 'batt_v'})


    
    # subset to same columns
    subset = np.intersect1d(df.columns, idl.columns)
    df = df[subset]
    idl = idl[subset]

    err = df - idl # need to understand data to understand error
    err_pct = err / idl.mean(axis='rows')*100 # % err but should work as long as mean != 0
    # err = (df - idl) / idl*100 # % err: large errors when values near 0
    # err = (df - idl) / idl.mean(axis='rows')*100 # % err but should work as long as mean != 0

    pd.options.display.float_format = "{:,.5f}".format

    err_desc = err.describe().T.drop(columns="count")
    err_pct_desc = err_pct.describe().T.drop(columns="count")

# diff_pct.plot()
# diff_pct.replace(0,np.nan).dropna(how='all', axis='columns').plot()
def plot_diff(df,idl,err,err_pct,var):
    import matplotlib.pyplot as plt
    fig = plt.figure(1)
    fig.clf()
    ax1 = fig.add_subplot(211)
    err[var].plot(label='err', color='red', marker='.', ax=ax1)
    ax1.set_ylabel("Err [units]")
    ax1_pct = ax1.twinx()
    err_pct[var].plot(label='err', color='black', marker='.', ax=ax1_pct)
    ax1_pct.set_ylabel("Err [%]")
    
    ax2 = fig.add_subplot(212, sharex=ax1)
    idl[var].plot(label='IDL '+var, linewidth=3, ax=ax2, marker='.', markersize=4)
    df[var].plot(label='Py', ax=ax2, marker='.', markersize=2)
    ax2.set_ylabel(var + " [units]")
    # (df[var]*0).plot(color='k', linestyle='--', ax=ax2, alpha=0.25, label='', marker='.')
    legend()
    
var = 't_1'; plot_diff(df,idl,err,err_pct,var)

# err_desc
# err_pct_desc
desc = err_desc.round(3).astype("string")
desc = desc + " (" + err_pct_desc.replace(np.nan,0).round().astype(int).astype("string") + ")"
desc

# if __name__ == "__main__":
#     print(desc)
#+END_SRC


** Compare 2

#+BEGIN_SRC jupyter-python
import numpy as np
import pandas as pd
import xarray as xr

station='QAS_L'

# if 'idl' in locals(): del(idl)
if 'df' in locals(): del(df)

def mydf(y,m,d,h):
    return pd.to_datetime(y+'-'+m+'-'+d+':'+h, format='%Y-%m-%d:%H')

## HOUR
idl = pd.read_csv("./test_QAS/processed_data/"+station+"_hour_v03.txt",
                  delimiter="\s+",
                  parse_dates={'time':[0,1,2,3]},
                  infer_datetime_format=True,
                  date_parser=mydf,
                  index_col=0)

idl = idl.drop(columns=['DayOfYear'])\
         .replace(-999, np.nan)\
         .apply(pd.to_numeric, errors='coerce')\
         .dropna(how='all')\
         .rename(columns={'AirPressure(hPa)' : 'p',
                          'AirTemperature(C)' : 't_1',
                          'AirTemperatureHygroClip(C)' : 't_2',
                          'RelativeHumidity(%)' : 'rh_cor',
                          'SpecificHumidity(g/kg)' : 'rh_cor2',
                          'WindSpeed(m/s)' : 'wspd',
                          'WindDirection(d)' : 'wdir',
                          'SensibleHeatFlux(W/m2)' : 'shf',
                          'LatentHeatFlux(W/m2)' : 'lhf',
                          'ShortwaveRadiationDown(W/m2)': 'dsr',
                          'ShortwaveRadiationDown_Cor(W/m2)' : 'dsr_cor',
                          'ShortwaveRadiationUp(W/m2)' : 'usr',
                          'ShortwaveRadiationUp_Cor(W/m2)' : 'usr_cor',
                          'Albedo_theta<70d' : 'albedo',
                          'Albedo' : 'albedo',
                          'LongwaveRadiationDown(W/m2)' : 'dlr',
                          'LongwaveRadiationUp(W/m2)' : 'ulr',
                          'CloudCover' : 'cc',
                          'SurfaceTemperature(C)' : 't_surf',
                          'HeightSensorBoom(m)' : 'z_boom',
                          'HeightStakes(m)': 'z_stake',
                          'DepthPressureTransducer(m)' : 'z_pt',
                          'DepthPressureTransducer_Cor(m)' : 'z_pt_cor',
                          'IceTemperature1(C)' : 't_i_1',
                          'IceTemperature2(C)' : 't_i_2',
                          'IceTemperature3(C)' : 't_i_3',
                          'IceTemperature4(C)' : 't_i_4',
                          'IceTemperature5(C)' : 't_i_5',
                          'IceTemperature6(C)' : 't_i_6',
                          'IceTemperature7(C)' : 't_i_7',
                          'IceTemperature8(C)' : 't_i_8',
                          'TiltToEast(d)' : 'tilt_x',
                          'TiltToNorth(d)' : 'tilt_y',
                          'TimeGPS(hhmmssUTC)' : 'gps_t',
                          'LatitudeGPS(degN)' : 'gps_lat',
                          'LongitudeGPS(degW)' : 'gps_lon',
                          'ElevationGPS(m)' : 'gps_alt',
                          'HorDilOfPrecGPS' : 'gps_hdop',
                          'LoggerTemperature(C)' : 't_logger',
                          'FanCurrent(mA)' : 'fan_dc',
                          'BatteryVoltage(V)' : 'batt_v'})


    
df = pd.read_csv("./test_QAS/L3/"+station+"/"+station+"-TX_hour.csv",
                 index_col=0, parse_dates=True)


# subset to same columns
subset = np.intersect1d(df.columns, idl.columns)
df = df[subset]
idl = idl[subset]

df = df['2021-10-22':'2021-10-28']
idl = idl['2021-10-22':'2021-10-28']

err = df - idl # need to understand data to understand error
err_pct = (err / idl.mean(axis='rows'))*100 # % err but should work as long as mean != 0
# err = (df - idl) / idl*100 # % err: large errors when values near 0
# err = (df - idl) / idl.mean(axis='rows')*100 # % err but should work as long as mean != 0

pd.options.display.float_format = "{:,.5f}".format

err_desc = err.describe().T.drop(columns="count")
err_pct_desc = err_pct.describe().T.drop(columns="count")

# diff_pct.plot()
# diff_pct.replace(0,np.nan).dropna(how='all', axis='columns').plot()
def plot_diff(df,idl,err,err_pct,var):
    import matplotlib.pyplot as plt
    fig = plt.figure(1)
    fig.clf()
    ax1 = fig.add_subplot(211)
    err[var].plot(label='err', color='red', marker='.', ax=ax1, linewidth=2)
    ax1.set_ylim(ax1.get_ylim()[0]*1.3, ax1.get_ylim()[1])
    ax1.set_ylabel("Err [units]")
    ax1_pct = ax1.twinx()
    err_pct[var].plot(label='err', color='black', marker='.', ax=ax1_pct, linewidth=0.5)
    ax1_pct.set_ylabel("Err [%]")
    # ax1.spines['left'].set_color('red')
    ax1.tick_params(axis='y', colors='red')
    ax1.yaxis.label.set_color('red')
    ax1.title.set_color('red')
    
    ax2 = fig.add_subplot(212, sharex=ax1)
    idl[var].plot(label='IDL '+var, linewidth=3, ax=ax2, marker='.', markersize=4)
    df[var].plot(label='Py '+var, ax=ax2, marker='.', markersize=3)
    ax2.set_ylabel(var + " [units]")
    # (df[var]*0).plot(color='k', linestyle='--', ax=ax2, alpha=0.25, label='', marker='.')
    legend()
    
var = 'wspd'; plot_diff(df,idl,err,err_pct,var)

# # err_desc
# # err_pct_desc
desc = err_desc.round(3).astype("string")
desc = desc + " (" + err_pct_desc.replace(np.nan,0).round().astype(int).astype("string") + ")"
desc

if __name__ == "__main__":
    print(desc.drop(columns=['std']))

#+END_SRC

#+RESULTS:
#+begin_example
                  mean            min         25%         50%        75%          max
albedo            <NA>           <NA>        <NA>        <NA>       <NA>         <NA>
batt_v         0.0 (0)        0.0 (0)     0.0 (0)     0.0 (0)    0.0 (0)      0.0 (0)
cc          -0.001 (0)    -0.17 (-34)     0.0 (0)     0.0 (0)    0.0 (0)    0.18 (36)
dlr          0.092 (0)    -13.79 (-5)   -0.03 (0)     0.0 (0)   0.03 (0)    18.32 (7)
dsr         -0.084 (0)  -59.49 (-144)   -0.03 (0)     0.0 (0)   0.03 (0)  71.08 (172)
dsr_cor           <NA>           <NA>        <NA>        <NA>       <NA>         <NA>
fan_dc         0.0 (0)        0.0 (0)     0.0 (0)     0.0 (0)    0.0 (0)      0.0 (0)
gps_alt        0.0 (0)        0.0 (0)     0.0 (0)     0.0 (0)    0.0 (0)      0.0 (0)
gps_hdop       0.0 (0)        0.0 (0)     0.0 (0)     0.0 (0)    0.0 (0)      0.0 (0)
gps_lat     -0.001 (0)     -0.001 (0)  -0.001 (0)  -0.001 (0)   -0.0 (0)     -0.0 (0)
gps_lon        0.0 (0)        0.0 (0)     0.0 (0)     0.0 (0)    0.0 (0)    0.001 (0)
p           -0.024 (0)       -1.0 (0)     0.0 (0)     0.0 (0)    0.0 (0)      1.0 (0)
rh_cor      -0.109 (0)    -8.63 (-12)   -0.03 (0)     0.0 (0)   0.02 (0)    7.51 (11)
t_1        0.034 (-84)  -0.06 (-1843)     0.0 (0)     0.0 (0)    0.0 (0)   0.75 (147)
t_2       0.032 (-128)  -0.27 (-3164)     0.0 (0)     0.0 (0)    0.0 (0)  0.78 (1095)
t_i_1          0.0 (0)        0.0 (0)     0.0 (0)     0.0 (0)    0.0 (0)      0.0 (0)
t_i_2       -0.001 (1)      -0.01 (0)     0.0 (0)     0.0 (0)    0.0 (0)     0.0 (13)
t_i_3          0.0 (0)        0.0 (0)     0.0 (0)     0.0 (0)    0.0 (0)      0.0 (0)
t_i_4       -0.001 (2)    -0.01 (-16)     0.0 (0)     0.0 (0)    0.0 (0)    0.01 (16)
t_i_5          0.0 (0)        0.0 (0)     0.0 (0)     0.0 (0)    0.0 (0)      0.0 (0)
t_i_6          0.0 (0)        0.0 (0)     0.0 (0)     0.0 (0)    0.0 (0)      0.0 (0)
t_i_7          0.0 (0)        0.0 (0)     0.0 (0)     0.0 (0)    0.0 (0)      0.0 (0)
t_i_8          0.0 (0)        0.0 (0)     0.0 (0)     0.0 (0)    0.0 (0)      0.0 (0)
t_surf       0.008 (0)    -0.23 (-44)     0.0 (0)     0.0 (0)    0.0 (0)    0.87 (12)
tilt_x      -0.101 (3)      -0.72 (0)     0.0 (0)     0.0 (0)    0.0 (0)     0.0 (18)
tilt_y     -0.109 (-4)    -0.71 (-27)     0.0 (0)     0.0 (0)    0.0 (0)      0.0 (0)
ulr          0.079 (0)      -1.32 (0)   -0.03 (0)    0.01 (0)   0.03 (0)     4.37 (1)
usr        -0.147 (-1)  -24.83 (-243)   -0.03 (0)     0.0 (0)   0.02 (0)   15.7 (153)
usr_cor           <NA>           <NA>        <NA>        <NA>       <NA>         <NA>
wdir          5.54 (5)   -12.28 (-11)   -0.02 (0)     0.0 (0)   0.01 (0)  316.5 (289)
wspd       -0.032 (-1)    -1.39 (-37)     0.0 (0)     0.0 (0)    0.0 (0)    1.62 (44)
z_boom       0.002 (0)     -0.004 (0)    -0.0 (0)   0.003 (0)  0.003 (0)    0.005 (0)
z_pt        -0.001 (0)     -0.005 (0)  -0.005 (0)  -0.004 (0)  0.005 (0)    0.005 (0)
z_stake        0.0 (0)     -0.004 (0)     0.0 (0)     0.0 (0)  0.001 (0)    0.003 (0)
#+end_example
