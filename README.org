
#+PROPERTY: header-args:jupyter-python :kernel PROMICE_dev :session PROMICE-README :exports both
#+PROPERTY: header-args:bash :exports both

* Table of contents                               :toc_3:noexport:
- [[#introduction][Introduction]]
  - [[#overview][Overview]]
- [[#level-0][Level 0]]
  - [[#l0-files][L0 files]]
    - [[#additional-files][Additional files]]
    - [[#l0-reader][L0 Reader]]
- [[#l0---l1][L0 -> L1]]
  - [[#wrapper][Wrapper]]
  - [[#imports][Imports]]
  - [[#read-in-file][Read in file]]
  - [[#eng-to-phys][Eng to phys]]
  - [[#export-file-as-l1][Export file as L1]]
- [[#level-1][Level 1]]
- [[#l1---l1a][L1 -> L1A]]
  - [[#wrapper-1][Wrapper]]
  - [[#imports-1][Imports]]
  - [[#merge-files][Merge files]]
  - [[#flag-data][Flag data]]
  - [[#export-file-as-l1a][Export file as L1A]]
- [[#level-1a][Level 1A]]
- [[#l1a---l2][L1A -> L2]]
  - [[#wrapper-2][Wrapper]]
  - [[#imports-2][Imports]]
  - [[#init][Init]]
  - [[#load][Load]]
  - [[#calibrate-using-secondary-sensors][Calibrate using secondary sensors]]
    - [[#correct-relative-humidity][Correct relative humidity]]
    - [[#cloud-cover][Cloud cover]]
    - [[#correct-shortwave-radiation][Correct shortwave radiation]]
    - [[#wind-direction][Wind direction]]
  - [[#export-file-as-l2][Export file as L2]]
- [[#l2---l3][L2 -> L3]]
  - [[#wrapper-3][Wrapper]]
  - [[#imports-3][Imports]]
  - [[#load-1][Load]]
  - [[#downsample-to-hourly-and-daily][Downsample to hourly and daily]]
    - [[#circular-averaging][Circular averaging]]
  - [[#derived-properties][Derived properties]]
    - [[#turbulent-heat-flux][Turbulent heat flux]]
  - [[#export-file-as-l3][Export file as L3]]
- [[#l0-to-l3][L0 to L3]]
- [[#helper-functions][Helper functions]]
  - [[#load-l0-hdr--data][Load L0 hdr + data]]
  - [[#flag-invalid-data][Flag invalid data]]
  - [[#add-variable-metadata][Add variable metadata]]
- [[#compare-python--idl][Compare Python & IDL]]
  - [[#graphic][Graphic]]
  - [[#columns][Columns]]
  - [[#load-both-to-dfs-10-min][Load both to dfs (10 min)]]

* Introduction

Code used to process the PROMICE AWS data from Level 0 through Level 3 (end-user product).

We use the following processing levels, described textually and graphically.

** Overview
+ L0: Raw data in CSV file format in one of three formats:
  + =raw=, =STM= (Slim Table Memory), and =TX= (transmitted)
  + Manually split so no file includes changed sensors
  + Manually created paired header files based on [[./hdr.template]]
+ L1:
  + Engineering units (e.g. current or volts) converted to physical units (e.g. temperature or wind speed)
+ L1A:
  + Invalid / bad / suspicious data flagged
  + Files merged to one time series per station
+ L2:
  + Calibration using secondary sources (e.g. radiometric correction requires input of tilt sensor)
+ L3:
  + Derived products (e.g. SHF and LHF)
  + Merge formats to one product here?

#+begin_src ditaa :file ./fig/levels.png :exports results

                    +----------------+
	            |{d}             |                         Legend
                    | Digital counts |                         +---------------+
                    |                |                         |input          |
		    | CR-1000 logger |                         +---------------+
	            |                |
	            +-------+--------+                         +---------------+   +=----+
	                    |				       |{io}process    +--=+ Note|
	                    v				       +---------------+   +-----+
                    +----------------+
	            |{io}            |                         +---------------+
                    |  Manual Carry  |      		       |{d}Files       |
                    |      or        |      		       +---------------+
		    |   Satellite    |
	            |                |			
	            +-------+--------+
	                    |                               +------------------+
	                    v         			  +-+Column names      |
                    +----------------+   +------------+   | +------------------+
	            |{d}             |   |{d}         |<--+
                    |  raw, STM, TX  |   |            |	    +------------------+
     Level 0 (L0)   |                |   |  L0 header |<----+Metadata          |
		    | GEUS text files|	 |            |	    +------------------+
	            |                |	 |            |<--+
	            +-------+--------+   +--+---------+   | +-----------------------------------+
	                    |               |	          +-+ Instrument calibration parameters |
                            |               |		    |      (recorded, not applied)      |
			    |  	+-----------+               +-----------------------------------+
	                    |	|			    
	                    v   v			    
	            +-----------------+           	            
	            |{io}             |                         
	            |  Engineering to |   	   	        
	            |  physical units |                         
	            |                 |   
                    +-------+---------+   
		            |      	  
	                    v             
                    +-----------------+   
		    |{d}              |   
    Level 1 (L1)    |Measured physical|   
		    |    properties   |
		    |                 |
		    +-------+---------+	  
                            |		  
                            v		  
                    +-----------------+
                    |{io}             |
                    |   Flag bad data |
                    |   Merge files   |
                    |                 |
                    +-------+---------+
                            |           
                            v          
                   +-------------------+
                   |{d}                |
    Level 1A (L1A) |Time series per AWS|
                   |  Initial data QC  |
		   |                   |
                   +-------+-----------+
                           |
                           v
                    +-----------------+
                    |{io}             |       +=------------------------------------------+ 
                    | Cross-sensor    |------=+e.g. ice at 1 m depth via interpolation, or| 
                    |  corrections    |       |radiation adjusting for platform rotation  |
                    |                 |       +-------------------------------------------+ 
                    +-------+---------+       
                            |          
                            v          
                   +-------------------+
                   |{d}                |
     Level 2 (L2)  |  Derived internal |
                   |      values       |
	           |                   |
                   +-------+-----------+
                           |
                           v
                    +-----------------+
                    |{io}             |
                    |     Derive      |       +=-----------------------+
                    |    external     |------=+e.g. sensible heat flux,|
                    |   properties    |       |latent heat flux        |
                    |                 |       +------------------------+
                    +-------+---------+
                            |          
                            v          
                   +-------------------+
                   |{d}                |
     Level 3 (L3)  |  Derived external |
                   |      values       |
		   |                   |
                   +-------------------+


#+END_SRC
		    
#+RESULTS:
[[file:./fig/levels.png]]

* Level 0

Level 0 is generated from one of three methods:
+ Copied from CF card in the field
+ Downloaded from logger box in the field
+ Transmitted via satellite and processed by https://github.com/GEUS-PROMICE/awsrx.

#+begin_src plantuml :file ./fig/L00_to_L0.png :exports results
@startuml

' plantuml activity diagram (beta)

component Sensor_1
component Sensor_n

frame CR_Logger {
  database DB_logger [
  <b>Database</b>
  10 minute sampling
  ----
  var0, var1, ..., varn
] 
}

Sensor_1 --> CR_Logger
Sensor_n --> CR_Logger

node GEUS_(Level_0) {
  file Raw [
  <b>raw</b>
  10 min sampling
  ]

  file SlimTableMem [
  <b>SlimTableMem</b>
  Hourly average from
  10 min sampling
  ]

  file TX [
  <b>TX</b>
  V3:
    DOY 100 to 300: hourly average
    DOY 300 to 100: daily average
  V4:
    hourly average all days
  ]
}

' DB -> hand carry -> raw
actor Scientist
DB_logger --> Scientist : Field\ndownload
Scientist --> Raw : Hand\ncarry
Scientist --> SlimTableMem : Hand\ncarry

' DB -> satellite -> Transmitted
cloud Satellite
file Email
queue awsrx
note right
   https://github.com/GEUS-PROMICE/awsrx
end note

DB_logger -[dashed]-> Satellite : Data subsampled and\npossible transmission loss
Satellite -[dashed]-> Email
Email --> awsrx
awsrx --> TX

@enduml
#+end_src

#+RESULTS:
[[file:./fig/L00_to_L0.png]]

** L0 files

+ =raw= : All 10-minute data stored on the CF-card (external module on CR logger)
+ =SlimTableMem= : Hourly averaged 10-min data stored in the internal logger memory
+ =transmitted= : Transmitted via satellite. Only a subset of data is transmitted, and only hourly or daily average depending on station and day of year.

Level 0 files are stored in the =data/L0/<S>/= folder, where =<S>= is the station name. File names should encode the station, end-of-year of download, a version number if there are multiple files for a given year, and the format. Best practices would use the following conventions:  

=data/<L>/<S>/<S>_<Y>[.<n>]_<F>.txt=

Where 

+ =<L>= is the processing level
  + =<L>= must be one of the following: [L0, L1, L1A, L2, L3]
+ =<S>= is a station ID
  + =<S>= must be one of the following strings: [CEN, EGP, KAN_B, KAN_L, KAN_M, KAN_U, KPC_L, KPC_U, MIT, NUK_K, NUK_L, NUK_N, NUK_U, QAS_A, QAS_L, QAS_M, QAS_U, SCO_L, SCO_U, TAS_A, TAS_L, TAS_U, THU_L, THU_U, UPE_L, UPE_U]
+ =<Y>= is a four-digit year with a value greater than =2008=
  + =<Y>= should represent the year at the last timestamp in the file
  + Optionally, =.<n>= is a version number if multiple files from the same year are present
+ =<F>= is the format, one of =raw=, =TX=, or =STM=

Each L0 file that will be processed must have an associated header file, with the same filename plus =.hdr= extension. An [[./template.hdr][example (template) L0 header file]] is:

#+BEGIN_SRC bash :results verbatim :exports results
cat hdr.template
#+END_SRC

#+RESULTS:
#+begin_example
columns     = time, rec, min_y, p, t_1, t_2, rh, wspd, wdir, wd_std, dsr, usr, dlr, ulr, t_rad, z_boom, z_boom_q, z_stake, z_stake_q, z_pt, t_i_1, t_i_2, t_i_3, t_i_4, t_i_5, t_i_6, t_i_7, t_i_8, tilt_x, tilt_y, gps_time, gps_lat, gps_lon, gps_alt, gps_geoid, gps_geounit, gps_q, gps_numsat, gps_hdop, t_log, fan_dc, batt_v_ss, batt_v
station_id  = EGP
format      = raw     # or TX or STM
nodata      = -999
skiprows    = 0       # number of header rows in L0 file

############################################
###
### Optional, depending on file contents
###
############################################

### Required if hygroclip ('t_2')
# hygroclip_t_offset =    # degrees C

### Required if radiation sensor present
### from manufacturer to convert from eng units (1E-5 V) to  physical units (W m-2)
# dsr_eng_coef       =
# usr_eng_coef       =
# dlr_eng_coef       =
# ulr_eng_coef       =

### Required if pressure transducer (z_pt)
# pt_z_coef          =    # Pressure transducer calibibration coefficiont [m water @ 20 C] (TBD: T @ calib?)
# pt_z_p_coef        =    # Air pressure at which the pt was calibrated [hPa]
# pt_z_factor        =    # Unitless. Scale for logger voltage measurement range (2.5 for CR1000, 1 for CR10X)
# pt_antifreeze      =    # Percent antifreeze in the pressure transducer

### Used to rotate boom by a fixed amount
# boom_azimuth       =    # degrees

### Used to adjust clock by HH:MM:SS
# clock_shift        =

############################################
###
### Suggested
###
############################################

# srid           = EPSG:4326
# geometry       = POINT(lon, lat)  # deg.frac W, deg.frac N
#+end_example

*** Additional files

Any files that do not have an associated =.hdr= will be ignored. However, for cleanliness, L0 files that will not be processed should be placed in an =archive= subfolder.

Any changes made to L0 files should be documented in the [[./L0/README.org]]. *Manual changes to these files should only be done when necessary*. An example of a manual change might be:

+ Raw file contains multiple years of data, including replacing sensors that have different calibration units. The file should be split so that each file only contains one version of each sensor (assuming different versions need different metadata).

*** L0 Reader

#+BEGIN_SRC jupyter-python :exports both

<<read_L0>>

ds = read_L0("./data/L0/EGP/EGP_2016_raw.txt.hdr")
print(ds)
#+END_SRC

#+RESULTS:
#+begin_example
<xarray.Dataset>
Dimensions:      (time: 10847)
Coordinates:
  ,* time         (time) datetime64[ns] 2016-05-01T14:30:00 ... 2016-07-19T17:...
Data variables:
    rec          (time) float64 51.0 52.0 53.0 ... 1.09e+04 1.09e+04 1.09e+04
    min_y        (time) float64 1.765e+05 1.766e+05 ... 2.905e+05 2.905e+05
    p            (time) float64 724.4 724.1 724.4 724.4 ... 730.8 731.2 730.7
    t_1          (time) float64 -20.1 -19.79 -19.31 ... -6.904 -6.904 -6.861
    t_2          (time) float64 -19.56 -19.11 -18.92 ... -6.866 -6.86 -6.799
    rh           (time) float64 54.1 51.7 50.23 49.51 ... 80.28 80.93 81.81
    wspd         (time) float64 1.062 0.918 0.636 0.486 ... 2.793 2.951 3.069
    wdir         (time) float64 265.1 259.2 216.8 208.4 ... 217.7 216.6 225.4
    wd_std       (time) float64 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0
    dsr          (time) float64 681.7 732.4 688.3 689.6 ... 724.7 711.4 698.8
    usr          (time) float64 518.6 559.3 531.8 534.4 ... 559.2 549.6 524.1
    dlr          (time) float64 -81.57 -102.0 -101.3 ... -135.8 -135.6 -132.4
    ulr          (time) float64 -23.97 -28.65 -33.92 ... -32.33 -32.52 -28.84
    t_rad        (time) float64 -12.78 -11.42 -9.929 ... -1.114 -1.03 -1.135
    z_boom       (time) float64 2.685 2.683 2.683 2.68 ... 2.583 2.584 2.58
    z_boom_q     (time) float64 190.0 192.0 189.0 187.0 ... 192.0 182.0 168.0
    z_stake      (time) float64 nan nan nan nan nan nan ... nan nan nan nan nan
    z_stake_q    (time) float64 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0
    z_pt         (time) float64 nan nan nan nan nan nan ... nan nan nan nan nan
    t_i_1        (time) float64 nan -109.0 -109.0 ... -8.478 -8.458 -8.448
    t_i_2        (time) float64 nan nan -109.0 -109.0 ... -9.67 -9.67 -9.67
    t_i_3        (time) float64 nan -109.0 -109.0 ... -8.879 -8.859 -8.849
    t_i_4        (time) float64 nan -109.0 -109.0 ... -10.74 -10.73 -10.74
    t_i_5        (time) float64 nan -109.0 -109.0 ... -12.67 -12.67 -12.67
    t_i_6        (time) float64 nan -109.0 nan -109.0 ... -14.9 -14.9 -14.9
    t_i_7        (time) float64 nan -109.0 -109.0 nan ... -17.16 -17.16 -17.16
    t_i_8        (time) float64 nan nan -109.0 nan ... -20.75 -20.76 -20.76
    tilt_x       (time) float64 3.527 3.492 3.516 3.489 ... 0.109 0.095 0.174
    tilt_y       (time) float64 -0.945 -0.938 -0.924 ... -0.828 -0.849 -0.859
    gps_time     (time) object nan nan nan ... 'GT170007.00' 'GT170007.00'
    gps_lat      (time) object nan nan nan ... 'NH7537.47563' 'NH7537.47563'
    gps_lon      (time) object nan nan nan ... 'WH03558.49655' 'WH03558.49655'
    gps_alt      (time) float64 nan nan nan ... 2.663e+03 2.663e+03 2.663e+03
    gps_geoid    (time) float64 nan nan nan nan nan ... 41.6 41.6 41.6 41.6 41.6
    gps_geounit  (time) object nan nan nan nan nan nan ... 'M' 'M' 'M' 'M' 'M'
    gps_q        (time) float64 nan nan nan nan nan nan ... 1.0 1.0 1.0 1.0 1.0
    gps_numsat   (time) float64 nan nan nan nan nan ... 11.0 12.0 12.0 12.0 12.0
    gps_hdop     (time) float64 nan nan nan nan nan ... 0.71 0.73 0.73 0.73 0.73
    t_log        (time) float64 -12.6 -12.08 -11.65 ... -1.801 -1.735 -1.5
    fan_dc       (time) float64 137.5 141.3 142.3 141.8 ... 123.5 123.9 124.1
    batt_v_ss    (time) float64 15.52 15.81 15.79 15.81 ... 14.47 14.47 14.47
    batt_v       (time) float64 15.23 15.56 15.53 15.63 ... 14.4 14.41 14.41
Attributes:
    station_id:          EGP
    format:              raw
    hygroclip_t_offset:  0
    dsr_eng_coef:        12.71
    usr_eng_coef:        12.71
    dlr_eng_coef:        12.71
    ulr_eng_coef:        12.71
#+end_example



* L0 -> L1
:PROPERTIES:
:header-args:jupyter-python+: :session L0_to_L1 :noweb-ref L0_to_L1 :noweb yes
:END:

+ Convert engineering units to physical units

** Wrapper

Run one:
#+BEGIN_SRC jupyter-python :noweb-ref
infile = "./data/L0/EGP/EGP_2016_raw.txt.hdr"
#+END_SRC

#+RESULTS:

#+BEGIN_SRC jupyter-python :noweb-ref
<<L0_to_L1>>
#+END_SRC

#+RESULTS:

Run all:
#+BEGIN_SRC bash
# conda activate PROMICE_dev
for f in $(ls ./data/L0/EGP/*raw*.hdr); do
  echo ${f}
  ./L0_to_L1.py ${f}
done

./L0_to_L1.py data/L0/EGP/EGP_2017_raw_SlimTableMem.txt.hdr
./L0_to_L1.py data/L0/EGP/EGP_2019_raw_transmitted.txt.hdr
#+END_SRC


#+BEGIN_SRC jupyter-python :tangle L0_to_L1.py :noweb-ref :tangle-mode (identity #o744)
#!/usr/bin/env python

def L0_to_L1(infile=None):
    <<L0_to_L1>>


if __name__ == "__main__":
    import sys
    for arg in sys.argv[1:]: L0_to_L1(arg)
    
#+END_SRC

** Imports

#+BEGIN_SRC jupyter-python
import re
import shapely
from shapely import geometry
import nead
import os
import sys
import numpy as np
import pandas as pd
#+END_SRC

#+RESULTS:

** Read in file

+ GitHub link: [[./IDL/AWSdataprocessing_v3.pro#L51]] through [[./IDL/AWSdataprocessing_v3.pro#L123]]
+ Org link: [[./IDL/AWSdataprocessing_v3.pro::51]] through [[./IDL/AWSdataprocessing_v3.pro::123]]
+ [X] Reads in the file
+ [X] Check that required metadata was included in the NEAD header

#+BEGIN_SRC jupyter-python
<<read_L0>>
ds = read_L0(infile)

<<flag_NAN>>
ds = flag_NAN(ds)

<<add_variable_metadata>>
ds = add_variable_metadata(ds)

if 'z_pt' in list(ds.variables):
    if ~ds['z_pt'].isnull().any():
        assert("pt_antifreeze" in ds.attrs.keys())
if 't_2' in list(ds.variables): assert("hygroclip_t_offset" in ds.attrs.keys())
#+END_SRC

#+RESULTS:

** Eng to phys

+ GitHub link: [[./IDL/AWSdataprocessing_v3.pro#L116]] through [[./IDL/AWSdataprocessing_v3.pro#L408]] 
+ Org link: [[./IDL/AWSdataprocessing_v3.pro::116]] through [[./IDL/AWSdataprocessing_v3.pro::408]] 
  + [-] Calculates derived date products (day of century, etc.)
  + [ ] Adjusts start times
    + [ ] ~if slimtablemem eq 'yes' then begin ; change time stamp to start of the hour instead of end~
    + [ ] ~if transmitted eq 'yes' then begin ; change transmission time to start of the hour/day instead of end~
      + [ ] ~if line[col_season-1] eq '!W' then begin ; daily transmissions~
      + [ ] ~if line[col_season-1] eq '!S' then begin ; hourly transmissions~
      + [ ] Makes guesses if season identifier not transmitted
  + [X] Adjusts UTC offset
  + [X] Remove HygroClip temperature offset
  + [X] Reads and adjusts SRin ~SRin = [SRin,float(line[col_SRin-1])*10/C_SRin] ; Calculating radiation (10^-5 V -> W/m2)~
  + [X] SRout
  + [X] LRin: ~LRin = [LRin,float(line[col_LRin-1])*10/C_LRin + 5.67e-8*(float(line[col_Trad-1])+T_0)^4]~
  + [X] LRout
  + [X] Haws: ~Haws = [Haws,float(line[col_Haws-1])*((float(line[col_T-1])+T_0)/T_0)^0.5]~
  + [X] Hstk: ~Hstk = [Hstk,float(line[col_Hstk-1])*((float(line[col_T-1])+T_0)/T_0)^0.5]~
  + [X] Hpt: ~Hpt = [Hpt,float(line[col_Hpt-1])*C_Hpt*F_Hpt*998./rho_af]~
  + [X] Derives Hpt_corrected
  + [X] Decodes GPS - some stations only record minutes not degrees


#+BEGIN_SRC jupyter-python

T_0 = 273.15

# Calculate pressure transducer fluid density
if ~ds['z_pt'].isnull().any():
    if ds.attrs['pt_antifreeze'] == 50:
        rho_af = 1092
    elif ds.attrs['pt_antifreeze'] == 100:
        rho_af = 1145
    else:
        rho_af = np.nan
        print("ERROR: Incorrect NEAD metadata: 'pt_antifreeze =' ", ds.attrs['pt_antifreeze'])
        print("Antifreeze mix only supported at 50 % or 100%")
        # assert(False)
    

for v in ['gps_geounit','min_y']:
    if v in list(ds.variables): ds = ds.drop_vars(v)
        
## adjust times based on file format.
# raw: No adjust (timestamp is at start of period)
# STM: Adjust timestamp from end of period to start of period
# TX: Adjust timestamp start of period (hour/day) also depending on season
# if ds.attrs['PROMICE_format'] == 'STM': ds['time'] = (('time'), ds['time'].to_dataframe().shift(periods=1))
# if ds.attrs['PROMICE_format'] == 'TX': ds['time'] = (('time'), ds['time'].to_dataframe().shift(periods=1))
# if ds.attrs['clock_shift'] != 0:
#     a = ds['time'].attrs
#     ds['time'] = (('time'), ds['time'].to_dataframe().shift(periods=ds.attrs['clock_shift'], freq='H').index)
#     ds['time'].attrs = a

# Remove HygroClip temperature offset
ds['t_2'] = ds['t_2'] - ds.attrs['hygroclip_t_offset']

# convert radiation from engineering to physical units
ds['dsr'] = (ds['dsr'] * 10) / ds.attrs['dsr_eng_coef']
ds['usr'] = (ds['usr'] * 10) / ds.attrs['usr_eng_coef']
ds['dlr'] = ((ds['dlr'] * 10) / ds.attrs['dlr_eng_coef']) + 5.67E-8*(ds['t_rad'] + T_0)**4
ds['ulr'] = ((ds['ulr'] * 10) / ds.attrs['ulr_eng_coef']) + 5.67E-8*(ds['t_rad'] + T_0)**4

# Adjust sonic ranger readings for sensitivity to air temperature
ds['z_boom'] = ds['z_boom'] * ((ds['t_1'] + T_0)/T_0)**0.5 
ds['z_stake'] = ds['z_stake'] * ((ds['t_1'] + T_0)/T_0)**0.5

# Adjust pressure transducer due to fluid properties
if ~ds['z_pt'].isnull().any():
    ds['z_pt'] = ds['z_pt'] * ds.attrs['pt_z_coef'] * ds.attrs['pt_z_factor'] * 998.0 / rho_af

    # Calculate pressure transducer depth
    ds['z_pt_corr'] = ds['z_pt'] * np.nan # new 'z_pt_corr' copied from 'z_pt'
    ds['z_pt_corr'].attrs['long_name'] = ds['z_pt'].long_name + " corrected"
    ds['z_pt_corr'] = ds['z_pt'] * ds.attrs['pt_z_coef'] * ds.attrs['pt_z_factor'] * 998.0 / rho_af \
        + 100 * (ds.attrs['pt_z_p_coef'] - ds['p']) / (rho_af * 9.81)


# Decode GPS
if ds['gps_lat'].dtype.kind == 'O': # not a float. Probably has "NH"
    assert('NH' in ds['gps_lat'].dropna(dim='time').values[0])
    for v in ['gps_lat','gps_lon','gps_time']:
        a = ds[v].attrs # store
        str2nums = [re.findall(r"[-+]?\d*\.\d+|\d+", _) if isinstance(_, str) else [np.nan] for _ in ds[v].values]
        ds[v][:] = pd.DataFrame(str2nums).astype(np.float).T.values[0]
        ds[v] = ds[v].astype(np.float)
        ds[v].attrs = a # restore

if np.any((ds['gps_lat'] <= 90) & (ds['gps_lat'] > 0)):  # Some stations only recorded minutes, not degrees
    xyz = np.array(re.findall("[-+]?[\d]*[.][\d]+", ds.attrs['geometry'])).astype(np.float)
    x=xyz[0]; y=xyz[1]; z=xyz[2] if len(xyz) == 3 else 0
    p = shapely.geometry.Point(x,y,z)
    assert(False) # should p be ints rather than floats here?
    ds['gps_lat'] = ds['gps_lat'] + 100*p.y
if np.any((ds['gps_lon'] <= 90) & (ds['gps_lon'] > 0)):
    ds['gps_lon'] = ds['gps_lon'] + 100*p.x

for v in ['gps_lat','gps_lon']:
    a = ds[v].attrs # store
    ds[v] = np.floor(ds[v] / 100) + (ds[v] / 100 - np.floor(ds[v] / 100)) * 100 / 60
    ds[v].attrs = a # restore

# tilt-o-meter voltage to degrees
# if transmitted ne 'yes' then begin
#    tiltX = smooth(tiltX,7,/EDGE_MIRROR,MISSING=-999) & tiltY = smooth(tiltY,7,/EDGE_MIRROR, MISSING=-999)
# endif

# Should just be
# if ds.attrs['PROMICE_format'] != 'TX': dstxy = dstxy.rolling(time=7, win_type='boxcar', center=True).mean()
# but the /EDGE_MIRROR makes it a bit more complicated...
win_size=7
s = np.int(win_size/2)
tdf = ds['tilt_x'].to_dataframe()
ds['tilt_x'] = (('time'), tdf.iloc[:s][::-1].append(tdf).append(tdf.iloc[-s:][::-1]).rolling(win_size, win_type='boxcar', center=True).mean()[s:-s].values.flatten())
tdf = ds['tilt_y'].to_dataframe()
ds['tilt_y'] = (('time'), tdf.iloc[:s][::-1].append(tdf).append(tdf.iloc[-s:][::-1]).rolling(win_size, win_type='boxcar', center=True).mean()[s:-s].values.flatten())

# # notOKtiltX = where(tiltX lt -100, complement=OKtiltX) & notOKtiltY = where(tiltY lt -100, complement=OKtiltY)
notOKtiltX = (ds['tilt_x'] < -100)
OKtiltX = (ds['tilt_x'] >= -100)
notOKtiltY = (ds['tilt_y'] < -100)
OKtiltY = (ds['tilt_y'] >= -100)

# tiltX = tiltX/10.
ds['tilt_x'] = ds['tilt_x'] / 10
ds['tilt_y'] = ds['tilt_y'] / 10

# tiltnonzero = where(tiltX ne 0 and tiltX gt -40 and tiltX lt 40)
# if n_elements(tiltnonzero) ne 1 then tiltX[tiltnonzero] = tiltX[tiltnonzero]/abs(tiltX[tiltnonzero])*(-0.49*(abs(tiltX[tiltnonzero]))^4 + 3.6*(abs(tiltX[tiltnonzero]))^3 - 10.4*(abs(tiltX[tiltnonzero]))^2 +21.1*(abs(tiltX[tiltnonzero])))

# tiltY = tiltY/10.
# tiltnonzero = where(tiltY ne 0 and tiltY gt -40 and tiltY lt 40)
# if n_elements(tiltnonzero) ne 1 then tiltY[tiltnonzero] = tiltY[tiltnonzero]/abs(tiltY[tiltnonzero])*(-0.49*(abs(tiltY[tiltnonzero]))^4 + 3.6*(abs(tiltY[tiltnonzero]))^3 - 10.4*(abs(tiltY[tiltnonzero]))^2 +21.1*(abs(tiltY[tiltnonzero])))

dstx = ds['tilt_x']
nz = (dstx != 0) & (np.abs(dstx) < 40)
dstx = dstx.where(~nz, other = dstx / np.abs(dstx) * (-0.49 * (np.abs(dstx))**4 + 3.6 * (np.abs(dstx))**3 - 10.4 * (np.abs(dstx))**2 + 21.1 * (np.abs(dstx))))
ds['tilt_x'] = dstx

dsty = ds['tilt_y']
nz = (dsty != 0) & (np.abs(dsty) < 40)
dsty = dsty.where(~nz, other = dsty / np.abs(dsty) * (-0.49 * (np.abs(dsty))**4 + 3.6 * (np.abs(dsty))**3 - 10.4 * (np.abs(dsty))**2 + 21.1 * (np.abs(dsty))))
ds['tilt_y'] = dsty

# if n_elements(OKtiltX) gt 1 then tiltX[notOKtiltX] = interpol(tiltX[OKtiltX],OKtiltX,notOKtiltX) ; Interpolate over gaps for radiation correction; set to -999 again below.
# if n_elements(OKtiltY) gt 1 then tiltY[notOKtiltY] = interpol(tiltY[OKtiltY],OKtiltY,notOKtiltY) ; Interpolate over gaps for radiation correction; set to -999 again below.
# ds['tilt_x'] = ds['tilt_x'].interpolate_na(dim='time')
# ds['tilt_y'] = ds['tilt_y'].interpolate_na(dim='time')
ds['tilt_x'] = ds['tilt_x'].ffill(dim='time')
ds['tilt_y'] = ds['tilt_y'].ffill(dim='time')

ds['tilt_x'] = ds['tilt_x'].where(~notOKtiltX)
ds['tilt_y'] = ds['tilt_y'].where(~notOKtiltY)

deg2rad = np.pi / 180
ds['wdir'] = ds['wdir'].where(ds['wspd'] != 0)
ds['wspd_x'] = ds['wspd'] * np.sin(ds['wdir'] * deg2rad)
ds['wspd_y'] = ds['wspd'] * np.cos(ds['wdir'] * deg2rad)
#+END_SRC

** Export file as L1

+ Check with ~cfchecks ./data/L1/EGP/EGP-2016-raw.nc~

#+BEGIN_SRC jupyter-python
outpath = os.path.split(infile)[0].split("/")
outpath[-2] = 'L1'
outpath = '/'.join(outpath)
outfile = os.path.splitext(os.path.splitext(os.path.basename(infile))[0])[0]

outpathfile = outpath + '/' + outfile + ".nc"
if os.path.exists(outpathfile): os.remove(outpathfile)
ds.to_netcdf(outpathfile, mode='w', format='NETCDF4', compute=True)
#+END_SRC

#+RESULTS:



* Level 1
:PROPERTIES:
:header-args:bash+: :exports both
:END:

File list:

#+BEGIN_SRC bash :exports both :results verbatim
find ./data/L1
#+END_SRC

#+RESULTS:
#+begin_example
./data/L1
./data/L1/EGP
./data/L1/EGP/EGP-2017-STM.nc
./data/L1/EGP/EGP-2016-raw.nc
./data/L1/EGP/EGP-2019-TX.nc
./data/L1/EGP/EGP-2017-raw.nc
./data/L1/EGP/EGP-2019.1-raw.nc
./data/L1/EGP/EGP-2018.2-raw.nc
./data/L1/EGP/EGP-2018.1-raw.nc
./data/L1/EGP/EGP-2019.2-raw.nc
#+end_example

NetCDF format

#+BEGIN_SRC bash :results verbatim :exports both
ncdump -ch ./data/L1/EGP/EGP-2016-raw.nc | head -n35
#+END_SRC

#+RESULTS:
#+begin_example
netcdf EGP-2016-raw {
dimensions:
	time = 10847 ;
variables:
	double rec(time) ;
		rec:_FillValue = NaN ;
		rec:standard_name = "record" ;
		rec:long_name = "Record" ;
		rec:units = "" ;
		rec:scale_factor = 1. ;
		rec:add_offset = 0. ;
	double p(time) ;
		p:_FillValue = NaN ;
		p:standard_name = "air_pressure" ;
		p:long_name = "Air pressure" ;
		p:units = "hPa" ;
		p:scale_factor = 0.01 ;
		p:add_offset = 0. ;
	double t_1(time) ;
		t_1:_FillValue = NaN ;
		t_1:standard_name = "air_temperature" ;
		t_1:long_name = "Air temperature 1" ;
		t_1:units = "C" ;
		t_1:scale_factor = 1. ;
		t_1:add_offset = 273.15 ;
	double t_2(time) ;
		t_2:_FillValue = NaN ;
		t_2:standard_name = "air_temperature" ;
		t_2:long_name = "Air temperature 2" ;
		t_2:units = "C" ;
		t_2:scale_factor = 1. ;
		t_2:add_offset = 273.15 ;
	double rh(time) ;
		rh:_FillValue = NaN ;
		rh:standard_name = "relative_humidity" ;
#+end_example


* L1 -> L1A
:PROPERTIES:
:header-args:jupyter-python+: :session L1_to_L1A :noweb-ref L1_to_L1A :noweb yes
:END:

+ Merge all files by type (keep =raw=, =STM=, and =TX=)
+ Flag out-of-limit (OOL) values from [[./flags.csv]]

** Wrapper

Run one:
#+BEGIN_SRC jupyter-python :noweb-ref
infile = "./data/L1/EGP/EGP_2016_raw.nc"
<<L1_to_L1A>>
#+END_SRC

#+RESULTS:

Run all:

#+BEGIN_SRC bash
# conda activate PROMICE_dev

# ./L1_to_L1A.py ./data/L1/EGP/EGP-2016-raw.nc
./L1_to_L1A.py data/L1/EGP/*raw.nc
./L1_to_L1A.py data/L1/EGP/*STM.nc
./L1_to_L1A.py data/L1/EGP/*TX.nc
#+END_SRC

#+RESULTS:

#+header:  :tangle L1_to_L1A.py :noweb-ref :tangle-mode (identity #o744)
#+BEGIN_SRC jupyter-python
#!/usr/bin/env python

<<L1_to_L1A_imports>>

def L1_to_L1A(infile=None):
    <<L1_to_L1A>>

if __name__ == "__main__":
    import sys
    # for arg in sys.argv[1:]: L1_to_L1A(arg)
    L1_to_L1A(sys.argv[1:])
#+END_SRC

** Imports

#+header: :noweb-ref L1_to_L1A_imports
#+BEGIN_SRC jupyter-python
import pandas as pd
import xarray as xr
import os
#+END_SRC

#+RESULTS:

** Merge files
#+BEGIN_SRC jupyter-python :exports both
# from IPython import embed; embed()
ds = xr.open_mfdataset(infile, combine='by_coords', mask_and_scale=False).load()
#+END_SRC

#+RESULTS:

** Flag data

Out of limit (OOL) data comes from the [[./variables.csv]] file.

+ Set each variable to NaN where it is OOL
+ Also set paired or associated variables to NaN

#+BEGIN_SRC jupyter-python
df = pd.read_csv("./variables.csv", index_col=0, comment="#", usecols=('fields','lo','hi','OOL'))
df = df.dropna(how='all')

for var in df.index:
    if var not in list(ds.variables): continue
    ds[var] = ds[var].where(ds[var] >= df.loc[var, 'lo'])
    ds[var] = ds[var].where(ds[var] <= df.loc[var, 'hi'])
    other_vars = df.loc[var]['OOL'] # either NaN or "foo" or "foo bar baz ..."
    if isinstance(other_vars, str):
        for o in other_vars.split():
            if o not in list(ds.variables): continue
            ds[o] = ds[o].where(ds[var] >= df.loc[var, 'lo'])
            ds[o] = ds[o].where(ds[var] <= df.loc[var, 'hi'])
#+END_SRC

#+RESULTS:


** Export file as L1A

+ Check with ~cfchecks ./data/L1A/EGP/EGP-2016-raw.nc~

#+BEGIN_SRC jupyter-python
if isinstance(infile, list): infile = infile[0]
# infile_parts = os.path.splitext(os.path.basename(infile))[0].split('_')
# outfile = infile_parts[0] + '-' + infile_parts[-1] + '.nc' # drop year
outfile = ds.attrs['station_id'] + '-' + ds.attrs['format'] + '.nc'

outpath = os.path.split(infile)[0].split("/")
outpath[-2] = 'L1A'
# outfile = os.path.splitext(os.path.basename(infile))[0] + '.nc'
outpath = '/'.join(outpath)
outpathfile = outpath + '/' + outfile
if os.path.exists(outpathfile): os.remove(outpathfile)
ds.to_netcdf(outpathfile, mode='w', format='NETCDF4', compute=True)
#+END_SRC

#+RESULTS:




* Level 1A
* L1A -> L2
:PROPERTIES:
:header-args:jupyter-python+: :session L1A_to_L2 :noweb-ref L1A_to_L2 :noweb yes
:END:

+ Calibration using secondary sources

** Wrapper

Run one:
#+BEGIN_SRC jupyter-python :noweb-ref
infile = "./data/L1A/EGP/EGP-raw.nc"
#+END_SRC

#+RESULTS:

#+BEGIN_SRC jupyter-python :noweb-ref
<<L1A_to_L2>>
#+END_SRC

#+RESULTS:

Run all:

#+BEGIN_SRC bash
# conda activate PROMICE_dev

./L1A_to_L2.py data/L1A/EGP/EGP-raw.nc
./L1A_to_L2.py data/L1A/EGP/EGP-STM.nc
./L1A_to_L2.py data/L1A/EGP/EGP-TX.nc
#+END_SRC


#+BEGIN_SRC jupyter-python :tangle L1A_to_L2.py :noweb-ref :tangle-mode (identity #o744)
#!/usr/bin/env python

<<L1A_to_L2_imports>>

def L1A_to_L2(infile=None):
    <<L1A_to_L2>>


if __name__ == "__main__":
    import sys
    for arg in sys.argv[1:]: L1A_to_L2(arg)
#+END_SRC

** Imports

#+header: :noweb-ref L1A_to_L2_imports
#+BEGIN_SRC jupyter-python
import xarray as xr
import pandas as pd
import os

#+END_SRC

#+RESULTS:

** Init

#+BEGIN_SRC jupyter-python
<<constants>>
#+END_SRC

#+RESULTS:


** Load
#+BEGIN_SRC jupyter-python :exports both
# infile = "./data/L1A/EGP/EGP-raw.nc"
ds = xr.open_dataset(infile, mask_and_scale=False).load()
# print(ds)
#+END_SRC

#+RESULTS:

** Calibrate using secondary sensors

*** Correct relative humidity

+ Correct relative humidity readings for T below 0 to give value with respect to ice
  + GitHub: [[./IDL/AWSdataprocessing_v3.pro#L411]]
  + Org Mode: [[./IDL/AWSdataprocessing_v3.pro::411]]

+ This section implements the Goff-Gratch equation
 
#+BEGIN_SRC jupyter-python
T_0 = 273.15

T_100 = T_0+100            # steam point temperature in K
ews = 1013.246             # saturation pressure at steam point temperature, normal atmosphere
ei0 = 6.1071

T = ds['t_1'].copy(deep=True)

# in hPa (Goff & Gratch)
e_s_wtr = 10**(-7.90298 * (T_100 / (T + T_0) - 1)
               + 5.02808 * np.log10(T_100 / (T + T_0)) 
               - 1.3816E-7 * (10**(11.344 * (1 - (T + T_0) / T_100)) - 1)
               + 8.1328E-3 * (10**(-3.49149 * (T_100/(T + T_0) - 1)) -1)
               + np.log10(ews))

# in hPa (Goff & Gratch)
e_s_ice = 10**(-9.09718 * (T_0 / (T + T_0) - 1)
               - 3.56654 * np.log10(T_0 / (T + T_0))
               + 0.876793 * (1 - (T + T_0) / T_0)
               + np.log10(ei0))

# ds['rh_cor'] = (e_s_wtr / e_s_ice) * ds['rh'].where((ds['t_1'] < 0) & (ds['t_1'] > -100))
freezing = (ds['t_1'] < 0) & (ds['t_1'] > -100).values # why > -100?
# set to Geoff & Gratch values when freezing, otherwise just rh.
ds['rh_cor'] = ds['rh'].where(~freezing, other = ds['rh']*(e_s_wtr / e_s_ice))


# https://github.com/GEUS-PROMICE/PROMICE-AWS-processing/issues/23
# Just adding special treatment here in service of replication. rh_cor is clipped not NaN'd
# https://github.com/GEUS-PROMICE/PROMICE-AWS-processing/issues/20
df = pd.read_csv("./variables.csv", index_col=0, comment="#", usecols=('fields','lo','hi','OOL'))
var = 'rh_cor'
if var in list(ds.variables):
    if var != 'rh_cor':
        ds[var] = ds[var].where(ds[var] >= df.loc[var, 'lo'])
        ds[var] = ds[var].where(ds[var] <= df.loc[var, 'hi'])
    else:
         ds[var] = ds[var].where(ds[var] >= df.loc[var, 'lo'], other = 100)
         ds[var] = ds[var].where(ds[var] <= df.loc[var, 'hi'], other = 100)
    other_vars = df.loc[var]['OOL'] # either NaN or "foo" or "foo bar baz ..."
    if isinstance(other_vars, str): 
        for o in other_vars.split():
            ds[o] = ds[o].where(ds[var] >= df.loc[var, 'lo'])
            ds[o] = ds[o].where(ds[var] <= df.loc[var, 'hi'])
#+END_SRC

#+RESULTS:



*** Cloud cover

+ cloud cover (for iswr correction) and surface temperature
  + GitHub: [[./IDL/AWSdataprocessing_v3.pro#L441]]
  + Org Mode: [[./IDL/AWSdataprocessing_v3.pro::441]]

This is a derived product and belongs is L2->L3 processing appearing in L3, but DifFrac is used in the iswr correction.

#+BEGIN_SRC jupyter-python

eps_overcast = 1.
eps_clear = 9.36508e-6
LR_overcast = eps_overcast * 5.67e-8 *(T + T_0)**4   # assumption
LR_clear = eps_clear * 5.67e-8 * (T + T_0)**6        # Swinbank (1963)

# Special case for selected stations (will need this for all stations eventually)
if ds.attrs['station_id'] == 'KAN_M':
   # print,'KAN_M cloud cover calculations'
   LR_overcast = 315 + 4*T
   LR_clear = 30 + 4.6e-13 * (T + T_0)**6
elif ds.attrs['station_id'] == 'KAN_U':
   # print,'KAN_U cloud cover calculations'
   LR_overcast = 305 + 4*T
   LR_clear = 220 + 3.5*T

cc = (ds['dlr'] - LR_clear) / (LR_overcast - LR_clear)
cc[cc > 1] = 1
cc[cc < 0] = 0
DifFrac = 0.2 + 0.8 * cc

ds['cc'] = (('time'), cc)

emissivity = 0.97
ds['t_surf'] = ((ds['ulr'] - (1 - emissivity) * ds['dlr']) / emissivity / 5.67e-8)**0.25 - T_0
ds['t_surf'] = ds['t_surf'].where(ds['t_surf'] <= 0, other = 0) # if > 0, set to 0
#+END_SRC

#+RESULTS:



*** Correct shortwave radiation

+ Take into account station tilt, sun angle, etc.
  + GitHub: [[./IDL/AWSdataprocessing_v3.pro#L475]]
  + Org Mode: [[./IDL/AWSdataprocessing_v3.pro::475]]

Calculate tilt angle and direction of sensor and rotating to a north-south aligned coordinate system
#+BEGIN_SRC jupyter-python
tx = ds['tilt_x'] * deg2rad
ty = ds['tilt_y'] * deg2rad

## cartesian coords
X = np.sin(tx) * np.cos(tx) * np.sin(ty)**2 + np.sin(tx) * np.cos(ty)**2
Y = np.sin(ty) * np.cos(ty) * np.sin(tx)**2 + np.sin(ty) * np.cos(tx)**2
Z = np.cos(tx) * np.cos(ty) + np.sin(tx)**2 * np.sin(ty)**2

# spherical coords
phi_sensor_rad = -np.pi /2 - np.arctan(Y/X)
phi_sensor_rad[X > 0] += np.pi
phi_sensor_rad[(X == 0) & (Y < 0)] = np.pi
phi_sensor_rad[(X == 0) & (Y == 0)] = 0
phi_sensor_rad[phi_sensor_rad < 0] += 2*np.pi

phi_sensor_deg = phi_sensor_rad * rad2deg

# spherical coordinate (or actually total tilt of the sensor, i.e. 0 when horizontal)
theta_sensor_rad = np.arccos(Z / (X**2 + Y**2 + Z**2)**0.5) 
theta_sensor_deg = theta_sensor_rad * rad2deg

## Offset correction (determine offset yourself using data for solar
## zenith angles larger than 110 deg) I actually don't do this as it
## shouldn't improve accuracy for well calibrated instruments
# ;ds['dsr'] = ds['dsr'] - ds['dwr_offset']
# ;SRout = SRout - SRout_offset

# Calculating zenith and hour angle of the sun
doy = ds['time'].to_dataframe().index.dayofyear.values
hour = ds['time'].to_dataframe().index.hour.values
minute = ds['time'].to_dataframe().index.minute.values
# lat = ds['gps_lat']
# lon = ds['gps_lon']
lat = ds.attrs['latitude']
lon = ds.attrs['longitude']

d0_rad = 2 * np.pi * (doy + (hour + minute / 60) / 24 -1) / 365

Declination_rad = np.arcsin(0.006918 - 0.399912 * np.cos(d0_rad) + 0.070257 * np.sin(d0_rad) - 0.006758 * np.cos(2 * d0_rad) + 0.000907 * np.sin(2 * d0_rad) - 0.002697 * np.cos(3 * d0_rad) + 0.00148 * np.sin(3 * d0_rad))

HourAngle_rad = 2 * np.pi * (((hour + minute / 60) / 24 - 0.5) - lon/360)
# ; - 15.*timezone/360.) ; NB: Make sure time is in UTC and longitude is positive when west! Hour angle should be 0 at noon.

# This is 180 deg at noon (NH), as opposed to HourAngle.
DirectionSun_deg = HourAngle_rad * 180/np.pi - 180

DirectionSun_deg[DirectionSun_deg < 0] += 360
DirectionSun_deg[DirectionSun_deg < 0] += 360

ZenithAngle_rad = np.arccos(np.cos(lat * np.pi/180) * np.cos(Declination_rad) * np.cos(HourAngle_rad) + np.sin(lat * np.pi/180) * np.sin(Declination_rad))

ZenithAngle_deg = ZenithAngle_rad * rad2deg

sundown = ZenithAngle_deg >= 90
isr_toa = 1372 * np.cos(ZenithAngle_rad) # Incoming shortware radiation at the top of the atmosphere
isr_toa[sundown] = 0

# Calculating the correction factor for direct beam radiation
# http://solardat.uoregon.edu/SolarRadiationBasics.html
CorFac = np.sin(Declination_rad) * np.sin(lat * deg2rad) * np.cos(theta_sensor_rad) - np.sin(Declination_rad) * np.cos(lat * deg2rad) * np.sin(theta_sensor_rad) * np.cos(phi_sensor_rad + np.pi) + np.cos(Declination_rad) * np.cos(lat * deg2rad) * np.cos(theta_sensor_rad) * np.cos(HourAngle_rad) + np.cos(Declination_rad) * np.sin(lat * deg2rad) * np.sin(theta_sensor_rad) * np.cos(phi_sensor_rad + np.pi) * np.cos(HourAngle_rad) + np.cos(Declination_rad) * np.sin(theta_sensor_rad) * np.sin(phi_sensor_rad + np.pi) * np.sin(HourAngle_rad)

CorFac = np.cos(ZenithAngle_rad) / CorFac
# sun out of field of view upper sensor
CorFac[(CorFac < 0) | (ZenithAngle_deg > 90)] = 1

# Calculating ds['dsr'] over a horizontal surface corrected for station/sensor tilt
CorFac_all = CorFac / (1 - DifFrac + CorFac * DifFrac)
ds['dsr_cor'] = ds['dsr'].copy(deep=True) * CorFac_all


# Calculating albedo based on albedo values when sun is in sight of the upper sensor
AngleDif_deg = 180 / np.pi * np.arccos(np.sin(ZenithAngle_rad) * np.cos(HourAngle_rad + np.pi) * np.sin(theta_sensor_rad) * np.cos(phi_sensor_rad) + np.sin(ZenithAngle_rad) * np.sin(HourAngle_rad + np.pi) * np.sin(theta_sensor_rad) * np.sin(phi_sensor_rad) + np.cos(ZenithAngle_rad) * np.cos(theta_sensor_rad)) # angle between sun and sensor

# ;AngleDif_deg = 180./!pi*acos(cos(!pi/2.-ZenithAngle_rad)*cos(!pi/2.-theta_sensor_rad)*cos(HourAngle_rad-phi_sensor_rad)+sin(!pi/2.-ZenithAngle_rad)*sin(!pi/2.-theta_sensor_rad)) ; angle between sun and sensor
# from IPython import embed; embed()

ds['albedo'] = ds['usr'] / ds['dsr_cor']
albedo_nan = np.isnan(ds['albedo']) # store existing NaN
OKalbedos = (AngleDif_deg < 70) & (ZenithAngle_deg < 70) & (ds['albedo'] < 1) & (ds['albedo'] > 0)
ds['albedo'][~OKalbedos] = np.nan
# ds['albedo'] = ds['albedo'].interpolate_na(dim='time') # Interpolate all NaN (old and new NotOK)
ds['albedo'] = ds['albedo'].ffill(dim='time') # Interpolate all NaN (old and new NotOK)
ds['albedo'][albedo_nan] = np.nan # restore old NaN

# ;OKalbedos = where(angleDif_deg lt 82.5 and ZenithAngle_deg lt 70 and albedo lt 1 and albedo gt 0, complement=notOKalbedos)
# ;The running mean calculation doesn't work for non-continuous data sets or variable temporal resolution (e.g. with multiple files)
# ;albedo_rm = 0*albedo
# ;albedo_rm[OKalbedos] = smooth(albedo[OKalbedos],obsrate+1,/edge_truncate) ; boxcar average of reliable albedo values
# ;albedo[notOKalbedos] = interpol(albedo_rm[OKalbedos],OKalbedos,notOKalbedos) ; interpolate over gaps
# ;albedo_rm[notOKalbedos] = albedo[notOKalbedos]
# ;So instead:

# albedo[notOKalbedos] = interpol(albedo[OKalbedos],OKalbedos,notOKalbedos) ; interpolate over gaps - gives problems for discontinuous data sets, but is not the end of the world

# Correcting SR using DWR when sun is in field of view of lower sensor assuming sensor measures only diffuse radiation
sunonlowerdome =(AngleDif_deg >= 90) & (ZenithAngle_deg <= 90)
# ds['dsr_cor'][sunonlowerdome] = ds['dsr'][sunonlowerdome] / DifFrac[sunonlowerdome]
ds['dsr_cor'] = ds['dsr_cor'].where(~sunonlowerdome, other=ds['dsr'] / DifFrac)
ds['usr_cor'] = ds['usr'].copy(deep=True)
# ds['usr_cor'][sunonlowerdome] = albedo * ds['dsr'][sunonlowerdome] / DifFrac[sunonlowerdome]
ds['usr_cor'] = ds['usr_cor'].where(~sunonlowerdome, other=ds['albedo'] * ds['dsr'] / DifFrac)


# Setting DWR and USWR to zero for solar zenith angles larger than 95 deg or either DWR or USWR are (less than) zero
bad = (ZenithAngle_deg > 95) | (ds['dsr_cor'] <= 0) | (ds['usr_cor'] <= 0)
ds['dsr_cor'][bad] = np.nan
ds['usr_cor'][bad] = np.nan

# Correcting DWR using more reliable USWR when sun not in sight of upper sensor
ds['dsr_cor'] = ds['usr_cor'].copy(deep=True) / ds['albedo']
# albedo[~OKalbedos] = np.nan
ds['albedo'] = ds['albedo'].where(OKalbedos)
# albedo[OKalbedos[n_elements(OKalbedos)-1]:*] = -999 ; Removing albedos that were extrapolated (as opposed to interpolated) at the end of the time series - see above
# ds['dsr']_cor[OKalbedos[n_elements(OKalbedos)-1]:*] = -999 ; Removing the corresponding ds['dsr']_cor as well
# ds['uswr_cor'][OKalbedos[n_elements(OKalbedos)-1]:*] = -999 ; Removing the corresponding ds['uswr_cor'] as well

# ; Removing spikes by interpolation based on a simple top-of-the-atmosphere limitation
#      TOA_crit_nopass = where(ds['dsr']_cor gt 0.9*dwr_toa+10)
#      TOA_crit_pass = where(ds['dsr']_cor le 0.9*dwr_toa+10)
#      if total(TOA_crit_nopass) ne -1 then begin
#         ds['dsr']_cor[TOA_crit_nopass] = interpol(ds['dsr']_cor[TOA_crit_pass],TOA_crit_pass,TOA_crit_nopass)
#         ds['uswr_cor'][TOA_crit_nopass] = interpol(ds['uswr_cor'][TOA_crit_pass],TOA_crit_pass,TOA_crit_nopass)
#      endif
TOA_crit_nopass = (ds['dsr_cor'] > (0.9 * isr_toa + 10))

# from IPython import embed; embed()
ds['dsr_cor'][TOA_crit_nopass] = np.nan
ds['usr_cor'][TOA_crit_nopass] = np.nan
# ds['dsr_cor'] = ds['dsr_cor'].interpolate_na(dim='time')
# ds['usr_cor'] = ds['usr_cor'].interpolate_na(dim='time')
ds['dsr_cor'] = ds['dsr_cor'].ffill(dim='time')
ds['usr_cor'] = ds['usr_cor'].ffill(dim='time')
# ds['dsr_cor'] = ds['dsr_cor'].interpolate_na(dim='time', method='linear', limit=12, max_gap='2H')
# ds['usr_cor'] = ds['usr_cor'].interpolate_na(dim='time', method='linear', limit=12, max_gap='2H')

# from IPython import embed; embed()
# print,'- Sun in view of upper sensor / workable albedos:',n_elements(OKalbedos),100*n_elements(OKalbedos)/n_elements(ds['dsr']),'%'
valid = (~(ds['dsr_cor'].isnull())).sum()
print('- Sun in view of upper sensor / workable albedos:',
      OKalbedos.sum().values,
      (100*OKalbedos.sum()/valid).round().values,
      "%")

# print,'- Sun below horizon:',n_elements(sundown),100*n_elements(sundown)/n_elements(ds['dsr']),'%'
print('- Sun below horizon:',
      sundown.sum(),
      (100*sundown.sum()/valid).round().values,
      "%")

# print,'- Sun in view of lower sensor:',n_elements(sunonlowerdome),100*n_elements(sunonlowerdome)/n_elements(ds['dsr']),'%'
print('- Sun in view of lower sensor:',
      sunonlowerdome.sum().values,
      (100*sunonlowerdome.sum()/valid).round().values,
      "%")

# print,'- Spikes removed using TOA criteria:',n_elements(TOA_crit_nopass),100*n_elements(TOA_crit_nopass)/n_elements(ds['dsr']),'%'
print('- Spikes removed using TOA criteria:',
      TOA_crit_nopass.sum().values,
      (100*TOA_crit_nopass.sum()/valid).round().values,
      "%")

# print,'- Mean net SR change by corrections:',total(ds['dsr']_cor-ds['uswr_cor']-ds['dsr']+SRout)/n_elements(ds['dsr']),' W/m2'
print('- Mean net SR change by corrections:',
      (ds['dsr_cor']-ds['usr_cor']-ds['dsr']+ds['usr']).sum().values/valid.values,
      "W/m2")

#+END_SRC

*** Wind direction

+ GitHub: [[./IDL/AWSdataprocessing_v3.pro#L423]]
+ Org Mode: [[./IDL/AWSdataprocessing_v3.pro::423]]
    
#+BEGIN_SRC jupyter-python

# ds['wspd_x'] = ds['wspd'] * np.sin(ds['wdir'] * deg2rad)
# ds['wspd_y'] = ds['wspd'] * np.cos(ds['wdir'] * deg2rad)

# adjust properties
#+END_SRC

#+RESULTS:


** Export file as L2

+ Check with ~cfchecks ./data/L2/EGP/EGP-raw.nc~

#+BEGIN_SRC jupyter-python
outpath = os.path.split(infile)[0].split("/")
outpath[-2] = 'L2'
outpath = '/'.join(outpath)
outfile = os.path.basename(infile)
outpathfile = outpath + '/' + outfile
if os.path.exists(outpathfile): os.remove(outpathfile)
ds.to_netcdf(outpathfile, mode='w', format='NETCDF4', compute=True)
#+END_SRC

#+RESULTS:





* L2 -> L3
:PROPERTIES:
:header-args:jupyter-python+: :session L2_to_L3 :noweb-ref L2_to_L3 :noweb yes
:END:

+ Derived values
  + [ ] Cloud cover
  + [ ] Wind direction components
  + [ ] Turbulent heat flux

** Wrapper

Run one:
#+BEGIN_SRC jupyter-python :noweb-ref
infile = "./data/L2/EGP/EGP-raw.nc"
<<L2_to_L3>>
#+END_SRC

#+RESULTS:
:RESULTS:
# [goto error]
:   File "<tokenize>", line 78
:     endfor
:     ^
: IndentationError: unindent does not match any outer indentation level
:END:

Run all:

#+BEGIN_SRC bash
# conda activate PROMICE_dev

./L2_to_L3.py data/L2/EGP/*raw.nc
./L2_to_L3.py data/L2/EGP/*STM.nc
./L2_to_L3.py data/L2/EGP/*TX.nc
#+END_SRC


#+BEGIN_SRC jupyter-python :tangle L2_to_L3.py :noweb-ref :tangle-mode (identity #o744)
#!/usr/bin/env python

def L2_to_L3(infile=None):
    <<L2_to_L3>>


if __name__ == "__main__":
    import sys
    for arg in sys.argv[1:]: L2_to_L3(arg)
#+END_SRC

** Imports

#+BEGIN_SRC jupyter-python
import xarray as xr
import os

<<constants>>
#+END_SRC

#+RESULTS:

** Load
#+BEGIN_SRC jupyter-python :exports both
ds = xr.open_dataset(infile, mask_and_scale=False).load()
# print(ds)
#+END_SRC

#+RESULTS:

** Downsample to hourly and daily

Downsampling should be 1 line
#+BEGIN_SRC jupyter-python :noweb-ref nil
ds_h = ds.resample({'time':"1H"}).mean() # this takes ~2-3 minutes
ds_d = ds.resample({'time':"1D"}).mean()
#+END_SRC

But due to xarray implementation, this takes several minutes, while it takes << 1 second in Pandas.
See https://github.com/pydata/xarray/issues/4498

Therefore, we do downsampling in Pandas (for now) even though the code is more complex.

#+BEGIN_SRC jupyter-python
df_h = ds.to_dataframe().resample("1H").mean()  # what we want (quickly), but in Pandas form
# now, rebuild xarray dataset (https://www.theurbanist.com.au/2020/03/how-to-create-an-xarray-dataset-from-scratch/)
vals = [xr.DataArray(data=df_h[c], dims=['time'], coords={'time':df_h.index}, attrs=ds[c].attrs) for c in df_h.columns]
ds_h = xr.Dataset(dict(zip(df_h.columns,vals)), attrs=ds.attrs)


df_d = ds.to_dataframe().resample("1D").mean()  # what we want (quickly), but in Pandas form
# now, rebuild xarray dataset (https://www.theurbanist.com.au/2020/03/how-to-create-an-xarray-dataset-from-scratch/)
vals = [xr.DataArray(data=df_d[c], dims=['time'], coords={'time':df_d.index}, attrs=ds[c].attrs) for c in df_d.columns]
ds_d = xr.Dataset(dict(zip(df_d.columns,vals)), attrs=ds.attrs)
#+END_SRC

*** Circular averaging

Calculating average wind direction takes a bit more work...

#+BEGIN_SRC jupyter-python
ds_h['wdir'] = np.arctan2(ds_h['wspd_x'], ds_h['wspd_y']) * rad2deg
ds_d['wdir'] = np.arctan2(ds_d['wspd_x'], ds_d['wspd_y']) * rad2deg
ds_h['wdir'] = (ds_h['wdir'] + 360) % 360
ds_d['wdir'] = (ds_d['wdir'] + 360) % 360
#+END_SRC

** Derived properties

*** Turbulent heat flux

+ GitHub: [[./IDL/AWSdataprocessing_v3.pro#L866]]
+ Org Mode: [[./IDL/AWSdataprocessing_v3.pro::866]]


+ Requires hourly averages

Constants

#+BEGIN_SRC jupyter-python
z_0    =    0.001    # aerodynamic surface roughness length for momention (assumed constant for all ice/snow surfaces)
eps    =    0.622
es_0   =    6.1071   # saturation vapour pressure at the melting point (hPa)
es_100 = 1013.246    # saturation vapour pressure at steam point temperature (hPa)
g      =    9.82     # gravitational acceleration (m/s2)
gamma  =   16.       # flux profile correction (Paulson & Dyer)
kappa  =    0.4      # Von Karman constant (0.35-0.42)
L_sub  =    2.83e6   # latent heat of sublimation (J/kg)
R_d    =  287.05     # gas constant of dry air
aa     =    0.7      # flux profile correction constants (Holtslag & De Bruin '88)
bb     =    0.75
cc     =    5.
dd     =    0.35
c_pd   = 1005.       # specific heat of dry air (J/kg/K)
WS_lim =    1.
L_dif_max = 0.01


T_0 = 273.15
T_100 = T_0+100            # steam point temperature in K

#+END_SRC

#+RESULTS:

#+BEGIN_SRC jupyter-python
# ds_h = ds.resample({'time':"1H"}).mean() # this takes ~2-3 minuteso

T_h = ds_h['t_1'].copy()
p_h = ds_h['p'].copy()
WS_h = ds_h['wspd'].copy()
Tsurf_h = ds_h['t_surf'].copy()
RH_cor_h = ds_h['rh_cor'].copy()

z_WS = ds_h['z_boom'].copy() + 0.4  # height of W
z_T = ds_h['z_boom'].copy() - 0.1   # height of thermometer

rho_atm = 100 * p_h / R_d / (T_h + T_0)   # atmospheric density

# dynamic viscosity of air (Pa s) (Sutherlands' equation using C = 120 K)
mu = 18.27e-6 * (291.15 + 120) / ((T_h + T_0) + 120) * ((T_h + T_0) / 291.15)**1.5

nu = mu / rho_atm                                                   # kinematic viscosity of air (m^2/s)
u_star = kappa * WS_h / np.log(z_WS / z_0)
Re = u_star * z_0 / nu
z_0h = z_0 * np.exp(1.5 - 0.2 * np.log(Re) - 0.11 * np.log(Re)**2) # rough surfaces: Smeets & Van den Broeke 2008
z_0h[WS_h <= 0] = 1e-10
es_ice_surf = 10**(-9.09718 * (T_0 / (Tsurf_h + T_0) -1) - 3.56654 * np.log10(T_0 / (Tsurf_h + T_0)) + 0.876793 * (1 - (Tsurf_h + T_0) / T_0) + np.log10(es_0))
q_surf = eps * es_ice_surf / (p_h - (1 - eps) * es_ice_surf)
# saturation vapour pressure above 0 C (hPa)
es_wtr = 10**(-7.90298 * (T_100 / (T_h + T_0) - 1) + 5.02808 * np.log10(T_100 / (T_h + T_0))
              - 1.3816E-7 * (10**(11.344 * (1 - (T_h + T_0) / T_100)) - 1)
              + 8.1328E-3 * (10**(-3.49149 * (T_100 / (T_h + T_0) -1)) - 1) + np.log10(es_100))
es_ice = 10**(-9.09718 * (T_0 / (T_h + T_0) - 1) - 3.56654 * np.log10(T_0 / (T_h + T_0)) + 0.876793 * (1 - (T_h + T_0) / T_0) + np.log10(es_0)) # saturation vapour pressure below 0 C (hPa)
q_sat = eps * es_wtr / (p_h - (1 - eps) * es_wtr) # specific humidity at saturation (incorrect below melting point)
freezing = T_h < 0  # replacing saturation specific humidity values below melting point
q_sat[freezing] = eps * es_ice[freezing] / (p_h[freezing] - (1 - eps) * es_ice[freezing])
q_h = RH_cor_h * q_sat / 100   # specific humidity in kg/kg
theta = T_h + z_T *g / c_pd
SHF_h = T_h
SHF_h[:] = 0
LHF_h = SHF_h
L = SHF_h + 1E5

stable = (theta > Tsurf_h) & (WS_h > WS_lim)
unstable = (theta < Tsurf_h) & (WS_h > WS_lim)
# no_wind  = (WS_h <= WS_lim)

for i in np.arange(0,31): # stable stratification
    psi_m1 = -(aa*         z_0/L[stable] + bb*(         z_0/L[stable]-cc/dd)*np.exp(-dd*         z_0/L[stable]) + bb*cc/dd)
    psi_m2 = -(aa*z_WS[stable]/L[stable] + bb*(z_WS[stable]/L[stable]-cc/dd)*np.exp(-dd*z_WS[stable]/L[stable]) + bb*cc/dd)
    psi_h1 = -(aa*z_0h[stable]/L[stable] + bb*(z_0h[stable]/L[stable]-cc/dd)*np.exp(-dd*z_0h[stable]/L[stable]) + bb*cc/dd)
    psi_h2 = -(aa* z_T[stable]/L[stable] + bb*( z_T[stable]/L[stable]-cc/dd)*np.exp(-dd* z_T[stable]/L[stable]) + bb*cc/dd)
    u_star[stable] = kappa*WS_h[stable]/(np.log(z_WS[stable]/z_0)-psi_m2+psi_m1)
    Re[stable] = u_star[stable]*z_0/nu[stable]
    z_0h[stable] = z_0*np.exp(1.5-0.2*np.log(Re[stable])-0.11*(np.log(Re[stable]))**2)
    # if n_elements(where(z_0h[stable] lt 1e-6)) gt 1 then z_0h[stable[where(z_0h[stable] lt 1e-6)]] = 1e-6
    z_0h[stable][z_0h[stable] < 1E-6] == 1E-6
    th_star = kappa*(theta[stable]-Tsurf_h[stable])/(np.log(z_T[stable]/z_0h[stable])-psi_h2+psi_h1)
    q_star  = kappa*(  q_h[stable]- q_surf[stable])/(np.log(z_T[stable]/z_0h[stable])-psi_h2+psi_h1)
    SHF_h[stable] = rho_atm[stable]*c_pd *u_star[stable]*th_star
    LHF_h[stable] = rho_atm[stable]*L_sub*u_star[stable]* q_star
    L_prev = L[stable]
    L[stable] = u_star[stable]**2*(theta[stable]+T_0)*(1+((1-eps)/eps)*q_h[stable])/(g*kappa*th_star*(1+((1-eps)/eps)*q_star))
    L_dif = np.abs((L_prev-L[stable])/L_prev)
    # print,"HF iterations stable stratification: ",i+1,n_elements(where(L_dif gt L_dif_max)),100.*n_elements(where(L_dif gt L_dif_max))/n_elements(where(L_dif))
    # if n_elements(where(L_dif gt L_dif_max)) eq 1 then break
    if np.all(L_dif <= L_dif_max):
        print("LDIF BREAK: ", i)
        break

if len(unstable) > 0:
    for i in np.arange(0,21):
        x1  = (1-gamma*z_0           /L[unstable])**0.25
        x2  = (1-gamma*z_WS[unstable]/L[unstable])**0.25
        y1  = (1-gamma*z_0h[unstable]/L[unstable])**0.5
        y2  = (1-gamma*z_T[unstable] /L[unstable])**0.5
        psi_m1 = np.log(((1+x1)/2)**2*(1+x1**2)/2)-2*np.arctan(x1)+np.pi/2
        psi_m2 = np.log(((1+x2)/2)**2*(1+x2**2)/2)-2*np.arctan(x2)+np.pi/2
        psi_h1 = np.log(((1+y1)/2)**2)
        psi_h2 = np.log(((1+y2)/2)**2)
        u_star[unstable] = kappa*WS_h[unstable]/(np.log(z_WS[unstable]/z_0)-psi_m2+psi_m1)
        Re[unstable] = u_star[unstable]*z_0/nu[unstable]
        z_0h[unstable] = z_0*np.exp(1.5-0.2*np.log(Re[unstable])-0.11*(np.log(Re[unstable]))**2)
        # if n_elements(where(z_0h[unstable] lt 1e-6)) gt 1 then z_0h[unstable[where(z_0h[unstable] lt 1e-6)]] = 1e-6
        z_0h[stable][z_0h[stable] < 1E-6] == 1E-6
        th_star = kappa*(theta[unstable]-Tsurf_h[unstable])/(np.log(z_T[unstable]/z_0h[unstable])-psi_h2+psi_h1)
        q_star  = kappa*(  q_h[unstable]- q_surf[unstable])/(np.log(z_T[unstable]/z_0h[unstable])-psi_h2+psi_h1)
        SHF_h[unstable] = rho_atm[unstable]*c_pd *u_star[unstable]*th_star
        LHF_h[unstable] = rho_atm[unstable]*L_sub*u_star[unstable]* q_star
        L_prev = L[unstable]
        L[unstable] = u_star[unstable]**2*(theta[unstable]+T_0)*(1+((1-eps)/eps)*q_h[unstable])/(g*kappa*th_star*(1+((1-eps)/eps)*q_star))
        L_dif = abs((L_prev-L[unstable])/L_prev)
        # print,"HF iterations unstable stratification: ",i+1,n_elements(where(L_dif gt L_dif_max)),100.*n_elements(where(L_dif gt L_dif_max))/n_elements(where(L_dif))
        # if n_elements(where(L_dif gt L_dif_max)) eq 1 then break
        if np.all(L_dif <= L_dif_max):
            print("LDIF BREAK: ", i)
            break

           
q_h = 1000 * q_h            # from kg/kg to g/kg
HF_nan = np.isnan(p_h) | np.isnan(T_h) | np.isnan(Tsurf_h) | np.isnan(RH_cor_h) | np.isnan(WS_h) | np.isnan(ds_h['z_boom'])
qh_nan = np.isnan(T_h) | np.isnan(RH_cor_h) | np.isnan(p_h) | np.isnan(Tsurf_h)
SHF_h[HF_nan] = np.nan
LHF_h[HF_nan] = np.nan
q_h[qh_nan] = np.nan

#+END_SRC

#+RESULTS:
: LDIF BREAK:  0
: LDIF BREAK:  0


** Export file as L3

+ Check with ~cfchecks ./data/L2/EGP/EGP-raw.nc~

#+BEGIN_SRC jupyter-python
outpath = os.path.split(infile)[0].split("/")
outpath[-2] = 'L3'
outpath = '/'.join(outpath)
outfile_base = os.path.splitext(os.path.basename(infile))[0]
outpathfile = outpath + '/' + outfile_base

ds.to_dataframe().dropna(how='all').to_csv(outpathfile+".csv", float_format="%.2f")

if os.path.exists(outpathfile+"_hour.nc"): os.remove(outpathfile+"_hour.nc")
ds_h.to_netcdf(outpathfile+"_hour.nc", mode='w', format='NETCDF4', compute=True)
ds_h.to_dataframe().dropna(how='all').to_csv(outpathfile+"_hour.csv", float_format="%.2f")

if os.path.exists(outpathfile+"_day.nc"): os.remove(outpathfile+"_day.nc")
ds_d.to_netcdf(outpathfile+"_day.nc", mode='w', format='NETCDF4', compute=True)
ds_d.to_dataframe().dropna(how='all').to_csv(outpathfile+"_day.csv")
#+END_SRC

#+RESULTS:





* L0 to L3

#+BEGIN_SRC bash :tangle ppp.sh :tangle-mode (identity #o744) :var s="EGP"

# conda activate PROMICE_dev

rm ./data/L{1,1A,2,3}/EGP/*

# L0 -> L1
# for f in $(ls ./data/L0/${s}/*); do ./L0_to_L1.py ${f}; done
# parallel --bar "./L0_to_L1.py {}" ::: $(ls ./data/L0/${s}/*)
./L0_to_L1.py ./data/L0/EGP/EGP_2016_raw.txt.hdr

# L1 -> L1A
# ./L1_to_L1A.py data/L1/${s}/*raw.nc
# ./L1_to_L1A.py data/L1/${s}/*STM.nc
# ./L1_to_L1A.py data/L1/${s}/*TX.nc
# parallel --bar "./L1_to_L1A.py {}" ::: $(ls data/L1/${s}/*)
./L1_to_L1A.py ./data/L1/EGP/*

# L1A to L2
# ./L1A_to_L2.py data/L1A/${s}/${s}-raw.nc
# ./L1A_to_L2.py data/L1A/${s}/${s}-STM.nc
# ./L1A_to_L2.py data/L1A/${s}/${s}-TX.nc
# parallel --bar "./L1A_to_L2.py {}" ::: $(ls data/L1A/${s}/*)
./L1A_to_L2.py ./data/L1A/EGP/*

# L2 to L3
# ./L2_to_L3.py data/L2/${s}/*raw.nc
# ./L2_to_L3.py data/L2/${s}/*STM.nc
# ./L2_to_L3.py data/L2/${s}/*TX.nc
# parallel --bar "./L2_to_L3.py {}" ::: $(ls data/L2/${s}/*)
./L2_to_L3.py ./data/L2/EGP/*

#+END_SRC


* Helper functions

#+NAME: constants
#+BEGIN_SRC jupyter-python
import numpy as np

deg2rad = np.pi / 180
rad2deg = 1 / deg2rad
#+END_SRC

** Load L0 hdr + data

#+NAME: read_L0
#+BEGIN_SRC jupyter-python

import os
import numpy as np
import pandas as pd
pd.set_option('display.precision', 2)
import xarray as xr
xr.set_options(keep_attrs=True)


def read_L0(filename):
    
    hdr = {}
    with open(filename) as f:
        for line in f:
            if line[0] == "\n": continue   # blank line
            if line[0] == "#": continue    # comment

            assert("=" in line)
            key = line.split("=")[0].strip()
            val = line.split("=")[1].strip()
            val = val.split("#")[0].strip() # remove trailing comments

            # Convert from string to number if it is a number
            if val.strip('-').strip('+').replace('.','').isdigit():
                val = np.float(val)
                if val == np.int(val):
                    val = np.int(val)

            hdr[key] = val
    # done reading header

    # check required fields were present
    for req in ["columns", "station_id", "format", "nodata", "skiprows"]: assert(req in hdr.keys())

    sep = ","
    names = [_.strip() for _ in hdr.pop('columns').split(sep)]
    datafile = os.path.splitext(filename)[0]

    df = pd.read_csv(datafile,
                     comment = "#",
                     index_col = 0,
                     names = names,
                     parse_dates = True,
                     sep = sep,
                     skiprows = hdr["skiprows"],
                     skip_blank_lines = True,
                     usecols=np.arange(len(names)))

    ds = df.to_xarray()
    ds = ds.where(ds != hdr['nodata'])     # Set nodata to NAN

    # carry relevant metadata with ds
    meta = {}
    skip = ["columns", "nodata", "skiprows"]
    for k in hdr.keys():
        if k not in skip: meta[k] = hdr[k]
    ds.attrs = meta

    return ds

# ds = read_L0("./data/L0/EGP/EGP_2016_raw.txt.hdr")
# print(ds)
#+END_SRC

#+RESULTS: read_L0

#+RESULTS:
#+begin_example
<xarray.Dataset>
Dimensions:      (time: 10847)
Coordinates:
  ,* time         (time) datetime64[ns] 2016-05-01T14:30:00 ... 2016-07-19T17:...
Data variables:
    rec          (time) float64 51.0 52.0 53.0 ... 1.09e+04 1.09e+04 1.09e+04
    min_y        (time) float64 1.765e+05 1.766e+05 ... 2.905e+05 2.905e+05
    p            (time) float64 724.4 724.1 724.4 724.4 ... 730.8 731.2 730.7
    t_1          (time) float64 -20.1 -19.79 -19.31 ... -6.904 -6.904 -6.861
    t_2          (time) float64 -19.56 -19.11 -18.92 ... -6.866 -6.86 -6.799
    rh           (time) float64 54.1 51.7 50.23 49.51 ... 80.28 80.93 81.81
    wspd         (time) float64 1.062 0.918 0.636 0.486 ... 2.793 2.951 3.069
    wdir         (time) float64 265.1 259.2 216.8 208.4 ... 217.7 216.6 225.4
    wd_std       (time) float64 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0
    dsr          (time) float64 681.7 732.4 688.3 689.6 ... 724.7 711.4 698.8
    usr          (time) float64 518.6 559.3 531.8 534.4 ... 559.2 549.6 524.1
    dlr          (time) float64 -81.57 -102.0 -101.3 ... -135.8 -135.6 -132.4
    ulr          (time) float64 -23.97 -28.65 -33.92 ... -32.33 -32.52 -28.84
    t_rad        (time) float64 -12.78 -11.42 -9.929 ... -1.114 -1.03 -1.135
    z_boom       (time) float64 2.685 2.683 2.683 2.68 ... 2.583 2.584 2.58
    z_boom_q     (time) float64 190.0 192.0 189.0 187.0 ... 192.0 182.0 168.0
    z_stake      (time) float64 nan nan nan nan nan nan ... nan nan nan nan nan
    z_stake_q    (time) float64 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0
    z_pt         (time) float64 nan nan nan nan nan nan ... nan nan nan nan nan
    t_i_1        (time) float64 nan -109.0 -109.0 ... -8.478 -8.458 -8.448
    t_i_2        (time) float64 nan nan -109.0 -109.0 ... -9.67 -9.67 -9.67
    t_i_3        (time) float64 nan -109.0 -109.0 ... -8.879 -8.859 -8.849
    t_i_4        (time) float64 nan -109.0 -109.0 ... -10.74 -10.73 -10.74
    t_i_5        (time) float64 nan -109.0 -109.0 ... -12.67 -12.67 -12.67
    t_i_6        (time) float64 nan -109.0 nan -109.0 ... -14.9 -14.9 -14.9
    t_i_7        (time) float64 nan -109.0 -109.0 nan ... -17.16 -17.16 -17.16
    t_i_8        (time) float64 nan nan -109.0 nan ... -20.75 -20.76 -20.76
    tilt_x       (time) float64 3.527 3.492 3.516 3.489 ... 0.109 0.095 0.174
    tilt_y       (time) float64 -0.945 -0.938 -0.924 ... -0.828 -0.849 -0.859
    gps_time     (time) object nan nan nan ... 'GT170007.00' 'GT170007.00'
    gps_lat      (time) object nan nan nan ... 'NH7537.47563' 'NH7537.47563'
    gps_lon      (time) object nan nan nan ... 'WH03558.49655' 'WH03558.49655'
    gps_alt      (time) float64 nan nan nan ... 2.663e+03 2.663e+03 2.663e+03
    gps_geoid    (time) float64 nan nan nan nan nan ... 41.6 41.6 41.6 41.6 41.6
    gps_geounit  (time) object nan nan nan nan nan nan ... 'M' 'M' 'M' 'M' 'M'
    gps_q        (time) float64 nan nan nan nan nan nan ... 1.0 1.0 1.0 1.0 1.0
    gps_numsat   (time) float64 nan nan nan nan nan ... 11.0 12.0 12.0 12.0 12.0
    gps_hdop     (time) float64 nan nan nan nan nan ... 0.71 0.73 0.73 0.73 0.73
    t_log        (time) float64 -12.6 -12.08 -11.65 ... -1.801 -1.735 -1.5
    fan_dc       (time) float64 137.5 141.3 142.3 141.8 ... 123.5 123.9 124.1
    batt_v_ss    (time) float64 15.52 15.81 15.79 15.81 ... 14.47 14.47 14.47
    batt_v       (time) float64 15.23 15.56 15.53 15.63 ... 14.4 14.41 14.41
Attributes:
    station_id:          EGP
    format:              raw
    hygroclip_t_offset:  0
    dsr_eng_coef:        12.71
    usr_eng_coef:        12.71
    dlr_eng_coef:        12.71
    ulr_eng_coef:        12.71
#+end_example

** Flag invalid data

#+NAME: flag_NAN
#+BEGIN_SRC jupyter-python
def flag_NAN(ds):
    """Uses the flag DB (flags.csv) to set stations, sensors, and times to NaN."""
    flag_LUT = pd.read_csv("./data/flags/PROMICE.csv", index_col=2, usecols=(0,3,4,5))
    flag_LUT = flag_LUT[flag_LUT['#'] == '#'].drop(columns="#").dropna(how='all', axis='rows')
    
    df = pd.read_csv("./data/flags/PROMICE.csv", parse_dates=[0,1], comment="#", header=1)\
           .dropna(how='all', axis='rows')\
           .drop(columns="comment")

    # check format of flags.csv. Either both or neither of t0 and t1 must be defined.
    assert(((np.isnan(df['t0']).astype(np.int) + np.isnan(df['t1']).astype(np.int)) % 2).sum() == 0)
    # for now we only process the NAN flag
    df = df[df['flag'] == "NAN"]
    if df.shape[0] == 0: return ds
    # subset to this station
    df = df[[((ds.attrs['station_id'] in _) | ('*' in _)) for _ in df['station']]]
    if df.shape[0] == 0: return ds

    for i in df.index:
        t0, t1, avar = df.loc[i,['t0','t1','variable']]
        varlist = avar.split() if avar != '*' else list(ds.variables)
        if 'time' in varlist: varlist.remove("time")
        if pd.isnull(t0): t0, t1 = ds['time'].values[[0,-1]]
        for v in varlist:
            ds[v] = ds[v].where((ds['time'] < t0) | (ds['time'] > t1))

        # TODO: Mark these values in the ds_flags dataset using perhaps flag_LUT.loc[NAN]['value']

    return ds
#+END_SRC

** Add variable metadata

This function reads in the variables db ([[./variables.csv]]) and adds the metadata contained therein to the xarray variable (and therefore, eventually, the NetCDF file). See [[./variables.org]] for documentation on the variable DB.

#+NAME: add_variable_metadata
#+BEGIN_SRC jupyter-python
def add_variable_metadata(ds):
    """Uses the variable DB (variables.csv) to add metadata to the xarray dataset."""
    df = pd.read_csv("./variables.csv", index_col=0, comment="#")

    for v in df.index:
        if v == 'time': continue # coordinate variable, not normal var
        if v not in list(ds.variables): continue
        for c in ['standard_name', 'long_name', 'units', 'scale_factor', 'add_offset']:
            if isinstance(df[c][v], np.float) and np.isnan(df[c][v]): continue
            ds[v].attrs[c] = df[c][v]
            
    return ds
#+END_SRC

* Compare Python & IDL
:PROPERTIES:
:header-args:jupyter-python+: :session compare
:END:
** Graphic
#+BEGIN_SRC jupyter-python
import pandas as pd

py = pd.read_csv("./data/L3/EGP/EGP-raw_hour.csv", index_col=0, parse_dates=True)


def mydf(y,m,d,h):
    return pd.datetime(int(y),int(m),int(d),int(h))

idl = pd.read_csv("./IDL/data/out/EGP_hour_v03.txt",
                  delimiter="\s+",
                  parse_dates={'time':[0,1,2,3]},
                  infer_datetime_format=True,
                  date_parser=mydf,
                  index_col=0)


# t
clf()
ax = py['t_1'].plot(linewidth=3, label='v4')
idl['AirTemperature(C)'].plot(ax=ax, label='v3')
legend()

# dwr
clf()
ax = py['dsr'].plot(linewidth=3, label='v4')
idl['ShortwaveRadiationDown(W/m2)'].plot(ax=ax, label='v3')
legend()
#+END_SRC

#+RESULTS:
:RESULTS:
: <ipython-input-37-4f0e9ef3a40c>:7: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.
:   return pd.datetime(int(y),int(m),int(d),int(h))
: <matplotlib.legend.Legend at 0x7f9512cac040>
:END:


** Columns

IDL output
#+BEGIN_SRC bash
# head -n1 ./IDL/data/out/EGP_month_v03.txt | tr ' ' '\n'
head -n1 ./IDL/data/out/EGP_hour_v03.txt | tr ' ' '\n'
#+END_SRC

#+RESULTS:
|                                  |
| Year                             |
| MonthOfYear                      |
| DayOfMonth                       |
| HourOfDay(UTC)                   |
| DayOfYear                        |
| DayOfCentury                     |
| AirPressure(hPa)                 |
| AirTemperature(C)                |
| AirTemperatureHygroClip(C)       |
| RelativeHumidity(%)              |
| SpecificHumidity(g/kg)           |
| WindSpeed(m/s)                   |
| WindDirection(d)                 |
| SensibleHeatFlux(W/m2)           |
| LatentHeatFlux(W/m2)             |
| ShortwaveRadiationDown(W/m2)     |
| ShortwaveRadiationDown_Cor(W/m2) |
| ShortwaveRadiationUp(W/m2)       |
| ShortwaveRadiationUp_Cor(W/m2)   |
| Albedo_theta<70d                 |
| LongwaveRadiationDown(W/m2)      |
| LongwaveRadiationUp(W/m2)        |
| CloudCover                       |
| SurfaceTemperature(C)            |
| HeightSensorBoom(m)              |
| HeightStakes(m)                  |
| DepthPressureTransducer(m)       |
| DepthPressureTransducer_Cor(m)   |
| IceTemperature1(C)               |
| IceTemperature2(C)               |
| IceTemperature3(C)               |
| IceTemperature4(C)               |
| IceTemperature5(C)               |
| IceTemperature6(C)               |
| IceTemperature7(C)               |
| IceTemperature8(C)               |
| TiltToEast(d)                    |
| TiltToNorth(d)                   |
| TimeGPS(hhmmssUTC)               |
| LatitudeGPS(degN)                |
| LongitudeGPS(degW)               |
| ElevationGPS(m)                  |
| HorDilOfPrecGPS                  |
| LoggerTemperature(C)             |
| FanCurrent(mA)                   |
| BatteryVoltage(V)                |

Python outputs:
#+BEGIN_SRC bash
head -n1 ./data/L3/EGP/EGP-raw_hour.csv | tr ',' '\n'
#+END_SRC

#+RESULTS:
| time       |
| rec        |
| p          |
| t_1        |
| t_2        |
| rh         |
| wspd       |
| wdir       |
| wd_std     |
| dsr        |
| usr        |
| dlr        |
| ulr        |
| t_rad      |
| z_boom     |
| z_boom_q   |
| z_stake    |
| z_stake_q  |
| z_pt       |
| t_i_1      |
| t_i_2      |
| t_i_3      |
| t_i_4      |
| t_i_5      |
| t_i_6      |
| t_i_7      |
| t_i_8      |
| tilt_x     |
| tilt_y     |
| gps_time   |
| gps_lat    |
| gps_lon    |
| gps_alt    |
| gps_geoid  |
| gps_q      |
| gps_numsat |
| gps_hdop   |
| t_log      |
| fan_dc     |
| batt_v_ss  |
| batt_v     |
| rh_cor     |
| cc         |
| wdir_std   |
| albedo     |
| t          |
| t_surf     |
| isr_cor    |
| usr_cor   |
| wspd_x     |
| wspd_y     |

** Load both to dfs (10 min)

#+BEGIN_SRC jupyter-python :tangle compare.py
import numpy as np
import pandas as pd
import xarray as xr

def mydf(y,m,d,h):
    return pd.datetime(int(y),int(m),int(d),int(h))

def mydf2(y,mo,d,h,mi):
    return pd.datetime(int(y),int(mo),int(d),int(h),int(mi))

idl = pd.read_csv("./IDL/out/EGP_inst_v03.txt",
                  delimiter="\s+",
                  parse_dates={'time':[0,1,2,3,4]},
                  infer_datetime_format=True,
                  date_parser=mydf2,
                  index_col=0)\
        .drop(columns=['DayOfYear'])\
        .replace(-999, np.nan)\
        .dropna(how='all')\
        .rename(columns={'AirPressure(hPa)' : 'p',
                         'AirTemperature(C)' : 't_1',
                         'AirTemperatureHygroClip(C)' : 't_2',
                         'RelativeHumidity(%)' : 'rh_cor',
                         'SpecificHumidity(g/kg)' : 'rh_cor2',
                         'WindSpeed(m/s)' : 'wspd',
                         'WindDirection(d)' : 'wdir',
                         'SensibleHeatFlux(W/m2)' : 'shf',
                         'LatentHeatFlux(W/m2)' : 'lhf',
                         'ShortwaveRadiationDown(W/m2)': 'dsr',
                         'ShortwaveRadiationDown_Cor(W/m2)' : 'dsr_cor',
                         'ShortwaveRadiationUp(W/m2)' : 'usr',
                         'ShortwaveRadiationUp_Cor(W/m2)' : 'usr_cor',
                         'Albedo_theta<70d' : 'albedo',
                         'Albedo' : 'albedo',
                         'LongwaveRadiationDown(W/m2)' : 'dlr',
                         'LongwaveRadiationUp(W/m2)' : 'ulr',
                         'CloudCover' : 'cc',
                         'SurfaceTemperature(C)' : 't_surf',
                         'HeightSensorBoom(m)' : 'z_boom',
                         'HeightStakes(m)': 'z_stake',
                         'DepthPressureTransducer(m)' : 'z_pt',
                         'DepthPressureTransducer_Cor(m)' : 'z_pt_cor',
                         'IceTemperature1(C)' : 't_i_1',
                         'IceTemperature2(C)' : 't_i_2',
                         'IceTemperature3(C)' : 't_i_3',
                         'IceTemperature4(C)' : 't_i_4',
                         'IceTemperature5(C)' : 't_i_5',
                         'IceTemperature6(C)' : 't_i_6',
                         'IceTemperature7(C)' : 't_i_7',
                         'IceTemperature8(C)' : 't_i_8',
                         'TiltToEast(d)' : 'tilt_x',
                         'TiltToNorth(d)' : 'tilt_y',
                         'TimeGPS(hhmmssUTC)' : 'gps_t',
                         'LatitudeGPS(degN)' : 'gps_lat',
                         'LongitudeGPS(degW)' : 'gps_lon',
                         'ElevationGPS(m)' : 'gps_alt',
                         'HorDilOfPrecGPS' : 'gps_hdop',
                         'LoggerTemperature(C)' : 't_logger',
                         'FanCurrent(mA)' : 'fan_dc',
                         'BatteryVoltage(V)' : 'batt_v'})

# df = xr.load_dataset("./data/L3/EGP/EGP-raw_hour.nc", mask_and_scale=False)\
#        .to_dataframe()\
#        .drop(columns='rh')\
#        .rename(columns={'rh_cor':'rh'})

df = pd.read_csv("./data/L3/EGP/EGP-raw.csv", index_col=0, parse_dates=True)#\
       # .drop(columns=['rh','usr','dsr'])\
       # .rename(columns={'rh_cor':'rh',
       #                  'dsr_cor': 'dsr',
       #                  'usr_cor':'usr'})


# subset to same columns
subset = np.intersect1d(df.columns, idl.columns)
df = df[subset]
idl = idl[subset]

# idl = idl.dropna(how='all', axis='columns')

# print(df.columns)

diff = df - idl
diff_pct = diff / idl*100

# diff_pct[abs(diff) < 1E-1] = 0

pd.options.display.float_format = "{:,.5f}".format

# limit to where
dd = diff_pct.describe().T.drop(columns="count")
# dd = diff_pct.describe().T.drop(columns="count").replace(0,np.nan).dropna(how='all', axis='rows').replace(np.nan,0)

if __name__ == "__main__": 
    print(dd)

# diff_pct.plot()
# diff_pct.replace(0,np.nan).dropna(how='all', axis='columns').plot()
def plot_diff(df,idl,var):
    clf()
    ax=idl[var].plot(label='IDL '+var, linewidth=3)
    df[var].plot(label='Py', ax=ax)
    legend()
    ((df-idl)/idl*100)[var].plot(label='diff', ax=ax.twinx(), color='black')
    
var = 'dsr_cor';plot_diff(df,idl,var)

dd
#+END_SRC

#+RESULTS:
:RESULTS:
#+begin_example
<ipython-input-190-06de5da93cd6>:9: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.
  return pd.datetime(int(y),int(mo),int(d),int(h),int(mi))
             mean     std      min      25%      50%      75%      max
albedo    0.01177 0.36231 -0.77519 -0.25575  0.00000  0.35419  0.71942
batt_v   -0.00002 0.00149 -0.07158  0.00000  0.00000  0.00000  0.06845
cc        0.00000 0.00000  0.00000  0.00000  0.00000  0.00000  0.00000
dlr       0.00007 0.01567 -0.03652 -0.01262  0.00000  0.01295  0.03597
dsr       0.00017 0.02741 -0.45455 -0.00736  0.00000  0.00758  0.30769
dsr_cor   0.00011 0.02209 -0.31447 -0.00740  0.00000  0.00727  0.26490
fan_dc    0.00000 0.00000  0.00000  0.00000  0.00000  0.00000  0.00000
gps_alt   0.00000 0.00000  0.00000  0.00000  0.00000  0.00000  0.00000
gps_hdop  0.00000 0.00000  0.00000  0.00000  0.00000  0.00000  0.00000
gps_lat  -0.00605 0.00002 -0.00612 -0.00607 -0.00605 -0.00604 -0.00598
gps_lon   0.00630 0.01224 -0.01390 -0.01370  0.01370  0.01380  0.01390
p        -0.00000 0.00009 -0.00141  0.00000  0.00000  0.00000  0.00138
rh_cor    0.00019 0.03147 -0.07937 -0.02347  0.00000  0.02375  0.07508
t_1       0.00000 0.00043 -0.00000 -0.00000 -0.00000 -0.00000  0.04439
t_2       0.00001 0.00207 -0.15456 -0.00000 -0.00000 -0.00000  0.08313
t_i_1     0.00001 0.00080 -0.03948 -0.00000 -0.00000 -0.00000  0.05260
t_i_2     0.00001 0.00049 -0.00000 -0.00000 -0.00000 -0.00000  0.04760
t_i_3    -0.00000 0.00039 -0.03778 -0.00000 -0.00000 -0.00000 -0.00000
t_i_4     0.00000 0.00099 -0.04673 -0.00000 -0.00000 -0.00000  0.08313
t_i_5    -0.00001 0.00078 -0.07564 -0.00000 -0.00000 -0.00000 -0.00000
t_i_6    -0.00001 0.00068 -0.06570 -0.00000 -0.00000 -0.00000 -0.00000
t_i_7    -0.00001 0.00064 -0.04673 -0.00000 -0.00000 -0.00000 -0.00000
t_i_8    -0.00001 0.00130 -0.04728 -0.00000 -0.00000 -0.00000  0.03602
t_surf   -0.00000 0.00390 -0.15198 -0.00000 -0.00000 -0.00000  0.12970
tilt_x    0.00000 0.00000  0.00000  0.00000  0.00000  0.00000  0.00000
tilt_y    0.00000 0.00000 -0.00000 -0.00000 -0.00000 -0.00000 -0.00000
ulr       0.00011 0.01194 -0.02762 -0.01026  0.00000  0.01041  0.02741
usr      -0.00000 0.02683 -0.29412 -0.00926  0.00000  0.00918  0.27972
usr_cor   0.00000 0.02615 -0.29412 -0.00928  0.00000  0.00918  0.27972
wdir      0.00047 0.07572 -3.33333  0.00000  0.00000  0.00000  4.00000
wspd      0.00023 0.06383 -0.75188  0.00000  0.00000  0.00000  1.96078
z_boom   -0.03392 0.12652 -0.34662 -0.15480 -0.07746  0.07849  0.37175
z_pt          nan     nan      nan      nan      nan      nan      nan
z_stake       nan     nan      nan      nan      nan      nan      nan
#+end_example
|          |          mean |           std |          min |          25% |          50% |          75% |          max |
|----------+---------------+---------------+--------------+--------------+--------------+--------------+--------------|
| albedo   |   0.0117673   |   0.362313    |  -0.775194   |  -0.255754   |   0          |   0.354191   |   0.719424   |
| batt_v   |  -1.93054e-05 |   0.00148737  |  -0.071582   |   0          |   0          |   0          |   0.0684463  |
| cc       |   0           |   0           |   0          |   0          |   0          |   0          |   0          |
| dlr      |   6.53708e-05 |   0.015672    |  -0.036523   |  -0.0126176  |   0          |   0.0129534  |   0.0359712  |
| dsr      |   0.000169655 |   0.0274063   |  -0.454545   |  -0.00735869 |   0          |   0.00757576 |   0.307692   |
| dsr_cor  |   0.000110582 |   0.0220876   |  -0.314465   |  -0.00739953 |   0          |   0.00727273 |   0.264901   |
| fan_dc   |   0           |   0           |   0          |   0          |   0          |   0          |   0          |
| gps_alt  |   0           |   0           |   0          |   0          |   0          |   0          |   0          |
| gps_hdop |   0           |   0           |   0          |   0          |   0          |   0          |   0          |
| gps_lat  |  -0.006053    |   2.35483e-05 |  -0.00612142 |  -0.00606972 |  -0.00605398 |  -0.00603706 |  -0.00597994 |
| gps_lon  |   0.00630119  |   0.0122361   |  -0.0138972  |  -0.0136951  |   0.0137006  |   0.0137951  |   0.0138972  |
| p        |  -1.65214e-06 |   8.63837e-05 |  -0.00140768 |   0          |   0          |   0          |   0.00138127 |
| rh_cor   |   0.000190723 |   0.0314736   |  -0.0793651  |  -0.0234742  |   0          |   0.023753   |   0.0750751  |
| t_1      |   4.09232e-06 |   0.000426191 |  -0          |  -0          |  -0          |  -0          |   0.0443853  |
| t_2      |   8.37241e-06 |   0.00207168  |  -0.15456    |  -0          |  -0          |  -0          |   0.0831255  |
| t_i_1    |   5.83036e-06 |   0.000803481 |  -0.0394789  |  -0          |  -0          |  -0          |   0.0526039  |
| t_i_2    |   5.08074e-06 |   0.000491757 |  -0          |  -0          |  -0          |  -0          |   0.0475964  |
| t_i_3    |  -4.03273e-06 |   0.000390322 |  -0.0377786  |  -0          |  -0          |  -0          |  -0          |
| t_i_4    |   3.8852e-06  |   0.000985282 |  -0.046729   |  -0          |  -0          |  -0          |   0.0831255  |
| t_i_5    |  -8.07461e-06 |   0.000781529 |  -0.075643   |  -0          |  -0          |  -0          |  -0          |
| t_i_6    |  -7.01356e-06 |   0.000678831 |  -0.065703   |  -0          |  -0          |  -0          |  -0          |
| t_i_7    |  -9.25971e-06 |   0.000635594 |  -0.046729   |  -0          |  -0          |  -0          |  -0          |
| t_i_8    |  -5.82101e-06 |   0.00129989  |  -0.0472813  |  -0          |  -0          |  -0          |   0.0360231  |
| t_surf   |  -2.36396e-06 |   0.0039039   |  -0.151976   |  -0          |  -0          |  -0          |   0.129702   |
| tilt_x   |   0           |   0           |   0          |   0          |   0          |   0          |   0          |
| tilt_y   |   0           |   0           |  -0          |  -0          |  -0          |  -0          |  -0          |
| ulr      |   0.000106376 |   0.0119403   |  -0.0276243  |  -0.0102643  |   0          |   0.0104067  |   0.0274123  |
| usr      |  -1.56102e-07 |   0.026833    |  -0.294118   |  -0.00925712 |   0          |   0.00918274 |   0.27972    |
| usr_cor  |   2.97889e-06 |   0.0261547   |  -0.294118   |  -0.00927698 |   0          |   0.00918133 |   0.27972    |
| wdir     |   0.00046569  |   0.0757244   |  -3.33333    |   0          |   0          |   0          |   4          |
| wspd     |   0.00023373  |   0.0638281   |  -0.75188    |   0          |   0          |   0          |   1.96078    |
| z_boom   |  -0.033924    |   0.126517    |  -0.34662    |  -0.154799   |  -0.0774593  |   0.0784929  |   0.371747   |
| z_pt     | nan           | nan           | nan          | nan          | nan          | nan          | nan          |
| z_stake  | nan           | nan           | nan          | nan          | nan          | nan          | nan          |
:END:
